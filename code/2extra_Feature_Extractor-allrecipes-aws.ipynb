{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T15:54:46.670053Z",
     "start_time": "2019-12-01T15:54:36.503015Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#code from Roberto, Nov.29, modify and see how long it takes to run \n",
    "from keras.applications import VGG16\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread  #installed skimage - image processing library for sklearn\n",
    "from skimage.transform import resize \n",
    "from keras.applications.imagenet_utils import preprocess_input, decode_predictions\n",
    "from keras.models import Model\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T15:54:46.686918Z",
     "start_time": "2019-12-01T15:54:46.673188Z"
    }
   },
   "outputs": [],
   "source": [
    "#my imports\n",
    "from keras.preprocessing import image\n",
    "from keras_applications import imagenet_utils\n",
    "#see doc for built-in functions https://github.com/keras-team/keras-applications/blob/master/keras_applications/imagenet_utils.py#L157\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T15:54:46.695052Z",
     "start_time": "2019-12-01T15:54:46.690066Z"
    }
   },
   "outputs": [],
   "source": [
    "#unzip:\n",
    "#with zipfile.ZipFile('image_data/allre_images.zip', 'r') as zip_ref:\n",
    "#    zip_ref.extractall('image_data/allrecipes_photos') #only needed to run this once to unzip \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T15:54:48.435356Z",
     "start_time": "2019-12-01T15:54:46.698129Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35299"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = !ls image_data/allrecipes_photos/allre_images/userphotos/250x250\n",
    "len(test)\n",
    " #was 35k before, upload zipped file again??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the list of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T15:54:48.706188Z",
     "start_time": "2019-12-01T15:54:48.438980Z"
    }
   },
   "outputs": [],
   "source": [
    "#shoe_list = !ls ZShoes/Images/\n",
    "\n",
    "#! - runs command line\n",
    "#!ls project5data/2017_140k/recipe_photos/bbc_photos/pages-photos\n",
    "\n",
    "#bbc_list = !ls project5data/2017_140k/recipe_photos/bbc_photos/pages-photos  \n",
    "    #local directory, doesn't work on aws? modified:\n",
    "    \n",
    "alr_list = !ls image_data/allrecipes_photos/allre_images/userphotos/250x250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T15:54:48.727043Z",
     "start_time": "2019-12-01T15:54:48.714724Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35299"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alr_list) #35300 images in allrecipes 250x250 dataset, ~60k in 560x315 dataset\n",
    "#len only 3001? normal on local data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T15:54:48.745492Z",
     "start_time": "2019-12-01T15:54:48.732709Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1000675.jpg'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alr_list[10] #not labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T15:54:48.762313Z",
     "start_time": "2019-12-01T15:54:48.751240Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IPython.utils.text.SList"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(alr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T15:54:48.770114Z",
     "start_time": "2019-12-01T15:54:48.766255Z"
    }
   },
   "outputs": [],
   "source": [
    "#shoe_number = [int(x.split('.')[0]) for x in shoe_list]\n",
    "#shoe_number.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T15:54:48.784078Z",
     "start_time": "2019-12-01T15:54:48.775652Z"
    }
   },
   "outputs": [],
   "source": [
    "#shoe_number[:5] # may need numbers later to grab every 10 images or so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T15:54:58.498367Z",
     "start_time": "2019-12-01T15:54:48.786433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/anaconda/envs/metis/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# load VGG16\n",
    "#Get back the convolutional part of a VGG network trained on ImageNet\n",
    "model = VGG16(weights='imagenet', include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T15:54:58.529435Z",
     "start_time": "2019-12-01T15:54:58.502513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T15:54:58.544887Z",
     "start_time": "2019-12-01T15:54:58.534102Z"
    }
   },
   "outputs": [],
   "source": [
    "def image_pre_process(img_file):\n",
    "    img = imread(img_file)  \n",
    "    img = resize(img, (224, 224), preserve_range=True).astype(np.float32) #resize image\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img) #keras preprocessing function\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T15:54:58.567411Z",
     "start_time": "2019-12-01T15:54:58.557176Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/anaconda/envs/metis/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"fc...)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "model2 = Model(input=model.input, output=model.get_layer('fc2').output) # gets the output in fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T15:54:58.579692Z",
     "start_time": "2019-12-01T15:54:58.573505Z"
    }
   },
   "outputs": [],
   "source": [
    "# def get_images(directory):\n",
    "#     '''stores all images in directory into a list'''\n",
    "#     img_lst = []\n",
    "#     for entry in os.scandir(directory): #loop through files in directory, path is file path of folder containing images\n",
    "#         if entry.path == directory + '/.DS_Store':\n",
    "#             continue  #avoid reading mac os's .DS_Store as an image\n",
    "#         else:\n",
    "#             img_lst.append(entry.path)\n",
    "#     return img_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that can get the data in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T15:54:58.597214Z",
     "start_time": "2019-12-01T15:54:58.588794Z"
    }
   },
   "outputs": [],
   "source": [
    "# def process_data(start, end):\n",
    "#     out1 = []\n",
    "#     for i in range(start,end):\n",
    "#         print(i)\n",
    "#         name = 'ZShoes/Images/' + str(i) + '.jpg'\n",
    "#         im = image_pre_process(name)\n",
    "#         out = model2.predict(im)\n",
    "#         out1.append(out[0])\n",
    "#     return out1\n",
    "\n",
    "#looping through images in directory\n",
    "# def get_images(directory):\n",
    "#     '''stores all images in directory into a list'''\n",
    "#     img_lst = []\n",
    "#     for entry in os.scandir(directory): #loop through files in directory, path is file path of folder containing images\n",
    "#         if entry.path == directory + '/.DS_Store':\n",
    "#             continue  #avoid reading mac os's .DS_Store as an image\n",
    "#         else:\n",
    "#             img_lst.append(entry.path)\n",
    "#     return img_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T15:54:58.638176Z",
     "start_time": "2019-12-01T15:54:58.624326Z"
    }
   },
   "outputs": [],
   "source": [
    "#def process_data(start, end, directory):\n",
    "def process_data(imglist,directory):\n",
    "    out1 = []\n",
    "    print(len(imglist))\n",
    "    for file in imglist:\n",
    "        #print(file)\n",
    "        filepath = directory + file\n",
    "        #print(filepath)\n",
    "        im = image_pre_process(filepath)\n",
    "        out = model2.predict(im)\n",
    "        out1.append(out[0])\n",
    "        if len(out1) %100 == 0:\n",
    "            print(str(len(out1)) + ' out of ' + str(len(imglist)) + ' done')\n",
    "    return out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T15:54:58.645717Z",
     "start_time": "2019-12-01T15:54:58.640598Z"
    }
   },
   "outputs": [],
   "source": [
    "alrpath = 'image_data/allrecipes_photos/allre_images/userphotos/250x250/' #new path for aws, add'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T04:23:10.391366Z",
     "start_time": "2019-12-01T04:21:03.773227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100 out of 100 done\n"
     ]
    }
   ],
   "source": [
    "out1 = process_data(alr_list[:100], alrpath)\n",
    "res = pd.DataFrame(out1)\n",
    "\n",
    "#still 1min for 100 images like in bbc case, makes sense (but just as slow as local one?)\n",
    "\n",
    "#Nov30, zipped and uploaded data again, on aws, 1min 34s, public wifi - slower than at Metis\n",
    "#home, aws, 2m for first 100? Internet speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-30T05:12:37.050357Z",
     "start_time": "2019-11-30T05:01:04.164861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n",
      "100 out of 1001 done\n",
      "200 out of 1001 done\n",
      "300 out of 1001 done\n",
      "400 out of 1001 done\n",
      "500 out of 1001 done\n",
      "600 out of 1001 done\n",
      "700 out of 1001 done\n",
      "800 out of 1001 done\n",
      "900 out of 1001 done\n",
      "1000 out of 1001 done\n"
     ]
    }
   ],
   "source": [
    "out1 = process_data(alr_list[:1001], alrpath)\n",
    "res = pd.DataFrame(out1)\n",
    "#for bbc case, full list of 2226 images, 35mins, still appending df though\n",
    "#*optimize code to add each row of df to csv first?\n",
    "\n",
    "#11min33s for 1000 images, reasonable (smaller chunk, faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-30T05:12:43.126185Z",
     "start_time": "2019-11-30T05:12:37.055236Z"
    }
   },
   "outputs": [],
   "source": [
    "res.to_csv('allreFeatures1001.csv') #took 6s to write one big dataframe to csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-30T05:28:15.942143Z",
     "start_time": "2019-11-30T05:16:29.109248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out2 = process_data(alr_list[1001:2001], alrpath)\n",
    "res = pd.DataFrame(out2)\n",
    "#11min47s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-30T05:28:24.393385Z",
     "start_time": "2019-11-30T05:28:15.947732Z"
    }
   },
   "outputs": [],
   "source": [
    "res.to_csv('allreFeatures2001.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-30T05:43:10.217421Z",
     "start_time": "2019-11-30T05:32:11.645482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out2 = process_data(alr_list[2001:3001], alrpath)\n",
    "res = pd.DataFrame(out2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-30T05:43:25.386689Z",
     "start_time": "2019-11-30T05:43:16.937198Z"
    }
   },
   "outputs": [],
   "source": [
    "res.to_csv('allreFeatures3001.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append every 1000 images to large df? or can just load every 1000 (small csv into df with function? - depending on how to use data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T02:09:38.215479Z",
     "start_time": "2019-12-01T01:51:18.304242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "#Nov 30 evening, start here, append to background while analyzing bbc /allre subset data (can do both on aws?)\n",
    "#technically can do this locally also, not much slower*\n",
    "out6 = process_data(alr_list[5001:6001], alrpath)\n",
    "res = pd.DataFrame(out6)\n",
    "\n",
    "#a lot slower, why? earlier 11min; aws shouldn't be affected by local changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T02:09:46.789164Z",
     "start_time": "2019-12-01T02:09:38.228373Z"
    }
   },
   "outputs": [],
   "source": [
    "#append processing results to csv:\n",
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T02:25:42.516120Z",
     "start_time": "2019-12-01T02:09:46.793456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out7 = process_data(alr_list[6001:7001], alrpath)\n",
    "res = pd.DataFrame(out7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T02:25:48.637885Z",
     "start_time": "2019-12-01T02:25:42.522711Z"
    }
   },
   "outputs": [],
   "source": [
    "#append processing results to csv:\n",
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T02:36:02.505551Z",
     "start_time": "2019-12-01T02:25:48.640272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out8 = process_data(alr_list[7001:8001], alrpath)\n",
    "res = pd.DataFrame(out8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T02:36:08.149934Z",
     "start_time": "2019-12-01T02:36:02.507910Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False) #better way to automate this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T02:47:22.647282Z",
     "start_time": "2019-12-01T02:36:08.152307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out9 = process_data(alr_list[8001:9001], alrpath)\n",
    "res = pd.DataFrame(out9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T02:47:28.617141Z",
     "start_time": "2019-12-01T02:47:22.656413Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T03:02:29.322763Z",
     "start_time": "2019-12-01T02:47:28.619889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out10 = process_data(alr_list[9001:10001], alrpath)\n",
    "res = pd.DataFrame(out10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T03:02:38.976956Z",
     "start_time": "2019-12-01T03:02:29.344375Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T03:23:38.616898Z",
     "start_time": "2019-12-01T03:02:38.985496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out11 = process_data(alr_list[10001:11001], alrpath)\n",
    "res = pd.DataFrame(out11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T03:23:46.964884Z",
     "start_time": "2019-12-01T03:23:38.632153Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T03:42:37.153009Z",
     "start_time": "2019-12-01T03:23:46.981928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out12 = process_data(alr_list[11001:12001], alrpath)\n",
    "res = pd.DataFrame(out12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T03:42:47.654205Z",
     "start_time": "2019-12-01T03:42:37.172986Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)  #1/3 of large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T04:05:57.864627Z",
     "start_time": "2019-12-01T03:46:24.268799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out13 = process_data(alr_list[12001:13001], alrpath)\n",
    "res = pd.DataFrame(out13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T04:06:07.204994Z",
     "start_time": "2019-12-01T04:05:57.882090Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#Nov 30 8:30pm start again from 14k, 0-13k csv already 249MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T04:44:01.980057Z",
     "start_time": "2019-12-01T04:29:13.786454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out14 = process_data(alr_list[13001:14001], alrpath) #stuck on this for 30mins without printing anything - why?\n",
    "res = pd.DataFrame(out14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T04:44:12.134176Z",
     "start_time": "2019-12-01T04:44:01.993259Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T05:00:28.036700Z",
     "start_time": "2019-12-01T04:44:12.137438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out15 = process_data(alr_list[14001:15001], alrpath)\n",
    "res = pd.DataFrame(out15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T05:00:36.283913Z",
     "start_time": "2019-12-01T05:00:28.040665Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T05:14:58.962452Z",
     "start_time": "2019-12-01T05:01:02.323778Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out16 = process_data(alr_list[15001:16001], alrpath)\n",
    "res = pd.DataFrame(out16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T05:15:06.651846Z",
     "start_time": "2019-12-01T05:14:58.972731Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T05:29:36.719357Z",
     "start_time": "2019-12-01T05:15:06.658310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out17 = process_data(alr_list[16001:17001], alrpath)\n",
    "res = pd.DataFrame(out17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T05:29:44.765412Z",
     "start_time": "2019-12-01T05:29:36.738551Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False) #~1/2 of 250x250 dataset, csv 375MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dec1, 2nd half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:16:25.571745Z",
     "start_time": "2019-12-01T15:58:10.456730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out18 = process_data(alr_list[17001:18001], alrpath)\n",
    "res = pd.DataFrame(out18) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:16:33.748697Z",
     "start_time": "2019-12-01T16:16:25.581018Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:33:09.827281Z",
     "start_time": "2019-12-01T16:16:33.755353Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out19 = process_data(alr_list[18001:19001], alrpath)\n",
    "res = pd.DataFrame(out19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:33:18.166888Z",
     "start_time": "2019-12-01T16:33:09.833695Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:49:44.099474Z",
     "start_time": "2019-12-01T16:33:18.173153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out20 = process_data(alr_list[19001:20001], alrpath)\n",
    "res = pd.DataFrame(out20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:49:52.553660Z",
     "start_time": "2019-12-01T16:49:44.104816Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T17:06:18.616749Z",
     "start_time": "2019-12-01T16:49:52.557147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out21 = process_data(alr_list[20001:21001], alrpath)\n",
    "res = pd.DataFrame(out21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T17:06:26.530651Z",
     "start_time": "2019-12-01T17:06:18.623187Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T17:23:16.545643Z",
     "start_time": "2019-12-01T17:06:26.534868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out22 = process_data(alr_list[21001:22001], alrpath)\n",
    "res = pd.DataFrame(out22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T17:23:24.750560Z",
     "start_time": "2019-12-01T17:23:16.551000Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T17:39:35.425356Z",
     "start_time": "2019-12-01T17:23:24.760984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out23 = process_data(alr_list[22001:23001], alrpath)\n",
    "res = pd.DataFrame(out23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T17:39:44.298668Z",
     "start_time": "2019-12-01T17:39:35.428066Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T17:55:58.638244Z",
     "start_time": "2019-12-01T17:39:44.301211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out24 = process_data(alr_list[23001:24001], alrpath)\n",
    "res = pd.DataFrame(out24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T17:56:07.173579Z",
     "start_time": "2019-12-01T17:55:58.648627Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T18:13:12.603098Z",
     "start_time": "2019-12-01T17:56:07.178537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out25 = process_data(alr_list[24001:25001], alrpath)\n",
    "res = pd.DataFrame(out25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T18:13:19.871430Z",
     "start_time": "2019-12-01T18:13:12.607675Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T18:29:28.957591Z",
     "start_time": "2019-12-01T18:13:19.876978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out26 = process_data(alr_list[25001:26001], alrpath)\n",
    "res = pd.DataFrame(out26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T18:29:36.655844Z",
     "start_time": "2019-12-01T18:29:28.969698Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T18:45:38.363012Z",
     "start_time": "2019-12-01T18:29:36.660656Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out27 = process_data(alr_list[26001:27001], alrpath)\n",
    "res = pd.DataFrame(out27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T18:45:46.262215Z",
     "start_time": "2019-12-01T18:45:38.366498Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T19:02:07.567858Z",
     "start_time": "2019-12-01T18:45:46.269127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out28 = process_data(alr_list[27001:28001], alrpath)\n",
    "res = pd.DataFrame(out28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T19:02:15.705070Z",
     "start_time": "2019-12-01T19:02:07.572687Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T19:22:14.685030Z",
     "start_time": "2019-12-01T19:02:15.735544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out29 = process_data(alr_list[28001:29001], alrpath)\n",
    "res = pd.DataFrame(out29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T19:22:22.858472Z",
     "start_time": "2019-12-01T19:22:14.708973Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T19:38:35.005127Z",
     "start_time": "2019-12-01T19:22:22.862703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out30 = process_data(alr_list[29001:30001], alrpath)\n",
    "res = pd.DataFrame(out30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T19:38:42.726322Z",
     "start_time": "2019-12-01T19:38:35.011090Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T19:56:36.433293Z",
     "start_time": "2019-12-01T19:38:42.728796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out31 = process_data(alr_list[30001:31001], alrpath)\n",
    "res = pd.DataFrame(out31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T19:56:44.639509Z",
     "start_time": "2019-12-01T19:56:36.447075Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T20:12:50.478799Z",
     "start_time": "2019-12-01T19:56:44.721641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out32 = process_data(alr_list[31001:32001], alrpath)\n",
    "res = pd.DataFrame(out32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T20:13:00.778869Z",
     "start_time": "2019-12-01T20:12:50.497983Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T20:29:23.404630Z",
     "start_time": "2019-12-01T20:13:00.787008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out33 = process_data(alr_list[32001:33001], alrpath)\n",
    "res = pd.DataFrame(out33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T20:29:31.539720Z",
     "start_time": "2019-12-01T20:29:23.412192Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T20:44:25.986829Z",
     "start_time": "2019-12-01T20:29:31.543503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out34 = process_data(alr_list[33001:34001], alrpath)\n",
    "res = pd.DataFrame(out34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T20:44:33.106154Z",
     "start_time": "2019-12-01T20:44:25.994377Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T21:00:08.403059Z",
     "start_time": "2019-12-01T20:44:33.118794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100 out of 1000 done\n",
      "200 out of 1000 done\n",
      "300 out of 1000 done\n",
      "400 out of 1000 done\n",
      "500 out of 1000 done\n",
      "600 out of 1000 done\n",
      "700 out of 1000 done\n",
      "800 out of 1000 done\n",
      "900 out of 1000 done\n",
      "1000 out of 1000 done\n"
     ]
    }
   ],
   "source": [
    "out35 = process_data(alr_list[34001:35001], alrpath)\n",
    "res = pd.DataFrame(out35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T21:00:16.703142Z",
     "start_time": "2019-12-01T21:00:08.411654Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T21:05:07.021530Z",
     "start_time": "2019-12-01T21:00:16.718505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298\n",
      "100 out of 298 done\n",
      "200 out of 298 done\n"
     ]
    }
   ],
   "source": [
    "out36 = process_data(alr_list[35001:], alrpath) #35299 images\n",
    "res = pd.DataFrame(out36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T21:05:09.592233Z",
     "start_time": "2019-12-01T21:05:07.027907Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('allreFeatures.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T23:53:20.208747Z",
     "start_time": "2019-12-01T23:53:13.270866Z"
    }
   },
   "outputs": [],
   "source": [
    "allrdata = open('allreFeatures.csv', 'r')\n",
    "datadf = pd.DataFrame(allrdata) #slow, data almost 1GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T23:53:20.208747Z",
     "start_time": "2019-12-01T23:53:13.270866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30298, 1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadf.shape #dimensions off?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T23:55:01.073140Z",
     "start_time": "2019-12-01T23:55:00.758232Z"
    }
   },
   "outputs": [],
   "source": [
    "allrdata1k = open('processed/allreFeatures1001.csv', 'r')\n",
    "datadf1k = pd.DataFrame(allrdata1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T23:55:10.245294Z",
     "start_time": "2019-12-01T23:55:09.919826Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                                                       0\n",
       "0     ,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,1...\n",
       "1     0,0.0,0.0,0.3309098482131958,0.021202772855758...\n",
       "2     1,0.0,0.0,0.0,0.0,0.0,0.0,5.261847019195557,0....\n",
       "3     2,0.0,0.0,0.0,0.0,0.0,0.0,2.614837408065796,0....\n",
       "4     3,0.0,0.0,0.0,0.0,3.464846134185791,0.0,0.3073...\n",
       "...                                                 ...\n",
       "997   996,0.0,0.0,0.0,0.0,0.0,0.26589658856391907,0....\n",
       "998   997,0.0,0.0,1.2459115982055664,0.0,0.0,0.0,2.3...\n",
       "999   998,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3....\n",
       "1000  999,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4.55414533...\n",
       "1001  1000,0.0,0.0,0.0,0.0,0.09837108850479126,0.0,0...\n",
       "\n",
       "[1002 rows x 1 columns]>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadf1k.info  #looks ok?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T23:55:18.679127Z",
     "start_time": "2019-12-01T23:55:18.673513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1002, 1)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadf1k.shape  #why not 1002 x 4096? separation eror when reading csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-30T00:09:34.749508Z",
     "start_time": "2019-11-30T00:09:34.731027Z"
    }
   },
   "outputs": [],
   "source": [
    "# def progressbar(it, prefix=\"\", size=60, file=sys.stdout):\n",
    "#     count = len(it)\n",
    "#     def show(j):\n",
    "#         x = int(size*j/count)\n",
    "#         file.write(\"%s[%s%s] %i/%i\\r\" % (prefix, \"#\"*x, \".\"*(size-x), j, count))\n",
    "#         file.flush()        \n",
    "#     show(0)\n",
    "#     for i, item in enumerate(it):\n",
    "#         yield item\n",
    "#         show(i+1)\n",
    "#     file.write(\"\\n\")\n",
    "#     file.flush()\n",
    "    \n",
    "    #progressbar really slowing things down! print out iterations in processing ftn instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-30T00:11:00.482834Z",
     "start_time": "2019-11-30T00:09:34.753281Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing: [##......................................] 1/20\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-4e0f69a17528>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#out1 = process_data(imglist,directory)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mout1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbc_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbcpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-4605baea8a81>\u001b[0m in \u001b[0;36mprocess_data\u001b[0;34m(imglist, directory)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m#print(filepath)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_pre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mout1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/anaconda/envs/metis/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1399\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1401\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/Library/anaconda/envs/metis/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/anaconda/envs/metis/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/anaconda/envs/metis/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/anaconda/envs/metis/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for i in progressbar(range(20), \"Computing: \", 40):\n",
    "\n",
    "# #out1 = process_data(imglist,directory)\n",
    "#     out1 = process_data(bbc_list[:100], bbcpath)\n",
    "#     res = pd.DataFrame(out1)\n",
    "    \n",
    "    #local:\n",
    "    #~0.66s per image, expect ~1min for 100 images, ~24min for 2k images? \n",
    "    #already took 2mins for 1/20 = 5 images, although right now I'm appending 100 images to dataframe. Memory pressure looks ok though\n",
    "    #took 22mins for 100 images, not proportional? \n",
    "    \n",
    "    #AWS also slower than expected? 1min for 5 images (twice as fast as local, consistent with prev)\n",
    "    \n",
    "    #activated tensorflow environment, still taking 1min for first 5 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-30T00:07:49.721771Z",
     "start_time": "2019-11-30T00:07:49.684023Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if file has already started it starts on next image, otherwise, at the beginning\n",
    "list_names = !ls\n",
    "if 'Features.csv' in list_names:\n",
    "    df = pd.read_csv('Features.csv')\n",
    "    shoe_num_start = df.shape[0]\n",
    "else:\n",
    "    shoe_num_start = 1\n",
    "shoe_num_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-30T00:07:50.684388Z",
     "start_time": "2019-11-30T00:07:49.725915Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-6c1dc1bc329c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msavetopath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'bbcFeatures100.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mshoe_num_start\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mshoe_num_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshoe_num_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshoe_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#to start where it left off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val' is not defined"
     ]
    }
   ],
   "source": [
    "savetopath = 'processed/'\n",
    "\n",
    "#append processing results to csv:\n",
    "if shoe_num_start==1:\n",
    "    res.to_csv(savetopath + 'bbcFeatures100.csv')\n",
    "else:\n",
    "    with open(savetopath + 'bbcFeatures100.csv', 'a') as f:\n",
    "        res.to_csv(f, header = False)\n",
    "shoe_num_start+=val\n",
    "shoe_num_start = min(shoe_num_start,len(shoe_number)) #to start where it left off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 10\n",
    "\n",
    "while shoe_num_start<=len(bbc_list):\n",
    "# for i in range(shoe_num_start,shoe_num_start+val):#range(1,int(np.ceil(len(shoe_number)/val))):\n",
    "    start = shoe_num_start\n",
    "    end = min(start+val,len(bbc_list))\n",
    "    print(start,end)\n",
    "    out1 = process_data(bbc_list[:20], bbcpath)\n",
    "    res = pd.DataFrame(out1)\n",
    "#   res['shoe_number'] = pd.DataFrame(list(range(start,end)))\n",
    "#  res.set_index('shoe_number', inplace=True)\n",
    "\n",
    "    if shoe_num_start==1:\n",
    "        res.to_csv('Features.csv')\n",
    "    else:\n",
    "        with open('Features.csv', 'a') as f:\n",
    "            res.to_csv(f, header = False)\n",
    "    shoe_num_start+=val\n",
    "    shoe_num_start = min(shoe_num_start,len(shoe_number)) #to start where it left off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T22:39:56.455398Z",
     "start_time": "2019-11-29T22:39:56.449285Z"
    }
   },
   "outputs": [],
   "source": [
    "#or skip for now? images not numbered\n",
    "\n",
    "# val = 10\n",
    "\n",
    "# while shoe_num_start<=len(shoe_number):\n",
    "# # for i in range(shoe_num_start,shoe_num_start+val):#range(1,int(np.ceil(len(shoe_number)/val))):\n",
    "#     start = shoe_num_start\n",
    "#     end = min(start+val,len(shoe_number))\n",
    "#     print(start,end)\n",
    "#     out1 = process_data(start, end)\n",
    "#     res = pd.DataFrame(out1)\n",
    "#     res['shoe_number'] = pd.DataFrame(list(range(start,end)))\n",
    "#     res.set_index('shoe_number', inplace=True)\n",
    "\n",
    "#     if shoe_num_start==1:\n",
    "#         res.to_csv('Features.csv')\n",
    "#     else:\n",
    "#         with open('Features.csv', 'a') as f:\n",
    "#             res.to_csv(f, header = False)\n",
    "#     shoe_num_start+=val #to start where it left off\n",
    "#     shoe_num_start = min(shoe_num_start,len(shoe_number))  #*this comparison is to get edge cases  \n",
    "                                                            #- if num =12, val=10, 10,20 can go over and miss final (12),\n",
    "                                                            #compare and choose smaller of the two\n",
    "    \n",
    "    #workaround for not having an image number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
