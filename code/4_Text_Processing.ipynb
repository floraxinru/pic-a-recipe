{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:31:57.279120Z",
     "start_time": "2019-12-09T00:31:50.069964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # TF-IDF\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import LancasterStemmer, PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy #for faster tokenization and lemmatization\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "#import project4_functions - rewrite?\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:31:57.360663Z",
     "start_time": "2019-12-09T00:31:57.345205Z"
    }
   },
   "outputs": [],
   "source": [
    "#also stored in sqlite database?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:31:59.869034Z",
     "start_time": "2019-12-09T00:31:57.370785Z"
    }
   },
   "outputs": [],
   "source": [
    "bbcdata = pd.read_json('/Users/xinrucheng/Documents/Metis_bootcamp/week_9/project5data/2017_140k/bbccouk-recipes.json', lines = True)\n",
    "#ValueError: Trailing data\n",
    "#JSON file has multiple lines, lines = True, or for loop:\n",
    "#with open('your.json', 'rb') as f:\n",
    "#    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:31:59.920246Z",
     "start_time": "2019-12-09T00:31:59.872158Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10599, 17)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbcdata.shape\n",
    "#has more recipes than image set! 10600, only 2225 have images\n",
    "#photo url doesn't work, but recipe url does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:00.073645Z",
     "start_time": "2019-12-09T00:31:59.937317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chef</th>\n",
       "      <th>chef_id</th>\n",
       "      <th>cooking_time_minutes</th>\n",
       "      <th>description</th>\n",
       "      <th>error</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>instructions</th>\n",
       "      <th>instructions_detailed</th>\n",
       "      <th>photo_url</th>\n",
       "      <th>preparation_time_minutes</th>\n",
       "      <th>program</th>\n",
       "      <th>program_id</th>\n",
       "      <th>serves</th>\n",
       "      <th>time_scraped</th>\n",
       "      <th>title</th>\n",
       "      <th>total_time_minutes</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>James Tanner</td>\n",
       "      <td>james_tanner</td>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[250g/8¾oz plain flour, plus extra for dusting...</td>\n",
       "      <td>[Preheat the oven to 200C/400F/Gas 6., For the...</td>\n",
       "      <td>[{'ingredient': 'plain flour', 'line': '250g/8...</td>\n",
       "      <td>None</td>\n",
       "      <td>30</td>\n",
       "      <td>Ready Steady Cook</td>\n",
       "      <td>b006vcgr</td>\n",
       "      <td>1</td>\n",
       "      <td>1499227761</td>\n",
       "      <td>Ten-minute pizza</td>\n",
       "      <td>40</td>\n",
       "      <td>http://bbc.co.uk/food/recipes/10minutepizza_87314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Mary Berry</td>\n",
       "      <td>mary_berry</td>\n",
       "      <td>0</td>\n",
       "      <td>This is my standby pasta supper as it is so de...</td>\n",
       "      <td>False</td>\n",
       "      <td>[350g/12oz penne pasta, 2 x 80g/3oz packs Parm...</td>\n",
       "      <td>[Cook the pasta in a pan of boiling salted wat...</td>\n",
       "      <td>[{'ingredient': 'pasta', 'line': '350g/12oz pe...</td>\n",
       "      <td>http://ichef.bbci.co.uk/food/ic/food_16x9_608/...</td>\n",
       "      <td>30</td>\n",
       "      <td>Mary Berry Cooks</td>\n",
       "      <td>p01s4q10</td>\n",
       "      <td>6</td>\n",
       "      <td>1499227763</td>\n",
       "      <td>15 minute pasta</td>\n",
       "      <td>30</td>\n",
       "      <td>http://bbc.co.uk/food/recipes/15_minute_pasta_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Mari Williams</td>\n",
       "      <td>mari_williams</td>\n",
       "      <td>0</td>\n",
       "      <td>Simple 3D iced biscuits inspired by Bake Off. ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[200g/7oz unsalted soft butter, 200g/7oz caste...</td>\n",
       "      <td>[To make the basic dough, line a baking tray w...</td>\n",
       "      <td>[{'ingredient': 'butter', 'line': '200g/7oz un...</td>\n",
       "      <td>http://ichef.bbci.co.uk/food/ic/food_16x9_608/...</td>\n",
       "      <td>30</td>\n",
       "      <td>The Great British Bake Off</td>\n",
       "      <td>b013pqnm</td>\n",
       "      <td>0</td>\n",
       "      <td>1499227766</td>\n",
       "      <td>3D biscuits</td>\n",
       "      <td>30</td>\n",
       "      <td>http://bbc.co.uk/food/recipes/3d_biscuits_29555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Justine Pattison</td>\n",
       "      <td>justine_pattison</td>\n",
       "      <td>0</td>\n",
       "      <td>This easy turkey crown recipe is served with s...</td>\n",
       "      <td>False</td>\n",
       "      <td>[2.5kg/5lb 8oz turkey crown (fully thawed if f...</td>\n",
       "      <td>[Preheat the oven to 220C/200C Fan/Gas 7., For...</td>\n",
       "      <td>[{'ingredient': 'turkey', 'line': '2.5kg/5lb 8...</td>\n",
       "      <td>http://ichef.bbci.co.uk/food/ic/food_16x9_608/...</td>\n",
       "      <td>30</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>1499227765</td>\n",
       "      <td>2-hour Christmas dinner</td>\n",
       "      <td>30</td>\n",
       "      <td>http://bbc.co.uk/food/recipes/2_hour_christmas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Ainsley Harriott</td>\n",
       "      <td>ainsley_harriott</td>\n",
       "      <td>0</td>\n",
       "      <td>Rib-eye is one of the most flavoursome cuts of...</td>\n",
       "      <td>False</td>\n",
       "      <td>[2.25kg/5lb rib-eye of beef, boned and rolled,...</td>\n",
       "      <td>[Place the rib-eye of beef into a large non-me...</td>\n",
       "      <td>[{'ingredient': None, 'line': '2.25kg/5lb rib-...</td>\n",
       "      <td>None</td>\n",
       "      <td>120</td>\n",
       "      <td>Great British Food Revival</td>\n",
       "      <td>b016pbs9</td>\n",
       "      <td>8</td>\n",
       "      <td>1499227769</td>\n",
       "      <td>Mustard and thyme crusted rib-eye of beef</td>\n",
       "      <td>120</td>\n",
       "      <td>http://bbc.co.uk/food/recipes/_81487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               chef           chef_id  cooking_time_minutes  \\\n",
       "0      James Tanner      james_tanner                    10   \n",
       "1        Mary Berry        mary_berry                     0   \n",
       "2     Mari Williams     mari_williams                     0   \n",
       "3  Justine Pattison  justine_pattison                     0   \n",
       "4  Ainsley Harriott  ainsley_harriott                     0   \n",
       "\n",
       "                                         description  error  \\\n",
       "0                                                     False   \n",
       "1  This is my standby pasta supper as it is so de...  False   \n",
       "2  Simple 3D iced biscuits inspired by Bake Off. ...  False   \n",
       "3  This easy turkey crown recipe is served with s...  False   \n",
       "4  Rib-eye is one of the most flavoursome cuts of...  False   \n",
       "\n",
       "                                         ingredients  \\\n",
       "0  [250g/8¾oz plain flour, plus extra for dusting...   \n",
       "1  [350g/12oz penne pasta, 2 x 80g/3oz packs Parm...   \n",
       "2  [200g/7oz unsalted soft butter, 200g/7oz caste...   \n",
       "3  [2.5kg/5lb 8oz turkey crown (fully thawed if f...   \n",
       "4  [2.25kg/5lb rib-eye of beef, boned and rolled,...   \n",
       "\n",
       "                                        instructions  \\\n",
       "0  [Preheat the oven to 200C/400F/Gas 6., For the...   \n",
       "1  [Cook the pasta in a pan of boiling salted wat...   \n",
       "2  [To make the basic dough, line a baking tray w...   \n",
       "3  [Preheat the oven to 220C/200C Fan/Gas 7., For...   \n",
       "4  [Place the rib-eye of beef into a large non-me...   \n",
       "\n",
       "                               instructions_detailed  \\\n",
       "0  [{'ingredient': 'plain flour', 'line': '250g/8...   \n",
       "1  [{'ingredient': 'pasta', 'line': '350g/12oz pe...   \n",
       "2  [{'ingredient': 'butter', 'line': '200g/7oz un...   \n",
       "3  [{'ingredient': 'turkey', 'line': '2.5kg/5lb 8...   \n",
       "4  [{'ingredient': None, 'line': '2.25kg/5lb rib-...   \n",
       "\n",
       "                                           photo_url  \\\n",
       "0                                               None   \n",
       "1  http://ichef.bbci.co.uk/food/ic/food_16x9_608/...   \n",
       "2  http://ichef.bbci.co.uk/food/ic/food_16x9_608/...   \n",
       "3  http://ichef.bbci.co.uk/food/ic/food_16x9_608/...   \n",
       "4                                               None   \n",
       "\n",
       "   preparation_time_minutes                     program program_id  serves  \\\n",
       "0                        30           Ready Steady Cook   b006vcgr       1   \n",
       "1                        30            Mary Berry Cooks   p01s4q10       6   \n",
       "2                        30  The Great British Bake Off   b013pqnm       0   \n",
       "3                        30                        None       None       6   \n",
       "4                       120  Great British Food Revival   b016pbs9       8   \n",
       "\n",
       "   time_scraped                                       title  \\\n",
       "0    1499227761                            Ten-minute pizza   \n",
       "1    1499227763                             15 minute pasta   \n",
       "2    1499227766                                3D biscuits    \n",
       "3    1499227765                     2-hour Christmas dinner   \n",
       "4    1499227769  Mustard and thyme crusted rib-eye of beef    \n",
       "\n",
       "   total_time_minutes                                                url  \n",
       "0                  40  http://bbc.co.uk/food/recipes/10minutepizza_87314  \n",
       "1                  30  http://bbc.co.uk/food/recipes/15_minute_pasta_...  \n",
       "2                  30    http://bbc.co.uk/food/recipes/3d_biscuits_29555  \n",
       "3                  30  http://bbc.co.uk/food/recipes/2_hour_christmas...  \n",
       "4                 120               http://bbc.co.uk/food/recipes/_81487  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbcdata.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:00.204402Z",
     "start_time": "2019-12-09T00:32:00.086396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10599 entries, 0 to 10598\n",
      "Data columns (total 17 columns):\n",
      "chef                        10599 non-null object\n",
      "chef_id                     10599 non-null object\n",
      "cooking_time_minutes        10599 non-null int64\n",
      "description                 10599 non-null object\n",
      "error                       10599 non-null bool\n",
      "ingredients                 10599 non-null object\n",
      "instructions                10599 non-null object\n",
      "instructions_detailed       10599 non-null object\n",
      "photo_url                   2225 non-null object\n",
      "preparation_time_minutes    10599 non-null int64\n",
      "program                     9626 non-null object\n",
      "program_id                  9626 non-null object\n",
      "serves                      10599 non-null int64\n",
      "time_scraped                10599 non-null int64\n",
      "title                       10599 non-null object\n",
      "total_time_minutes          10599 non-null int64\n",
      "url                         10599 non-null object\n",
      "dtypes: bool(1), int64(5), object(11)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "bbcdata.info() #no nulls in text columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:00.230688Z",
     "start_time": "2019-12-09T00:32:00.207220Z"
    }
   },
   "outputs": [],
   "source": [
    "#drop columns, only keep title, description, ingredients, instructions, instructions_detailed (has ingredients) and url for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:00.293838Z",
     "start_time": "2019-12-09T00:32:00.239815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ingredient': 'pasta', 'line': '350g/12oz penne pasta'},\n",
       " {'ingredient': 'ham',\n",
       "  'line': '2 x 80g/3oz packs Parma ham, snipped into small pieces'},\n",
       " {'ingredient': 'chestnut mushrooms',\n",
       "  'line': '250g/9oz small brown chestnut mushrooms, halved or quartered'},\n",
       " {'ingredient': 'crème fraîche', 'line': '200g/7oz full-fat crème fraîche'},\n",
       " {'ingredient': 'Parmesan', 'line': '100g/3½oz Parmesan, grated'},\n",
       " {'ingredient': 'parsley', 'line': '2 tbsp chopped parsley'},\n",
       " {'ingredient': 'pepper', 'line': 'salt and pepper, to taste'},\n",
       " {'ingredient': 'salad', 'line': 'green salad'},\n",
       " {'ingredient': 'bread', 'line': 'crunchy bread'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbcdata.iloc[1]['instructions_detailed'] #looks like ingredients? saved as list of dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:00.335922Z",
     "start_time": "2019-12-09T00:32:00.311743Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['350g/12oz penne pasta',\n",
       " '2 x 80g/3oz packs Parma ham, snipped into small pieces',\n",
       " '250g/9oz small brown chestnut mushrooms, halved or quartered',\n",
       " '200g/7oz full-fat crème fraîche',\n",
       " '100g/3½oz Parmesan, grated',\n",
       " '2 tbsp chopped parsley',\n",
       " 'salt and pepper, to taste',\n",
       " 'green salad',\n",
       " 'crunchy bread']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbcdata.iloc[1]['ingredients'] #easier to clean than the above, use this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:00.355235Z",
     "start_time": "2019-12-09T00:32:00.339205Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cook the pasta in a pan of boiling salted water according to the packet instructions. Drain and set aside',\n",
       " 'Heat a frying pan until hot. Add the pieces of Parma ham and fry until crisp, remove half of the ham onto a plate and set aside. Add the mushrooms to the pan and fry for two minutes. Add the crème fraîche and bring up to the boil. Add the pasta, Parmesan and parsley and toss together over the heat. Season well with salt and pepper.',\n",
       " 'Serve with a green salad and crunchy bread.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbcdata.iloc[1]['instructions'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get topics \n",
    "### Vectorize recipe titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:00.385983Z",
     "start_time": "2019-12-09T00:32:00.358017Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = bbcdata['title']\n",
    "type(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:00.421687Z",
     "start_time": "2019-12-09T00:32:00.392761Z"
    }
   },
   "outputs": [],
   "source": [
    "#remove digits\n",
    "def regex_nodigits(s):\n",
    "    '''use regex to clean string: \n",
    "    get rid of numbers, punctuations and capitalized letters''' \n",
    "    s = re.sub(r'[\\d]','',str(s).lower())#convert to lower case, replace digits with empty str\n",
    "    s = re.sub(r'[^a-z]',' ',str(s))#replace puctuations with space\n",
    "    return s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:00.929393Z",
     "start_time": "2019-12-09T00:32:00.426345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              ten minute pizza\n",
       "1                                  minute pasta\n",
       "2                                   d biscuits \n",
       "3                         hour christmas dinner\n",
       "4    mustard and thyme crusted rib eye of beef \n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_t=titles.apply(regex_nodigits)\n",
    "cleaned_t.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:00.953886Z",
     "start_time": "2019-12-09T00:32:00.937939Z"
    }
   },
   "outputs": [],
   "source": [
    "#countVectorizer \n",
    "#Converts a collection of text documents to a matrix of token counts\n",
    "#input is expected to be the sequence strings or bytes items are expected to be analyzed directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:00.975542Z",
     "start_time": "2019-12-09T00:32:00.961666Z"
    }
   },
   "outputs": [],
   "source": [
    "#start with standard nltk stopwords\n",
    "stopwords_nltk = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:01.269256Z",
     "start_time": "2019-12-09T00:32:00.985602Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aberdeen', 'abundant', 'acacia', 'ackee', 'adaptable', 'ade', 'adhraki', 'adobo', 'adraki', 'aduki', 'affogato', 'african', 'aged', 'aglio', 'agnello', 'ahead', 'aioli', 'ais', 'aji', 'ajillo', 'ajitsuke', 'ajo', 'ajwain', 'al', 'alaska', 'albanian', 'albert', 'albondigas', 'alcoholic', 'ale', 'alert', 'alfredo', 'alla', 'alle', 'alloro', 'allspice', 'almond', 'almondine', 'almonds', 'almondy', 'aloo', 'alpine', 'alsatian', 'alternative', 'amann', 'amaretti', 'amaretto', 'amarillo', 'amatriciana', 'amazing', 'ambassadeur', 'ambassador', 'amber', 'american', 'amoul', 'anatra', 'ancho', 'anchovade', 'anchovies', 'anchovy', 'ancienne', 'andalucian', 'angel', 'angela', 'anglais', 'anglaise', 'anglesey', 'anguilla', 'anise', 'aniseed', 'annika', 'anolini', 'antony', 'anya', 'aperitivo', 'apple', 'apples', 'apricot', 'apricots', 'arabian', 'aragon', 'arancine', 'arancini', 'arbequina', 'arbroath', 'arctic', 'arlettes', 'armagnac', 'arnaise', 'arnold', 'aromatic', 'arrabbiata', 'arrabiata', 'arrabiatta', 'arroz', 'arselle', 'artichoke', 'artichokes', 'asaparagus', 'asian', 'asparagus', 'aster', 'atar', 'au', 'aubergine', 'aubergines', 'aussie', 'authentic', 'autumn', 'auvergne', 'aux', 'avgolemono', 'avial', 'avocado', 'avocados', 'ayam', 'aztec', 'baba', 'babas', 'babka', 'baby', 'baccal', 'back', 'bacon', 'bag', 'bagel', 'bagels', 'bagna', 'bagnat', 'baguette', 'baguettes', 'bake', 'baked', 'bakes', 'bakewell', 'baklava', 'balinese', 'ball', 'balls', 'balm', 'balsamic', 'balti', 'baltic', 'bamboo', 'banana', 'bananas', 'bandera', 'bang', 'bangers', 'bangladeshi', 'banh', 'banoffee', 'bao', 'bar', 'bara', 'barbary', 'barbecue', 'barbecued', 'barbied', 'barcoletta', 'barigoule', 'bark', 'barley', 'barmbrack', 'barnsley', 'barquettes', 'barramundi', 'barrel', 'bars', 'base', 'bashed', 'basic', 'basil', 'basket', 'baskets', 'basmati', 'basque', 'bass', 'bastilla', 'batons', 'battenberg', 'batter', 'battered', 'bauble', 'baubles', 'bavarian', 'bavette', 'bay', 'bayonne', 'bbq', 'beachside', 'bean', 'beans', 'beansprout', 'beansprouts', 'beany', 'bearnaise', 'beccafico', 'bed', 'beef', 'beefburger', 'beehive', 'beer', 'beet', 'beetroot', 'beetroots', 'beets', 'begendi', 'beggars', 'beignets', 'belgian', 'believe', 'bellini', 'belly', 'belts', 'ben', 'benedict', 'bengal', 'bengali', 'bennett', 'bergamot', 'berkswell', 'berries', 'berry', 'besan', 'bessara', 'best', 'beurre', 'bhaji', 'bhajia', 'bhajis', 'bhindi', 'bhuna', 'bi', 'bianco', 'biber', 'bierocks', 'big', 'bigarade', 'bigos', 'biker', 'bikers', 'bill', 'bing', 'bircher', 'birthday', 'biryani', 'biscotti', 'biscuit', 'biscuits', 'bishop', 'bisque', 'bissau', 'bite', 'bites', 'bitter', 'bitterballen', 'black', 'blackbean', 'blackberries', 'blackberry', 'blackcurrant', 'blackcurrants', 'blackened', 'blackeye', 'blackwater', 'blade', 'blanc', 'blancmange', 'blancs', 'blanket', 'blankets', 'blanquette', 'bleu', 'blini', 'blinis', 'blondies', 'blood', 'bloody', 'bloomer', 'blow', 'blue', 'blueberries', 'blueberry', 'blush', 'blushed', 'boar', 'board', 'boat', 'boats', 'bobotie', 'boeuf', 'boiled', 'bois', 'bok', 'bokkeum', 'bolinhos', 'bologna', 'bolognaise', 'bolognese', 'bomb', 'bombay', 'bombs', 'bonbons', 'bone', 'bonfire', 'boozy', 'borage', 'bordelaise', 'boreks', 'borlotti', 'borsch', 'borscht', 'boston', 'boti', 'bottarga', 'boudran', 'bouillabaisse', 'boulang', 'boulangere', 'bourbon', 'bourguignon', 'bourride', 'bowl', 'bowls', 'bows', 'box', 'boxing', 'boxty', 'boys', 'br', 'braciole', 'braised', 'bramble', 'bramley', 'bran', 'branches', 'brandade', 'brandied', 'brandy', 'branzino', 'brassicas', 'bratwurst', 'bravas', 'brazil', 'brazils', 'bread', 'breadcrumb', 'breadcrumbed', 'breadcrumbs', 'breaded', 'breads', 'breadsticks', 'breakfast', 'bream', 'breast', 'breasts', 'bredie', 'bresaola', 'brest', 'bretonne', 'brick', 'bridie', 'brie', 'brik', 'brill', 'brined', 'brioche', 'brioches', 'brisbane', 'brisket', 'brith', 'british', 'brittle', 'brixton', 'broad', 'broccoli', 'brochette', 'brodo', 'broth', 'brothy', 'brown', 'brownie', 'brownies', 'browns', 'brunch', 'bruno', 'bruschetta', 'bruschettas', 'brushed', 'brussels', 'bubble', 'buccaneer', 'bucco', 'bucket', 'buckwheat', 'buco', 'buddha', 'buerre', 'buffalo', 'bulgar', 'bulgogi', 'bulgur', 'buljol', 'bun', 'bundles', 'bundt', 'bunny', 'buns', 'bunski', 'bunting', 'burger', 'burgers', 'burn', 'burnt', 'burrata', 'burrida', 'burrito', 'bursts', 'busara', 'butter', 'butterbean', 'butterbeans', 'buttercream', 'buttered', 'butterflied', 'butterfly', 'butteries', 'buttermilk', 'butternut', 'butters', 'butterscotch', 'buttery', 'butties', 'button', 'butty', 'cabbage', 'cabbge', 'cacciatora', 'cacciatore', 'cacio', 'caerphilly', 'caesar', 'cafriela', 'caipirosca', 'cajun', 'cake', 'cakes', 'calabrese', 'calamari', 'calda', 'caldeirada', 'caldo', 'calf', 'californian', 'callaloo', 'calvados', 'calves', 'calypso', 'calzone', 'cambozola', 'camembert', 'canadian', 'canap', 'canapes', 'cancalaise', 'candied', 'candy', 'cane', 'cannellini', 'cannelloni', 'cannon', 'canterbury', 'cantonese', 'cao', 'cape', 'caper', 'capers', 'caponata', 'cappellacci', 'capperi', 'cappuccino', 'caprese', 'capricorn', 'captain', 'caramel', 'caramelised', 'caramelized', 'caramels', 'caraway', 'carbonara', 'carbonnade', 'cardamom', 'cardamon', 'caribbean', 'carlina', 'carlo', 'carne', 'carol', 'carpaccio', 'carpet', 'carrot', 'carrots', 'carrozza', 'case', 'cashel', 'cashew', 'cashews', 'cassava', 'casserole', 'cassis', 'cassoulet', 'catalan', 'catalonian', 'cauda', 'cauliflower', 'cavalo', 'cavatelli', 'caviar', 'cavolo', 'cawl', 'cayenne', 'celariac', 'celebration', 'celeriac', 'celery', 'celtic', 'centred', 'cep', 'ceps', 'ceviche', 'chaap', 'chaat', 'chai', 'cham', 'chamel', 'champ', 'champagne', 'chana', 'chantenay', 'chanterelle', 'chanterelles', 'chantilly', 'chapati', 'chapatis', 'chapel', 'chappatis', 'char', 'charcoal', 'charcuti', 'chard', 'chardonnay', 'charentais', 'chargrilled', 'charlotte', 'charlottes', 'charred', 'chartreuse', 'chashu', 'chasseur', 'chat', 'chateaubriand', 'chaussons', 'chavignol', 'che', 'cheat', 'cheats', 'cheaty', 'cheddar', 'cheek', 'cheeks', 'cheese', 'cheeseburger', 'cheeseburgers', 'cheesecake', 'cheesecakes', 'cheeses', 'cheesesteak', 'cheesy', 'chef', 'chelsea', 'chemmeen', 'chen', 'chermoula', 'cherries', 'cherry', 'chervil', 'cheshire', 'chestnut', 'chestnuts', 'chettinad', 'chewy', 'chia', 'chicken', 'chickens', 'chickpea', 'chickpeas', 'chicory', 'chiffon', 'childhood', 'chili', 'chilled', 'chilli', 'chillies', 'chimichanga', 'chimichurri', 'chimneys', 'chinese', 'ching', 'chingri', 'chip', 'chipolatas', 'chipotle', 'chips', 'chiu', 'chive', 'chives', 'chlodnik', 'chocolat', 'chocolate', 'chocolates', 'choi', 'cholent', 'chop', 'chopped', 'chops', 'chorizo', 'choron', 'choucroute', 'choux', 'chow', 'chowder', 'choy', 'chris', 'christian', 'christmas', 'christophine', 'chump', 'chunks', 'chunky', 'chupe', 'churn', 'churros', 'chutney', 'ciabatta', 'ciambellone', 'cider', 'cinder', 'cinnamon', 'cipolle', 'citron', 'citrus', 'clafoutis', 'clair', 'clairs', 'clam', 'clams', 'clap', 'clapshot', 'classic', 'claws', 'clear', 'cleaves', 'clement', 'clementine', 'clementines', 'clements', 'clock', 'clootie', 'clotted', 'cloud', 'clove', 'cloves', 'club', 'coarse', 'coated', 'cob', 'cobbled', 'cobbler', 'cobnut', 'cobnuts', 'cock', 'cockle', 'cockles', 'cocktail', 'coco', 'cocoa', 'coconut', 'cocotte', 'cod', 'coddled', 'codfish', 'coffee', 'coins', 'cointreau', 'cola', 'colada', 'colatura', 'colcannon', 'cold', 'coleslaw', 'coley', 'collar', 'colonel', 'colossal', 'comforting', 'comp', 'compost', 'compote', 'con', 'concasse', 'cond', 'confit', 'consomm', 'contaldo', 'continental', 'cook', 'cooked', 'cooker', 'cookie', 'cookies', 'cooler', 'coorg', 'coq', 'coquilles', 'cordial', 'cordon', 'coriander', 'corn', 'cornbread', 'corned', 'cornish', 'cornmeal', 'coronation', 'cos', 'cote', 'cotswold', 'cotta', 'cottage', 'cou', 'coulibiac', 'coulis', 'country', 'courgette', 'courgettes', 'courgetti', 'couronne', 'couscous', 'cow', 'coxes', 'cr', 'crab', 'crabcakes', 'crabs', 'cracked', 'crackers', 'crackle', 'crackling', 'cracklings', 'cranachan', 'cranberries', 'cranberry', 'crayfish', 'cream', 'creamed', 'creams', 'creamy', 'creation', 'crema', 'cremat', 'cremosa', 'creole', 'cress', 'crisp', 'crispbread', 'crisps', 'crispy', 'cro', 'crocodile', 'croissant', 'croissants', 'croque', 'croquembouche', 'croquetas', 'croquette', 'croquettes', 'cross', 'crostini', 'crottin', 'croustade', 'croustades', 'croustillant', 'croute', 'croutes', 'crouton', 'croutons', 'crowd', 'crown', 'crudite', 'crudites', 'crudo', 'crumb', 'crumbed', 'crumble', 'crumbled', 'crumbs', 'crumpet', 'crumpets', 'crunch', 'crunched', 'crunchies', 'crunchy', 'crush', 'crushed', 'crust', 'crusted', 'crusty', 'crystallised', 'csa', 'cuba', 'cuban', 'cucumber', 'cucumbers', 'cullen', 'cumberland', 'cumin', 'cup', 'cupboard', 'cupcakes', 'cups', 'curd', 'cured', 'curly', 'currant', 'currants', 'curranty', 'curried', 'currimbhoy', 'curry', 'custard', 'customised', 'cut', 'cutless', 'cutlet', 'cutlets', 'cuttlefish', 'cypriot', 'da', 'daab', 'dacquoise', 'dad', 'daikon', 'daiquiri', 'dal', 'damper', 'dampfnudel', 'damson', 'dandelion', 'dangs', 'danish', 'danishes', 'dappy', 'dark', 'darnes', 'dashi', 'date', 'dates', 'daube', 'dauphinoise', 'daurade', 'day', 'de', 'decker', 'deconstructed', 'deep', 'deer', 'definitive', 'delhi', 'delia', 'delice', 'delicious', 'delight', 'deluxe', 'dengaku', 'dentdale', 'dermot', 'derry', 'design', 'designer', 'dessert', 'devil', 'devilish', 'devilled', 'devils', 'devon', 'devonshire', 'dhal', 'dhansak', 'di', 'diamond', 'diamonds', 'diane', 'diced', 'dick', 'digger', 'diggers', 'dijon', 'dill', 'dillisk', 'dim', 'ding', 'dinner', 'dip', 'dipped', 'dipper', 'dippers', 'dipping', 'dips', 'dirty', 'disco', 'dish', 'diy', 'dobos', 'dolcelatte', 'dolmades', 'doner', 'dong', 'donuts', 'doorstep', 'dopiaza', 'doris', 'dory', 'dosa', 'double', 'dough', 'doughballs', 'doughnut', 'doughnuts', 'douglas', 'dovedale', 'dover', 'dowdy', 'dragon', 'dragoncella', 'drambuie', 'dream', 'dreamlike', 'dress', 'dressed', 'dressing', 'dried', 'drink', 'dripping', 'drizzle', 'drizzled', 'drizzly', 'drop', 'dropped', 'drowned', 'drumstick', 'drumsticks', 'drunken', 'dry', 'duchess', 'duchesse', 'duck', 'duckling', 'duff', 'duja', 'dukkah', 'dulce', 'dulse', 'dum', 'dumpling', 'dumplings', 'dundee', 'duo', 'dust', 'dusted', 'dwarf', 'ear', 'earl', 'ears', 'east', 'easter', 'eastern', 'easy', 'eccles', 'ed', 'edamame', 'eeda', 'eel', 'eels', 'egg', 'eggnog', 'eggs', 'eggy', 'egyptian', 'eight', 'el', 'elderberry', 'elderflower', 'elizabeth', 'elizabethan', 'emergency', 'emilion', 'emmental', 'empanada', 'empanadas', 'emulsion', 'en', 'enchiladas', 'encrusted', 'end', 'endive', 'energy', 'england', 'english', 'enoki', 'entremets', 'es', 'escabeche', 'escalope', 'escalopes', 'escargot', 'escarole', 'escovitch', 'espresso', 'essence', 'essex', 'estonian', 'eton', 'european', 'evans', 'eve', 'ever', 'everlasting', 'everyday', 'ewes', 'exceptional', 'exotic', 'express', 'expressed', 'extra', 'extraordinary', 'extravagant', 'eye', 'eyed', 'fa', 'fabulous', 'faced', 'faggot', 'faggots', 'fagioli', 'fail', 'fairy', 'fajita', 'fajitas', 'falafel', 'falafels', 'family', 'fancies', 'faraizi', 'farci', 'farfalle', 'farinata', 'farl', 'farls', 'farmhouse', 'farthing', 'faschierter', 'fashioned', 'fast', 'fat', 'fatless', 'fattoush', 'favourite', 'fazool', 'feast', 'feather', 'feathered', 'fed', 'feed', 'feet', 'fegato', 'feijoada', 'fennel', 'fenugreek', 'feria', 'fermented', 'fesenjoon', 'fest', 'festive', 'feta', 'fettuccine', 'feu', 'feuillete', 'fideu', 'fidget', 'field', 'fiery', 'fig', 'figgy', 'figs', 'filets', 'filled', 'fillet', 'filleto', 'fillets', 'filling', 'fillings', 'filly', 'filo', 'fin', 'financiers', 'find', 'fine', 'finely', 'finger', 'fingers', 'finished', 'finnish', 'finocchio', 'fior', 'fir', 'firecracker', 'fired', 'fish', 'fishcake', 'fishcakes', 'fisherman', 'fita', 'five', 'fix', 'fizz', 'fizzy', 'flageolet', 'flaked', 'flamb', 'flamed', 'flamenco', 'flan', 'flank', 'flaounes', 'flapjacks', 'flash', 'flat', 'flatbread', 'flatbreads', 'flatleaf', 'flattened', 'flavoured', 'flavours', 'flecked', 'fleur', 'flexible', 'float', 'floating', 'floral', 'florentine', 'florentines', 'florets', 'floss', 'flottante', 'flottantes', 'flour', 'flourless', 'flower', 'flowerpot', 'flowers', 'fluffy', 'flyers', 'foam', 'focaccia', 'foccacia', 'foil', 'fondant', 'fondants', 'fondue', 'fontina', 'foo', 'food', 'fool', 'foolproof', 'football', 'fore', 'forest', 'forestiere', 'form', 'forno', 'fortune', 'forty', 'foster', 'fougasse', 'four', 'fours', 'fowl', 'fra', 'fragrant', 'fraiche', 'frais', 'fraisier', 'fran', 'francais', 'frangipane', 'frango', 'freddo', 'free', 'freekeh', 'freewheeling', 'freeze', 'fregola', 'french', 'freschi', 'fresco', 'fresh', 'freshly', 'frestelse', 'fricass', 'fricasse', 'friday', 'fridge', 'fried', 'fries', 'frikadeller', 'fris', 'frites', 'frittata', 'frittatas', 'fritter', 'fritters', 'fritti', 'fritto', 'fromage', 'frond', 'frosted', 'frosting', 'froth', 'frozen', 'fruit', 'fruits', 'fruity', 'frutti', 'fry', 'frying', 'fu', 'fudge', 'fudgy', 'full', 'fum', 'funfetti', 'funghi', 'funky', 'fusilli', 'gado', 'gai', 'gala', 'galaktoboureko', 'galbi', 'galette', 'galettes', 'galician', 'galinha', 'gallega', 'gallina', 'gallo', 'gamberones', 'game', 'gamekeeper', 'gammon', 'ganache', 'ganoush', 'garage', 'garam', 'garden', 'garibaldi', 'garlic', 'garlicky', 'garnish', 'gateau', 'gazpacho', 'geisha', 'gelati', 'gelato', 'gem', 'general', 'genoa', 'genoise', 'genovese', 'gentleman', 'george', 'georgian', 'germain', 'german', 'getaway', 'gg', 'ghasemi', 'gherkin', 'ghoogra', 'ghost', 'ghotala', 'gianduia', 'giant', 'gift', 'gigantes', 'gilt', 'gilthead', 'gin', 'ginger', 'gingerbread', 'gingernut', 'girl', 'girolle', 'girolles', 'gl', 'glac', 'glam', 'glamming', 'glamorgan', 'glass', 'glaze', 'glazed', 'glitter', 'globe', 'glory', 'gloucester', 'gloucestershire', 'gluten', 'gnocchi', 'gnudi', 'goan', 'goat', 'goats', 'gobhi', 'gobi', 'godfather', 'goji', 'gold', 'golden', 'golek', 'gong', 'good', 'goodwood', 'gooey', 'goose', 'gooseberries', 'gooseberry', 'goosnargh', 'goreng', 'gorgonzola', 'gosht', 'goug', 'goujons', 'goulash', 'gourmet', 'graffiti', 'grain', 'grains', 'gran', 'grand', 'grandma', 'granger', 'granita', 'granny', 'grano', 'granola', 'granseola', 'grant', 'grape', 'grapefruit', 'grapes', 'grappa', 'grated', 'gratin', 'gratinated', 'gratins', 'gravad', 'gravadlax', 'gravalax', 'gravlax', 'gravy', 'great', 'greek', 'green', 'greengage', 'greengrocers', 'greens', 'gremolata', 'grenoble', 'grenobloise', 'gressingham', 'grey', 'gribiche', 'griddle', 'griddled', 'griglia', 'grill', 'grilled', 'griottine', 'grissini', 'grits', 'ground', 'grouse', 'gruy', 'gruyere', 'guacamole', 'guard', 'guava', 'guavas', 'gui', 'guilt', 'guinea', 'guinness', 'gujarati', 'gulab', 'gulasch', 'gulyas', 'gumbo', 'gunpowder', 'gurnard', 'gymkhana', 'gyoza', 'gypsy', 'ha', 'haddock', 'haggerty', 'haggis', 'hairy', 'haitian', 'hake', 'half', 'halibut', 'halloumi', 'halloween', 'halwa', 'ham', 'hand', 'handkerchief', 'hanger', 'hanncha', 'hanout', 'hara', 'hard', 'hardcore', 'haricot', 'harissa', 'harvest', 'harvey', 'hash', 'hasselback', 'hat', 'hawaiian', 'hay', 'hazelnut', 'hazelnuts', 'hazlenut', 'head', 'health', 'healthy', 'heart', 'hearts', 'hearty', 'heather', 'hedgehog', 'heel', 'heirloom', 'hemp', 'hen', 'henna', 'herb', 'herbed', 'herbelicious', 'herbs', 'herby', 'herd', 'hereford', 'herefordshire', 'heritage', 'herring', 'herrings', 'hidden', 'hide', 'highland', 'hispaniola', 'hispi', 'hive', 'hock', 'hocks', 'hogget', 'hoi', 'hoisin', 'hoki', 'hole', 'holes', 'hollandaise', 'hollywood', 'holstein', 'holy', 'home', 'homecoming', 'homemade', 'homestyle', 'homity', 'homme', 'honey', 'honeycomb', 'honeyed', 'honor', 'hopkinson', 'horns', 'horseback', 'horseradish', 'hot', 'hotdogs', 'hotpot', 'hounds', 'hour', 'house', 'huevos', 'hugh', 'hummus', 'hungarian', 'hunkar', 'husks', 'hyderabadi', 'ib', 'iberico', 'ice', 'iced', 'icelandic', 'ices', 'icing', 'iles', 'indian', 'individual', 'indo', 'indonesian', 'indulgence', 'indulgent', 'infused', 'inglese', 'ink', 'insalata', 'inspired', 'instant', 'intense', 'involtini', 'inzimino', 'iranian', 'irene', 'irish', 'isaan', 'ishtew', 'ishtu', 'island', 'islands', 'isle', 'israeli', 'issan', 'ital', 'italian', 'italiana', 'jack', 'jacket', 'jackets', 'jacob', 'jacques', 'jaffa', 'jaggery', 'jalape', 'jalapeno', 'jalfrezi', 'jalousie', 'jam', 'jamaican', 'jambalaya', 'james', 'jammy', 'jamon', 'jamun', 'jansson', 'january', 'japanese', 'jardiniere', 'jarlsberg', 'jasmine', 'jean', 'jellies', 'jelly', 'jenny', 'jerk', 'jersey', 'jerseys', 'jerusalem', 'jewel', 'jewelled', 'jhal', 'jhol', 'jian', 'joe', 'john', 'joint', 'jollof', 'jorge', 'josh', 'jowl', 'jug', 'juice', 'juices', 'julie', 'julienne', 'jumble', 'jumbo', 'jump', 'jungle', 'juniper', 'jus', 'ka', 'kabob', 'kachori', 'kachumber', 'kadaifi', 'kadhai', 'kadhi', 'kaffir', 'kai', 'kalamata', 'kale', 'kali', 'kanell', 'kapitan', 'kara', 'karara', 'karhai', 'kartoffelbrot', 'kasha', 'kashmiri', 'katsu', 'katy', 'kavaab', 'kebab', 'kebabs', 'kedgeree', 'kefta', 'kelp', 'kent', 'kentish', 'kerala', 'keralan', 'keralian', 'kernel', 'kerridge', 'ketchup', 'key', 'kha', 'khao', 'kharu', 'kheema', 'kheer', 'ki', 'kibbe', 'kichri', 'kick', 'kid', 'kidney', 'kidneys', 'kids', 'kiev', 'kievs', 'kimchi', 'kind', 'kinds', 'king', 'kipper', 'kippers', 'kisses', 'kitchen', 'kiteria', 'kites', 'kiwi', 'klippfisk', 'knickerbocker', 'knocking', 'knots', 'knuckle', 'kofta', 'koftas', 'kofte', 'kohlrabi', 'koji', 'koli', 'korean', 'korma', 'kosi', 'kouign', 'koulibiac', 'kozani', 'kransekake', 'krob', 'kroppkakor', 'kugelhopf', 'kulfi', 'kumquat', 'kumquats', 'la', 'labneh', 'lace', 'lacy', 'ladder', 'lady', 'lait', 'laksa', 'lamb', 'lambs', 'lancashire', 'landaise', 'langosi', 'langoustine', 'langoustines', 'languages', 'langues', 'lankan', 'lapsang', 'larb', 'lardo', 'lardons', 'lardy', 'large', 'lasagna', 'lasagne', 'lassi', 'last', 'late', 'latkes', 'latte', 'lattice', 'latvian', 'lau', 'lava', 'lavender', 'laverbread', 'lax', 'layer', 'layered', 'layers', 'lazy', 'lazybones', 'le', 'leaf', 'lean', 'leather', 'leaves', 'lebanese', 'lee', 'leek', 'leekie', 'leeks', 'leftover', 'leg', 'legs', 'legume', 'leicester', 'lemak', 'lemon', 'lemonade', 'lemongrass', 'lemons', 'lemony', 'lentil', 'lentils', 'lepur', 'les', 'less', 'lettuce', 'leves', 'levi', 'libre', 'licorice', 'lid', 'light', 'lighter', 'lightly', 'lime', 'limed', 'limes', 'limoncello', 'lincolnshire', 'ling', 'lingonberries', 'linguine', 'linguini', 'linseed', 'linzertorte', 'lion', 'liqueur', 'liquor', 'liquorice', 'little', 'liver', 'livers', 'lo', 'loaf', 'loaves', 'lobscouse', 'lobster', 'log', 'logs', 'loin', 'lokshen', 'lollies', 'lollipop', 'lollipops', 'lomo', 'london', 'londonderry', 'long', 'longhorn', 'lorraine', 'lotus', 'loubet', 'louisiana', 'lovage', 'love', 'lovers', 'low', 'lucia', 'lucy', 'luilakbollen', 'lunchbox', 'lune', 'luscious', 'lush', 'luxe', 'luxury', 'lychee', 'lychees', 'lyonnaise', 'mac', 'macadamia', 'macanese', 'macaroni', 'macarons', 'macaroon', 'macaroons', 'macchi', 'macchiato', 'mace', 'macerated', 'machchi', 'macher', 'mackerel', 'madagascan', 'madame', 'made', 'madeira', 'madeleines', 'maderia', 'madras', 'magic', 'mai', 'mainstay', 'maize', 'make', 'maki', 'mala', 'malai', 'malay', 'malaysian', 'maldivian', 'mallard', 'malone', 'malt', 'maltaise', 'malted', 'malvern', 'maman', 'mammole', 'mamon', 'mamoosa', 'man', 'manchego', 'manchester', 'mandarin', 'mandioca', 'mandorle', 'maneesh', 'mangalorean', 'mange', 'mangetout', 'mango', 'mangsho', 'maple', 'marble', 'marbled', 'margarita', 'margherita', 'marie', 'marinade', 'marinara', 'marinated', 'marini', 'mariniere', 'marjolaine', 'marjoram', 'marmalade', 'marmitako', 'marnier', 'marrons', 'marrow', 'mars', 'marsala', 'marsh', 'marshmallow', 'marshmallows', 'martin', 'martini', 'martinique', 'maru', 'marula', 'mary', 'maryland', 'marzano', 'marzipan', 'masala', 'mascarpone', 'mash', 'mashed', 'mashwi', 'masour', 'massaman', 'matcha', 'matchstick', 'matchsticks', 'matrimony', 'mature', 'may', 'mayan', 'mayo', 'mayonnaise', 'mcsingh', 'mead', 'meal', 'mealie', 'meat', 'meatball', 'meatballs', 'meatloaf', 'meatzza', 'meculin', 'medallions', 'medieval', 'mediterranean', 'medium', 'medjool', 'medlars', 'medley', 'mee', 'meen', 'megrim', 'mein', 'melagrana', 'melanzane', 'melba', 'mele', 'melon', 'melt', 'melted', 'melting', 'melton', 'melts', 'membrillo', 'men', 'menta', 'mer', 'merguez', 'meringue', 'meringues', 'merluza', 'merrett', 'merry', 'mess', 'meuni', 'mex', 'mexican', 'meze', 'mezze', 'mice', 'micro', 'microwave', 'middle', 'midnight', 'migas', 'milanese', 'miles', 'milk', 'milkshake', 'milkshakes', 'millefeuille', 'millefeuilles', 'millionaire', 'mince', 'minced', 'mincemeat', 'minervois', 'minestrone', 'mini', 'miniature', 'mint', 'minted', 'minty', 'minute', 'miracle', 'mirch', 'mirin', 'mirror', 'mirza', 'miso', 'mississippi', 'misto', 'mitzuna', 'mix', 'mixed', 'mizuna', 'mkawra', 'mo', 'mocha', 'mock', 'moist', 'mojito', 'mojo', 'mokatines', 'molasses', 'molcajete', 'mole', 'molee', 'molle', 'molly', 'molten', 'mom', 'moments', 'mongolian', 'monkey', 'monkfish', 'monmouth', 'monsieur', 'monster', 'mont', 'monte', 'montenebro', 'monterey', 'monts', 'mooli', 'moorish', 'morecambe', 'morel', 'morels', 'mornay', 'morning', 'moroccan', 'morocco', 'morrocan', 'morroccan', 'morston', 'morteau', 'moscito', 'mother', 'moules', 'moussaka', 'mousse', 'mousseline', 'mousseron', 'mouthwatering', 'mowbray', 'mozzarella', 'mr', 'mtland', 'mu', 'mud', 'muek', 'muesli', 'muffin', 'muffins', 'mug', 'muhallabi', 'mujaddarah', 'mulberry', 'mulled', 'mullet', 'mulligatawny', 'multi', 'mum', 'murg', 'murgh', 'murghi', 'murgi', 'muscat', 'muscovado', 'mushroom', 'mushrooms', 'mushy', 'musical', 'mussakhan', 'mussel', 'mussels', 'mustard', 'mustia', 'mutton', 'myoga', 'na', 'naan', 'naanwich', 'nachos', 'nadiya', 'nage', 'napoleons', 'napoletana', 'nargis', 'nasi', 'nasturtium', 'nasu', 'natalizio', 'navarin', 'nduja', 'neapolitan', 'neck', 'nectar', 'nectarine', 'nectarines', 'neeps', 'negra', 'neige', 'neopolitan', 'nero', 'nest', 'nests', 'nettle', 'nettles', 'never', 'new', 'ngd', 'ni', 'nibbles', 'nibs', 'nice', 'nicky', 'nigel', 'night', 'nimish', 'noci', 'nog', 'noise', 'noisette', 'noisettes', 'non', 'nonya', 'noodle', 'noodles', 'norfolk', 'north', 'northumberland', 'norwegian', 'nose', 'nougat', 'nourishing', 'novelty', 'nuggets', 'nuoc', 'nut', 'nutmeg', 'nuts', 'nutty', 'oat', 'oatcake', 'oatcakes', 'oatmeal', 'oats', 'oaty', 'ocopa', 'octopus', 'oeufs', 'offal', 'oil', 'oilves', 'oise', 'oisin', 'ojingeo', 'okra', 'old', 'oli', 'olive', 'olives', 'olly', 'oloroso', 'omelette', 'omlek', 'one', 'onglet', 'onion', 'onions', 'oolong', 'oops', 'oozing', 'open', 'opera', 'orange', 'oranges', 'orangey', 'orchid', 'oregano', 'organic', 'oriental', 'original', 'orleans', 'orzo', 'oscietra', 'osso', 'ossobuco', 'ottoman', 'outstanding', 'oven', 'overnight', 'ox', 'oxford', 'oxtail', 'oyster', 'oysters', 'pa', 'package', 'pad', 'padr', 'padron', 'paella', 'pagne', 'pain', 'pak', 'pakora', 'pakoras', 'palak', 'palm', 'palmiers', 'paloise', 'pampushki', 'pan', 'panacotta', 'panang', 'pancake', 'pancakes', 'pancetta', 'panchporan', 'pandan', 'pandhi', 'pandoro', 'pane', 'paneed', 'paneer', 'panettone', 'panettones', 'panforte', 'pangrattato', 'panhaggerty', 'panko', 'panna', 'pannacotta', 'pansotti', 'panzanella', 'papardelle', 'papaya', 'paper', 'papeta', 'papillote', 'papillotte', 'pappa', 'pappardelle', 'paprika', 'parachute', 'paradise', 'paratha', 'parathas', 'parcel', 'parcels', 'pardina', 'parfait', 'paris', 'parisienne', 'parkin', 'parma', 'parmentier', 'parmesan', 'parmigiana', 'parmo', 'parnsip', 'parsee', 'parsley', 'parsnip', 'parsnips', 'partridge', 'partridges', 'party', 'pasanda', 'pasilla', 'pasquale', 'pasqualina', 'passata', 'passion', 'passionfruit', 'pasta', 'paste', 'pastiera', 'pasties', 'pastilla', 'pastis', 'pastrami', 'pastries', 'pastry', 'pasty', 'pat', 'pata', 'patagonian', 'patatas', 'patate', 'pate', 'patia', 'patties', 'paul', 'paupiette', 'pauvre', 'pavlova', 'pavlovas', 'paw', 'payasam', 'paysanne', 'pe', 'pea', 'peach', 'peaches', 'peachy', 'peacock', 'peanut', 'peanuts', 'pear', 'pearl', 'pears', 'peas', 'pease', 'pebbles', 'pecan', 'pecans', 'pecorino', 'peel', 'peeling', 'pegs', 'peking', 'pembrokeshire', 'penang', 'pencil', 'penne', 'people', 'pepe', 'peperonata', 'peperoncino', 'peperoni', 'pepper', 'peppercorn', 'peppercorns', 'peppered', 'peppermint', 'pepperoni', 'pepperpot', 'peppers', 'peppery', 'perch', 'perdu', 'perfect', 'perfectly', 'peri', 'perky', 'perry', 'persian', 'persillade', 'peruvian', 'pes', 'pesce', 'peshwari', 'pesto', 'petal', 'petalberry', 'petit', 'petits', 'pheasant', 'pho', 'pi', 'piadina', 'picada', 'piccalilli', 'pici', 'pickle', 'pickled', 'pickles', 'pickling', 'picnic', 'pico', 'pie', 'pieces', 'piedmontese', 'pierogi', 'pies', 'pig', 'pigeon', 'pigeons', 'pigs', 'pikelets', 'pilaf', 'pilaff', 'pilau', 'piment', 'pimenton', 'pimiento', 'pimientos', 'pimms', 'pin', 'pina', 'pinchos', 'pine', 'pineapple', 'pinenuts', 'pink', 'pinwheel', 'pinwheels', 'piperade', 'piquillo', 'piri', 'pisco', 'piselli', 'pisellini', 'pissaladi', 'pistaches', 'pistachio', 'pistachios', 'pistou', 'pithivier', 'pithiviers', 'pitta', 'pittas', 'pittsburgh', 'pizza', 'pizzaiola', 'pizzas', 'pizzette', 'pizzoccheri', 'pla', 'plaice', 'plait', 'plaited', 'plaits', 'plancha', 'plantain', 'plantains', 'plate', 'plateau', 'platter', 'plot', 'ploughdue', 'ploughman', 'plum', 'pluma', 'plums', 'po', 'poached', 'poacher', 'pockets', 'pog', 'poha', 'poire', 'pois', 'poivre', 'poke', 'polenta', 'polish', 'pollack', 'pollen', 'pollo', 'pollock', 'polonaise', 'polpettone', 'poly', 'pomegranate', 'pomegranates', 'pomme', 'pommes', 'pomodoro', 'pond', 'ponzu', 'pop', 'popcorn', 'poppadoms', 'poppadum', 'poppy', 'poppyseed', 'pops', 'porchetta', 'porcini', 'poriyal', 'pork', 'porridge', 'port', 'portobello', 'portuguesa', 'portuguese', 'posh', 'posset', 'pot', 'potato', 'potatoes', 'pots', 'potsticker', 'potted', 'pouilly', 'poulet', 'pound', 'poussin', 'poussins', 'poutine', 'povitica', 'powder', 'power', 'pra', 'praline', 'prawn', 'prawns', 'prepare', 'present', 'preserve', 'preserved', 'pressed', 'pressure', 'pretzels', 'primavera', 'prime', 'prince', 'prinsesst', 'profiterole', 'profiteroles', 'proof', 'proper', 'prosciutto', 'proscuitto', 'prosecco', 'prosperity', 'proven', 'provence', 'prune', 'prunes', 'pudding', 'puddings', 'puff', 'puffball', 'puffed', 'puffs', 'pugliese', 'pul', 'pulao', 'pullao', 'pulled', 'pulpo', 'pumpkin', 'punch', 'punjab', 'punjabi', 'pur', 'pure', 'puri', 'purple', 'purpose', 'purses', 'purslane', 'puttanesca', 'puy', 'px', 'pyramid', 'pyramids', 'pyrizhky', 'pytt', 'quackers', 'quadruple', 'quail', 'quails', 'queen', 'queijo', 'quenelles', 'quesada', 'quesadilla', 'quesadillas', 'quiche', 'quiches', 'quick', 'quickalilli', 'quince', 'quinces', 'quinoa', 'quorn', 'raan', 'rabbit', 'rack', 'racks', 'raclette', 'radicchio', 'radish', 'radishes', 'rag', 'rago', 'ragu', 'rahmbraten', 'rainbow', 'raised', 'raisin', 'raisins', 'raita', 'raj', 'rajma', 'ram', 'ramen', 'rancheros', 'range', 'rao', 'rapeseed', 'rarebit', 'ras', 'rasayana', 'rascals', 'raspberries', 'raspberry', 'ratatouille', 'ratte', 'ravioli', 'raviolo', 'raw', 'raymond', 'razor', 'real', 'really', 'reblochon', 'red', 'redcurrant', 'redcurrants', 'reduced', 'reduction', 'reflection', 'refried', 'refrigerator', 'regina', 'reindeer', 'religieuse', 'relish', 'remoulade', 'rendang', 'res', 'return', 'reuben', 'rhubarb', 'rib', 'ribbon', 'ribbons', 'ribs', 'rice', 'rich', 'rick', 'rico', 'ricotta', 'riesling', 'rieslingspaschteit', 'rillettes', 'rillons', 'rimmer', 'rinforzo', 'ring', 'rings', 'rioja', 'ripieni', 'ripple', 'risotto', 'rissoles', 'river', 'riz', 'road', 'roased', 'roast', 'roasted', 'roasties', 'roasting', 'robins', 'rock', 'rockefeller', 'rocket', 'rockpool', 'rocky', 'roe', 'rogan', 'rojak', 'roll', 'rolled', 'rolls', 'roly', 'romaine', 'roman', 'romana', 'romanoff', 'romero', 'romesco', 'ronique', 'root', 'roots', 'roquamole', 'roquefort', 'rosace', 'rosato', 'roscoff', 'rose', 'rosemary', 'rosette', 'rosewater', 'rosse', 'rossini', 'rosti', 'rostis', 'rota', 'roti', 'rotolo', 'rouge', 'rough', 'rouille', 'roulade', 'roux', 'rowies', 'royal', 'royale', 'royals', 'rta', 'rub', 'rubbed', 'ruby', 'rucola', 'rudolph', 'rum', 'rumbledethumps', 'rump', 'rumtopf', 'runner', 'russe', 'russian', 'rustic', 'rustico', 'rye', 'saag', 'saas', 'sabayon', 'sabl', 'sable', 'sables', 'sachertorte', 'saddle', 'sadza', 'saffron', 'saganaki', 'sage', 'saikyo', 'saint', 'salad', 'salade', 'salame', 'salami', 'sali', 'salmi', 'salmon', 'salmoriglio', 'salsa', 'salsify', 'salt', 'saltado', 'salted', 'saltfish', 'saltimbocca', 'salut', 'salzburg', 'sambar', 'sambuca', 'samosa', 'samosas', 'samphire', 'san', 'sand', 'sandwich', 'sandwiches', 'sangchae', 'sangria', 'sansho', 'sao', 'saor', 'sapeur', 'sarde', 'sardine', 'sardines', 'sardinian', 'sarnie', 'sarnies', 'sarsaparilla', 'sashimi', 'satay', 'satsumas', 'saturday', 'sauce', 'sauces', 'saucisson', 'sauer', 'sauerkraut', 'sausage', 'sausagemeat', 'sausages', 'saut', 'sauternes', 'savarin', 'savoury', 'savoy', 'scad', 'scales', 'scallion', 'scallop', 'scallops', 'scampi', 'scandinavian', 'scapece', 'scapes', 'scarlet', 'scary', 'scented', 'schichttorte', 'schnecken', 'schnitzel', 'schnitzels', 'schokogugelhupf', 'scone', 'scones', 'scorched', 'scotch', 'scottish', 'scouse', 'scrambled', 'scraps', 'scratchings', 'scrolls', 'scrumpy', 'sea', 'seabass', 'seafood', 'seared', 'seashell', 'seaside', 'season', 'seasonal', 'seasoned', 'seasoning', 'seaweed', 'second', 'secret', 'seed', 'seeded', 'seeds', 'seedy', 'seek', 'seekh', 'seera', 'segments', 'selection', 'semifreddo', 'semolina', 'senegalese', 'serrano', 'served', 'sesame', 'seven', 'seville', 'sgombro', 'shahi', 'shake', 'shaken', 'shakshuka', 'shallot', 'shallots', 'sham', 'shami', 'shank', 'shanks', 'shaped', 'shards', 'share', 'sharing', 'sharlotka', 'sharp', 'shashlik', 'shatkora', 'shaved', 'shavings', 'shawarma', 'sheek', 'sheep', 'sheeps', 'sheera', 'sheftalia', 'shell', 'shellfish', 'shells', 'shepherd', 'sherbet', 'sherry', 'shetland', 'shichimi', 'shiitake', 'shikar', 'shimeji', 'shin', 'shinni', 'shirley', 'shish', 'shiso', 'shoestring', 'shoot', 'shoots', 'shorshe', 'short', 'shortbread', 'shortbreads', 'shortcake', 'shortcakes', 'shortcrust', 'shortcut', 'shots', 'shoulder', 'showstopper', 'showstoppers', 'shredded', 'shreds', 'shrikand', 'shrikhand', 'shrimp', 'shrimps', 'shropshire', 'shu', 'si', 'sichuan', 'sicilian', 'silk', 'simmered', 'simnel', 'simon', 'simple', 'sin', 'singapore', 'sioh', 'sirloin', 'siu', 'sized', 'sizzler', 'sizzling', 'skate', 'skewer', 'skewered', 'skewers', 'skin', 'skink', 'skinless', 'skinned', 'skinny', 'skinnylicious', 'skins', 'skirlie', 'skirt', 'skolebr', 'skordalia', 'skye', 'slater', 'slaw', 'slice', 'sliced', 'slices', 'sliders', 'slightly', 'sling', 'slipcote', 'sloe', 'sloppy', 'slow', 'small', 'smashed', 'smiles', 'smoked', 'smokie', 'smoky', 'smooth', 'smoothie', 'smothered', 'snail', 'snake', 'snap', 'snapper', 'snaps', 'snazzy', 'snickers', 'snow', 'snowman', 'soaked', 'soba', 'socca', 'soda', 'soffritto', 'soft', 'softened', 'softly', 'sogliola', 'soi', 'soil', 'soldiers', 'sole', 'somerset', 'sorbet', 'sorrel', 'souchong', 'souffl', 'soup', 'sour', 'sourdough', 'soured', 'soused', 'south', 'southern', 'soutzoukakia', 'souvlaki', 'souvlakia', 'soy', 'soya', 'sp', 'space', 'spada', 'spaghetti', 'spaghettini', 'spanakopita', 'spanische', 'spanish', 'spare', 'spargel', 'sparkle', 'sparkles', 'sparkling', 'spatchcock', 'spatchcocked', 'spatzle', 'spears', 'special', 'speck', 'speculaas', 'speedy', 'spelt', 'spice', 'spiced', 'spices', 'spicy', 'spider', 'spigola', 'spillers', 'spinach', 'spinaci', 'spirals', 'splashed', 'splendidly', 'split', 'splits', 'sponge', 'sponges', 'spotted', 'spread', 'spring', 'sprinkles', 'spritzer', 'sprout', 'sprouting', 'sprouts', 'spruced', 'spun', 'squab', 'squares', 'squash', 'squashes', 'squeak', 'squid', 'squirrel', 'sri', 'ssamjang', 'st', 'stack', 'stacks', 'staffordshire', 'stained', 'stamped', 'star', 'stargazey', 'starter', 'stawberry', 'steak', 'steaks', 'steamed', 'steeped', 'stefan', 'stein', 'stem', 'stems', 'step', 'stew', 'stewed', 'sti', 'sticks', 'sticky', 'stillton', 'stilton', 'stinging', 'stinking', 'stir', 'stis', 'stock', 'stollen', 'stone', 'store', 'stout', 'stovies', 'strand', 'strata', 'straw', 'strawberries', 'strawberry', 'straws', 'streaky', 'stress', 'streusel', 'strips', 'stroganoff', 'stromboli', 'strudel', 'strudels', 'struffoli', 'studded', 'stuffed', 'stuffin', 'stuffing', 'stuffings', 'style', 'succotash', 'succulent', 'suckling', 'suet', 'suffolk', 'sugar', 'sugared', 'suissesse', 'sukiyaki', 'sukka', 'sultan', 'sultana', 'sultanas', 'sum', 'sumac', 'sumiko', 'summer', 'summerberry', 'summery', 'sumptuous', 'sun', 'sunblush', 'sundae', 'sundaes', 'sunflower', 'sunny', 'sunrise', 'sunshine', 'super', 'superfood', 'supper', 'supr', 'surf', 'suriani', 'surprise', 'sushi', 'sussex', 'suzette', 'swahili', 'swans', 'swede', 'swedish', 'sweet', 'sweetbread', 'sweetbreads', 'sweetcorn', 'sweetheart', 'swirl', 'swirls', 'swiss', 'swordfish', 'syllabub', 'syrian', 'syrup', 'tabbouleh', 'tablier', 'taboon', 'taboulleh', 'taco', 'tacos', 'tadka', 'tagine', 'tagliarini', 'tagliata', 'tagliatelle', 'taglierini', 'tagliolini', 'tags', 'tahini', 'tail', 'tails', 'taleggio', 'talla', 'tamago', 'tamale', 'tamarind', 'tandoori', 'tangerines', 'tangiers', 'tangy', 'tapas', 'tapenade', 'taquitos', 'tar', 'taramasalata', 'tarator', 'tarka', 'tarkari', 'tarragon', 'tart', 'tartar', 'tartare', 'tarte', 'tartiflette', 'tartlets', 'tarts', 'tasted', 'tasty', 'tatin', 'tatins', 'tattie', 'tatties', 'taukwa', 'tav', 'te', 'tea', 'teacakes', 'teacup', 'teal', 'tear', 'teau', 'teaux', 'tempered', 'temple', 'temptation', 'tempura', 'ten', 'tenderloin', 'tenderstem', 'tendon', 'tennis', 'tequila', 'teriyaki', 'terrine', 'teryaki', 'tex', 'texan', 'textured', 'thai', 'thermidor', 'thick', 'thigh', 'thighs', 'thin', 'thins', 'thoran', 'thousand', 'thread', 'three', 'throw', 'thyme', 'tian', 'ticada', 'tidy', 'tielle', 'tier', 'tiered', 'tiger', 'tikka', 'tilapia', 'timbale', 'timbales', 'time', 'tinned', 'tiny', 'tips', 'tipsy', 'tiramis', 'tiramisini', 'tiramisu', 'tissi', 'toad', 'toast', 'toasted', 'toastie', 'toasts', 'toasty', 'toffee', 'tofu', 'together', 'toise', 'tom', 'tomatillo', 'tomato', 'tomatoes', 'ton', 'tongue', 'tonic', 'tonkatsu', 'tonnato', 'tons', 'top', 'topped', 'topping', 'toppings', 'tops', 'topside', 'torbay', 'torched', 'toridashi', 'torn', 'torrese', 'torroncino', 'torrone', 'torta', 'torte', 'tortelli', 'tortellini', 'tortiglioni', 'tortilla', 'tortillas', 'toscana', 'toscano', 'tossed', 'tostadas', 'totally', 'toulouse', 'tournedos', 'tout', 'tower', 'towers', 'towey', 'town', 'traditional', 'tranche', 'transit', 'trapanese', 'travelling', 'tray', 'traybake', 'traybaked', 'treacle', 'treats', 'tree', 'trencher', 'triangle', 'triangles', 'tricolor', 'trifle', 'trifolati', 'trimmed', 'trimmings', 'trinity', 'trio', 'tripe', 'triple', 'tripolium', 'tropical', 'trotter', 'trout', 'truffle', 'truffled', 'truffles', 'tso', 'tsoureki', 'tubetti', 'tuile', 'tuiles', 'tuille', 'tumbet', 'tuna', 'tunis', 'turbigo', 'turbot', 'turf', 'turkey', 'turkish', 'turmeric', 'turnip', 'turnips', 'turnover', 'turnovers', 'tuscan', 'tusha', 'tutti', 'twice', 'twist', 'twister', 'twists', 'twisty', 'two', 'txistorra', 'tzatziki', 'tzle', 'udon', 'ulster', 'ultimate', 'umami', 'umeboshi', 'unbelievable', 'underground', 'union', 'ups', 'upside', 'urad', 'vacherin', 'valencian', 'valensole', 'valentine', 'vanilla', 'vanille', 'variations', 'various', 'veal', 'veg', 'vegan', 'vegetable', 'vegetables', 'vegetarian', 'veggie', 'veggies', 'velodrome', 'velout', 'veloute', 'velvet', 'velvety', 'venetian', 'veneziana', 'venison', 'vents', 'verbena', 'verde', 'verdure', 'verjus', 'vermicelli', 'vermouth', 'veronique', 'verte', 'vevichathu', 'via', 'vichy', 'vichyssoise', 'victoria', 'viennese', 'vierge', 'vietnamese', 'vignarola', 'vignotte', 'vin', 'vinaigre', 'vinaigrette', 'vincisgrassi', 'vindaloo', 'vine', 'vinegar', 'violet', 'virgin', 'vitello', 'vodka', 'vol', 'volcanoes', 'vongole', 'wafer', 'wafers', 'waffle', 'waffles', 'wagyu', 'wakame', 'waldorf', 'wallbanger', 'walnut', 'walnuts', 'warm', 'warmed', 'wasabi', 'wastenot', 'water', 'watercress', 'watermelon', 'way', 'ways', 'wedding', 'wedges', 'wee', 'weeping', 'wellington', 'wellingtons', 'welsh', 'wensleydale', 'wenslydale', 'west', 'westie', 'wet', 'wheat', 'wheaten', 'wheel', 'whip', 'whipped', 'whips', 'whirl', 'whirls', 'whiskey', 'whisky', 'white', 'whitebait', 'whiting', 'whitstable', 'whole', 'wholegrain', 'wholemeal', 'wholewheat', 'whoopie', 'wight', 'wild', 'william', 'wilted', 'window', 'windtorte', 'wine', 'wing', 'wings', 'winter', 'witchill', 'wobbly', 'wok', 'wontons', 'wood', 'worcestershire', 'woven', 'wrap', 'wrapped', 'wraps', 'wreath', 'wreck', 'xeo', 'xo', 'yaan', 'yaki', 'yakitori', 'yam', 'yarg', 'yasai', 'yellow', 'yoghurt', 'yolk', 'york', 'yorkshire', 'yorkshires', 'young', 'yukhoe', 'yule', 'yum', 'yung', 'yunnanese', 'yusheng', 'yuzu', 'za', 'zabaglione', 'zampone', 'zanzibar', 'zemlovka', 'zest', 'zesty', 'zingy', 'zoe', 'zombie', 'zucca', 'zucchero', 'zucchini', 'zuccotto', 'zupfe']\n"
     ]
    }
   ],
   "source": [
    "corpus = cleaned_t\n",
    "ct_vectorizer = CountVectorizer(stop_words=stopwords_nltk)\n",
    "\n",
    "titles_ct = ct_vectorizer.fit_transform(corpus)\n",
    "print(ct_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "TF-IDF = (Term Frequency) * (Inverse Document Frequency)\n",
    "\n",
    "(how often word occurs in this doc) * inverse of (how often this word occurs in all documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:01.896683Z",
     "start_time": "2019-12-09T00:32:01.281993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aberdeen', 'abundant', 'acacia', 'ackee', 'adaptable', 'ade', 'adhraki', 'adobo', 'adraki', 'aduki', 'affogato', 'african', 'aged', 'aglio', 'agnello', 'ahead', 'aioli', 'ais', 'aji', 'ajillo', 'ajitsuke', 'ajo', 'ajwain', 'al', 'alaska', 'albanian', 'albert', 'albondigas', 'alcoholic', 'ale', 'alert', 'alfredo', 'alla', 'alle', 'alloro', 'allspice', 'almond', 'almondine', 'almonds', 'almondy', 'aloo', 'alpine', 'alsatian', 'alternative', 'amann', 'amaretti', 'amaretto', 'amarillo', 'amatriciana', 'amazing', 'ambassadeur', 'ambassador', 'amber', 'american', 'amoul', 'anatra', 'ancho', 'anchovade', 'anchovies', 'anchovy', 'ancienne', 'andalucian', 'angel', 'angela', 'anglais', 'anglaise', 'anglesey', 'anguilla', 'anise', 'aniseed', 'annika', 'anolini', 'antony', 'anya', 'aperitivo', 'apple', 'apples', 'apricot', 'apricots', 'arabian', 'aragon', 'arancine', 'arancini', 'arbequina', 'arbroath', 'arctic', 'arlettes', 'armagnac', 'arnaise', 'arnold', 'aromatic', 'arrabbiata', 'arrabiata', 'arrabiatta', 'arroz', 'arselle', 'artichoke', 'artichokes', 'asaparagus', 'asian', 'asparagus', 'aster', 'atar', 'au', 'aubergine', 'aubergines', 'aussie', 'authentic', 'autumn', 'auvergne', 'aux', 'avgolemono', 'avial', 'avocado', 'avocados', 'ayam', 'aztec', 'baba', 'babas', 'babka', 'baby', 'baccal', 'back', 'bacon', 'bag', 'bagel', 'bagels', 'bagna', 'bagnat', 'baguette', 'baguettes', 'bake', 'baked', 'bakes', 'bakewell', 'baklava', 'balinese', 'ball', 'balls', 'balm', 'balsamic', 'balti', 'baltic', 'bamboo', 'banana', 'bananas', 'bandera', 'bang', 'bangers', 'bangladeshi', 'banh', 'banoffee', 'bao', 'bar', 'bara', 'barbary', 'barbecue', 'barbecued', 'barbied', 'barcoletta', 'barigoule', 'bark', 'barley', 'barmbrack', 'barnsley', 'barquettes', 'barramundi', 'barrel', 'bars', 'base', 'bashed', 'basic', 'basil', 'basket', 'baskets', 'basmati', 'basque', 'bass', 'bastilla', 'batons', 'battenberg', 'batter', 'battered', 'bauble', 'baubles', 'bavarian', 'bavette', 'bay', 'bayonne', 'bbq', 'beachside', 'bean', 'beans', 'beansprout', 'beansprouts', 'beany', 'bearnaise', 'beccafico', 'bed', 'beef', 'beefburger', 'beehive', 'beer', 'beet', 'beetroot', 'beetroots', 'beets', 'begendi', 'beggars', 'beignets', 'belgian', 'believe', 'bellini', 'belly', 'belts', 'ben', 'benedict', 'bengal', 'bengali', 'bennett', 'bergamot', 'berkswell', 'berries', 'berry', 'besan', 'bessara', 'best', 'beurre', 'bhaji', 'bhajia', 'bhajis', 'bhindi', 'bhuna', 'bi', 'bianco', 'biber', 'bierocks', 'big', 'bigarade', 'bigos', 'biker', 'bikers', 'bill', 'bing', 'bircher', 'birthday', 'biryani', 'biscotti', 'biscuit', 'biscuits', 'bishop', 'bisque', 'bissau', 'bite', 'bites', 'bitter', 'bitterballen', 'black', 'blackbean', 'blackberries', 'blackberry', 'blackcurrant', 'blackcurrants', 'blackened', 'blackeye', 'blackwater', 'blade', 'blanc', 'blancmange', 'blancs', 'blanket', 'blankets', 'blanquette', 'bleu', 'blini', 'blinis', 'blondies', 'blood', 'bloody', 'bloomer', 'blow', 'blue', 'blueberries', 'blueberry', 'blush', 'blushed', 'boar', 'board', 'boat', 'boats', 'bobotie', 'boeuf', 'boiled', 'bois', 'bok', 'bokkeum', 'bolinhos', 'bologna', 'bolognaise', 'bolognese', 'bomb', 'bombay', 'bombs', 'bonbons', 'bone', 'bonfire', 'boozy', 'borage', 'bordelaise', 'boreks', 'borlotti', 'borsch', 'borscht', 'boston', 'boti', 'bottarga', 'boudran', 'bouillabaisse', 'boulang', 'boulangere', 'bourbon', 'bourguignon', 'bourride', 'bowl', 'bowls', 'bows', 'box', 'boxing', 'boxty', 'boys', 'br', 'braciole', 'braised', 'bramble', 'bramley', 'bran', 'branches', 'brandade', 'brandied', 'brandy', 'branzino', 'brassicas', 'bratwurst', 'bravas', 'brazil', 'brazils', 'bread', 'breadcrumb', 'breadcrumbed', 'breadcrumbs', 'breaded', 'breads', 'breadsticks', 'breakfast', 'bream', 'breast', 'breasts', 'bredie', 'bresaola', 'brest', 'bretonne', 'brick', 'bridie', 'brie', 'brik', 'brill', 'brined', 'brioche', 'brioches', 'brisbane', 'brisket', 'brith', 'british', 'brittle', 'brixton', 'broad', 'broccoli', 'brochette', 'brodo', 'broth', 'brothy', 'brown', 'brownie', 'brownies', 'browns', 'brunch', 'bruno', 'bruschetta', 'bruschettas', 'brushed', 'brussels', 'bubble', 'buccaneer', 'bucco', 'bucket', 'buckwheat', 'buco', 'buddha', 'buerre', 'buffalo', 'bulgar', 'bulgogi', 'bulgur', 'buljol', 'bun', 'bundles', 'bundt', 'bunny', 'buns', 'bunski', 'bunting', 'burger', 'burgers', 'burn', 'burnt', 'burrata', 'burrida', 'burrito', 'bursts', 'busara', 'butter', 'butterbean', 'butterbeans', 'buttercream', 'buttered', 'butterflied', 'butterfly', 'butteries', 'buttermilk', 'butternut', 'butters', 'butterscotch', 'buttery', 'butties', 'button', 'butty', 'cabbage', 'cabbge', 'cacciatora', 'cacciatore', 'cacio', 'caerphilly', 'caesar', 'cafriela', 'caipirosca', 'cajun', 'cake', 'cakes', 'calabrese', 'calamari', 'calda', 'caldeirada', 'caldo', 'calf', 'californian', 'callaloo', 'calvados', 'calves', 'calypso', 'calzone', 'cambozola', 'camembert', 'canadian', 'canap', 'canapes', 'cancalaise', 'candied', 'candy', 'cane', 'cannellini', 'cannelloni', 'cannon', 'canterbury', 'cantonese', 'cao', 'cape', 'caper', 'capers', 'caponata', 'cappellacci', 'capperi', 'cappuccino', 'caprese', 'capricorn', 'captain', 'caramel', 'caramelised', 'caramelized', 'caramels', 'caraway', 'carbonara', 'carbonnade', 'cardamom', 'cardamon', 'caribbean', 'carlina', 'carlo', 'carne', 'carol', 'carpaccio', 'carpet', 'carrot', 'carrots', 'carrozza', 'case', 'cashel', 'cashew', 'cashews', 'cassava', 'casserole', 'cassis', 'cassoulet', 'catalan', 'catalonian', 'cauda', 'cauliflower', 'cavalo', 'cavatelli', 'caviar', 'cavolo', 'cawl', 'cayenne', 'celariac', 'celebration', 'celeriac', 'celery', 'celtic', 'centred', 'cep', 'ceps', 'ceviche', 'chaap', 'chaat', 'chai', 'cham', 'chamel', 'champ', 'champagne', 'chana', 'chantenay', 'chanterelle', 'chanterelles', 'chantilly', 'chapati', 'chapatis', 'chapel', 'chappatis', 'char', 'charcoal', 'charcuti', 'chard', 'chardonnay', 'charentais', 'chargrilled', 'charlotte', 'charlottes', 'charred', 'chartreuse', 'chashu', 'chasseur', 'chat', 'chateaubriand', 'chaussons', 'chavignol', 'che', 'cheat', 'cheats', 'cheaty', 'cheddar', 'cheek', 'cheeks', 'cheese', 'cheeseburger', 'cheeseburgers', 'cheesecake', 'cheesecakes', 'cheeses', 'cheesesteak', 'cheesy', 'chef', 'chelsea', 'chemmeen', 'chen', 'chermoula', 'cherries', 'cherry', 'chervil', 'cheshire', 'chestnut', 'chestnuts', 'chettinad', 'chewy', 'chia', 'chicken', 'chickens', 'chickpea', 'chickpeas', 'chicory', 'chiffon', 'childhood', 'chili', 'chilled', 'chilli', 'chillies', 'chimichanga', 'chimichurri', 'chimneys', 'chinese', 'ching', 'chingri', 'chip', 'chipolatas', 'chipotle', 'chips', 'chiu', 'chive', 'chives', 'chlodnik', 'chocolat', 'chocolate', 'chocolates', 'choi', 'cholent', 'chop', 'chopped', 'chops', 'chorizo', 'choron', 'choucroute', 'choux', 'chow', 'chowder', 'choy', 'chris', 'christian', 'christmas', 'christophine', 'chump', 'chunks', 'chunky', 'chupe', 'churn', 'churros', 'chutney', 'ciabatta', 'ciambellone', 'cider', 'cinder', 'cinnamon', 'cipolle', 'citron', 'citrus', 'clafoutis', 'clair', 'clairs', 'clam', 'clams', 'clap', 'clapshot', 'classic', 'claws', 'clear', 'cleaves', 'clement', 'clementine', 'clementines', 'clements', 'clock', 'clootie', 'clotted', 'cloud', 'clove', 'cloves', 'club', 'coarse', 'coated', 'cob', 'cobbled', 'cobbler', 'cobnut', 'cobnuts', 'cock', 'cockle', 'cockles', 'cocktail', 'coco', 'cocoa', 'coconut', 'cocotte', 'cod', 'coddled', 'codfish', 'coffee', 'coins', 'cointreau', 'cola', 'colada', 'colatura', 'colcannon', 'cold', 'coleslaw', 'coley', 'collar', 'colonel', 'colossal', 'comforting', 'comp', 'compost', 'compote', 'con', 'concasse', 'cond', 'confit', 'consomm', 'contaldo', 'continental', 'cook', 'cooked', 'cooker', 'cookie', 'cookies', 'cooler', 'coorg', 'coq', 'coquilles', 'cordial', 'cordon', 'coriander', 'corn', 'cornbread', 'corned', 'cornish', 'cornmeal', 'coronation', 'cos', 'cote', 'cotswold', 'cotta', 'cottage', 'cou', 'coulibiac', 'coulis', 'country', 'courgette', 'courgettes', 'courgetti', 'couronne', 'couscous', 'cow', 'coxes', 'cr', 'crab', 'crabcakes', 'crabs', 'cracked', 'crackers', 'crackle', 'crackling', 'cracklings', 'cranachan', 'cranberries', 'cranberry', 'crayfish', 'cream', 'creamed', 'creams', 'creamy', 'creation', 'crema', 'cremat', 'cremosa', 'creole', 'cress', 'crisp', 'crispbread', 'crisps', 'crispy', 'cro', 'crocodile', 'croissant', 'croissants', 'croque', 'croquembouche', 'croquetas', 'croquette', 'croquettes', 'cross', 'crostini', 'crottin', 'croustade', 'croustades', 'croustillant', 'croute', 'croutes', 'crouton', 'croutons', 'crowd', 'crown', 'crudite', 'crudites', 'crudo', 'crumb', 'crumbed', 'crumble', 'crumbled', 'crumbs', 'crumpet', 'crumpets', 'crunch', 'crunched', 'crunchies', 'crunchy', 'crush', 'crushed', 'crust', 'crusted', 'crusty', 'crystallised', 'csa', 'cuba', 'cuban', 'cucumber', 'cucumbers', 'cullen', 'cumberland', 'cumin', 'cup', 'cupboard', 'cupcakes', 'cups', 'curd', 'cured', 'curly', 'currant', 'currants', 'curranty', 'curried', 'currimbhoy', 'curry', 'custard', 'customised', 'cut', 'cutless', 'cutlet', 'cutlets', 'cuttlefish', 'cypriot', 'da', 'daab', 'dacquoise', 'dad', 'daikon', 'daiquiri', 'dal', 'damper', 'dampfnudel', 'damson', 'dandelion', 'dangs', 'danish', 'danishes', 'dappy', 'dark', 'darnes', 'dashi', 'date', 'dates', 'daube', 'dauphinoise', 'daurade', 'day', 'de', 'decker', 'deconstructed', 'deep', 'deer', 'definitive', 'delhi', 'delia', 'delice', 'delicious', 'delight', 'deluxe', 'dengaku', 'dentdale', 'dermot', 'derry', 'design', 'designer', 'dessert', 'devil', 'devilish', 'devilled', 'devils', 'devon', 'devonshire', 'dhal', 'dhansak', 'di', 'diamond', 'diamonds', 'diane', 'diced', 'dick', 'digger', 'diggers', 'dijon', 'dill', 'dillisk', 'dim', 'ding', 'dinner', 'dip', 'dipped', 'dipper', 'dippers', 'dipping', 'dips', 'dirty', 'disco', 'dish', 'diy', 'dobos', 'dolcelatte', 'dolmades', 'doner', 'dong', 'donuts', 'doorstep', 'dopiaza', 'doris', 'dory', 'dosa', 'double', 'dough', 'doughballs', 'doughnut', 'doughnuts', 'douglas', 'dovedale', 'dover', 'dowdy', 'dragon', 'dragoncella', 'drambuie', 'dream', 'dreamlike', 'dress', 'dressed', 'dressing', 'dried', 'drink', 'dripping', 'drizzle', 'drizzled', 'drizzly', 'drop', 'dropped', 'drowned', 'drumstick', 'drumsticks', 'drunken', 'dry', 'duchess', 'duchesse', 'duck', 'duckling', 'duff', 'duja', 'dukkah', 'dulce', 'dulse', 'dum', 'dumpling', 'dumplings', 'dundee', 'duo', 'dust', 'dusted', 'dwarf', 'ear', 'earl', 'ears', 'east', 'easter', 'eastern', 'easy', 'eccles', 'ed', 'edamame', 'eeda', 'eel', 'eels', 'egg', 'eggnog', 'eggs', 'eggy', 'egyptian', 'eight', 'el', 'elderberry', 'elderflower', 'elizabeth', 'elizabethan', 'emergency', 'emilion', 'emmental', 'empanada', 'empanadas', 'emulsion', 'en', 'enchiladas', 'encrusted', 'end', 'endive', 'energy', 'england', 'english', 'enoki', 'entremets', 'es', 'escabeche', 'escalope', 'escalopes', 'escargot', 'escarole', 'escovitch', 'espresso', 'essence', 'essex', 'estonian', 'eton', 'european', 'evans', 'eve', 'ever', 'everlasting', 'everyday', 'ewes', 'exceptional', 'exotic', 'express', 'expressed', 'extra', 'extraordinary', 'extravagant', 'eye', 'eyed', 'fa', 'fabulous', 'faced', 'faggot', 'faggots', 'fagioli', 'fail', 'fairy', 'fajita', 'fajitas', 'falafel', 'falafels', 'family', 'fancies', 'faraizi', 'farci', 'farfalle', 'farinata', 'farl', 'farls', 'farmhouse', 'farthing', 'faschierter', 'fashioned', 'fast', 'fat', 'fatless', 'fattoush', 'favourite', 'fazool', 'feast', 'feather', 'feathered', 'fed', 'feed', 'feet', 'fegato', 'feijoada', 'fennel', 'fenugreek', 'feria', 'fermented', 'fesenjoon', 'fest', 'festive', 'feta', 'fettuccine', 'feu', 'feuillete', 'fideu', 'fidget', 'field', 'fiery', 'fig', 'figgy', 'figs', 'filets', 'filled', 'fillet', 'filleto', 'fillets', 'filling', 'fillings', 'filly', 'filo', 'fin', 'financiers', 'find', 'fine', 'finely', 'finger', 'fingers', 'finished', 'finnish', 'finocchio', 'fior', 'fir', 'firecracker', 'fired', 'fish', 'fishcake', 'fishcakes', 'fisherman', 'fita', 'five', 'fix', 'fizz', 'fizzy', 'flageolet', 'flaked', 'flamb', 'flamed', 'flamenco', 'flan', 'flank', 'flaounes', 'flapjacks', 'flash', 'flat', 'flatbread', 'flatbreads', 'flatleaf', 'flattened', 'flavoured', 'flavours', 'flecked', 'fleur', 'flexible', 'float', 'floating', 'floral', 'florentine', 'florentines', 'florets', 'floss', 'flottante', 'flottantes', 'flour', 'flourless', 'flower', 'flowerpot', 'flowers', 'fluffy', 'flyers', 'foam', 'focaccia', 'foccacia', 'foil', 'fondant', 'fondants', 'fondue', 'fontina', 'foo', 'food', 'fool', 'foolproof', 'football', 'fore', 'forest', 'forestiere', 'form', 'forno', 'fortune', 'forty', 'foster', 'fougasse', 'four', 'fours', 'fowl', 'fra', 'fragrant', 'fraiche', 'frais', 'fraisier', 'fran', 'francais', 'frangipane', 'frango', 'freddo', 'free', 'freekeh', 'freewheeling', 'freeze', 'fregola', 'french', 'freschi', 'fresco', 'fresh', 'freshly', 'frestelse', 'fricass', 'fricasse', 'friday', 'fridge', 'fried', 'fries', 'frikadeller', 'fris', 'frites', 'frittata', 'frittatas', 'fritter', 'fritters', 'fritti', 'fritto', 'fromage', 'frond', 'frosted', 'frosting', 'froth', 'frozen', 'fruit', 'fruits', 'fruity', 'frutti', 'fry', 'frying', 'fu', 'fudge', 'fudgy', 'full', 'fum', 'funfetti', 'funghi', 'funky', 'fusilli', 'gado', 'gai', 'gala', 'galaktoboureko', 'galbi', 'galette', 'galettes', 'galician', 'galinha', 'gallega', 'gallina', 'gallo', 'gamberones', 'game', 'gamekeeper', 'gammon', 'ganache', 'ganoush', 'garage', 'garam', 'garden', 'garibaldi', 'garlic', 'garlicky', 'garnish', 'gateau', 'gazpacho', 'geisha', 'gelati', 'gelato', 'gem', 'general', 'genoa', 'genoise', 'genovese', 'gentleman', 'george', 'georgian', 'germain', 'german', 'getaway', 'gg', 'ghasemi', 'gherkin', 'ghoogra', 'ghost', 'ghotala', 'gianduia', 'giant', 'gift', 'gigantes', 'gilt', 'gilthead', 'gin', 'ginger', 'gingerbread', 'gingernut', 'girl', 'girolle', 'girolles', 'gl', 'glac', 'glam', 'glamming', 'glamorgan', 'glass', 'glaze', 'glazed', 'glitter', 'globe', 'glory', 'gloucester', 'gloucestershire', 'gluten', 'gnocchi', 'gnudi', 'goan', 'goat', 'goats', 'gobhi', 'gobi', 'godfather', 'goji', 'gold', 'golden', 'golek', 'gong', 'good', 'goodwood', 'gooey', 'goose', 'gooseberries', 'gooseberry', 'goosnargh', 'goreng', 'gorgonzola', 'gosht', 'goug', 'goujons', 'goulash', 'gourmet', 'graffiti', 'grain', 'grains', 'gran', 'grand', 'grandma', 'granger', 'granita', 'granny', 'grano', 'granola', 'granseola', 'grant', 'grape', 'grapefruit', 'grapes', 'grappa', 'grated', 'gratin', 'gratinated', 'gratins', 'gravad', 'gravadlax', 'gravalax', 'gravlax', 'gravy', 'great', 'greek', 'green', 'greengage', 'greengrocers', 'greens', 'gremolata', 'grenoble', 'grenobloise', 'gressingham', 'grey', 'gribiche', 'griddle', 'griddled', 'griglia', 'grill', 'grilled', 'griottine', 'grissini', 'grits', 'ground', 'grouse', 'gruy', 'gruyere', 'guacamole', 'guard', 'guava', 'guavas', 'gui', 'guilt', 'guinea', 'guinness', 'gujarati', 'gulab', 'gulasch', 'gulyas', 'gumbo', 'gunpowder', 'gurnard', 'gymkhana', 'gyoza', 'gypsy', 'ha', 'haddock', 'haggerty', 'haggis', 'hairy', 'haitian', 'hake', 'half', 'halibut', 'halloumi', 'halloween', 'halwa', 'ham', 'hand', 'handkerchief', 'hanger', 'hanncha', 'hanout', 'hara', 'hard', 'hardcore', 'haricot', 'harissa', 'harvest', 'harvey', 'hash', 'hasselback', 'hat', 'hawaiian', 'hay', 'hazelnut', 'hazelnuts', 'hazlenut', 'head', 'health', 'healthy', 'heart', 'hearts', 'hearty', 'heather', 'hedgehog', 'heel', 'heirloom', 'hemp', 'hen', 'henna', 'herb', 'herbed', 'herbelicious', 'herbs', 'herby', 'herd', 'hereford', 'herefordshire', 'heritage', 'herring', 'herrings', 'hidden', 'hide', 'highland', 'hispaniola', 'hispi', 'hive', 'hock', 'hocks', 'hogget', 'hoi', 'hoisin', 'hoki', 'hole', 'holes', 'hollandaise', 'hollywood', 'holstein', 'holy', 'home', 'homecoming', 'homemade', 'homestyle', 'homity', 'homme', 'honey', 'honeycomb', 'honeyed', 'honor', 'hopkinson', 'horns', 'horseback', 'horseradish', 'hot', 'hotdogs', 'hotpot', 'hounds', 'hour', 'house', 'huevos', 'hugh', 'hummus', 'hungarian', 'hunkar', 'husks', 'hyderabadi', 'ib', 'iberico', 'ice', 'iced', 'icelandic', 'ices', 'icing', 'iles', 'indian', 'individual', 'indo', 'indonesian', 'indulgence', 'indulgent', 'infused', 'inglese', 'ink', 'insalata', 'inspired', 'instant', 'intense', 'involtini', 'inzimino', 'iranian', 'irene', 'irish', 'isaan', 'ishtew', 'ishtu', 'island', 'islands', 'isle', 'israeli', 'issan', 'ital', 'italian', 'italiana', 'jack', 'jacket', 'jackets', 'jacob', 'jacques', 'jaffa', 'jaggery', 'jalape', 'jalapeno', 'jalfrezi', 'jalousie', 'jam', 'jamaican', 'jambalaya', 'james', 'jammy', 'jamon', 'jamun', 'jansson', 'january', 'japanese', 'jardiniere', 'jarlsberg', 'jasmine', 'jean', 'jellies', 'jelly', 'jenny', 'jerk', 'jersey', 'jerseys', 'jerusalem', 'jewel', 'jewelled', 'jhal', 'jhol', 'jian', 'joe', 'john', 'joint', 'jollof', 'jorge', 'josh', 'jowl', 'jug', 'juice', 'juices', 'julie', 'julienne', 'jumble', 'jumbo', 'jump', 'jungle', 'juniper', 'jus', 'ka', 'kabob', 'kachori', 'kachumber', 'kadaifi', 'kadhai', 'kadhi', 'kaffir', 'kai', 'kalamata', 'kale', 'kali', 'kanell', 'kapitan', 'kara', 'karara', 'karhai', 'kartoffelbrot', 'kasha', 'kashmiri', 'katsu', 'katy', 'kavaab', 'kebab', 'kebabs', 'kedgeree', 'kefta', 'kelp', 'kent', 'kentish', 'kerala', 'keralan', 'keralian', 'kernel', 'kerridge', 'ketchup', 'key', 'kha', 'khao', 'kharu', 'kheema', 'kheer', 'ki', 'kibbe', 'kichri', 'kick', 'kid', 'kidney', 'kidneys', 'kids', 'kiev', 'kievs', 'kimchi', 'kind', 'kinds', 'king', 'kipper', 'kippers', 'kisses', 'kitchen', 'kiteria', 'kites', 'kiwi', 'klippfisk', 'knickerbocker', 'knocking', 'knots', 'knuckle', 'kofta', 'koftas', 'kofte', 'kohlrabi', 'koji', 'koli', 'korean', 'korma', 'kosi', 'kouign', 'koulibiac', 'kozani', 'kransekake', 'krob', 'kroppkakor', 'kugelhopf', 'kulfi', 'kumquat', 'kumquats', 'la', 'labneh', 'lace', 'lacy', 'ladder', 'lady', 'lait', 'laksa', 'lamb', 'lambs', 'lancashire', 'landaise', 'langosi', 'langoustine', 'langoustines', 'languages', 'langues', 'lankan', 'lapsang', 'larb', 'lardo', 'lardons', 'lardy', 'large', 'lasagna', 'lasagne', 'lassi', 'last', 'late', 'latkes', 'latte', 'lattice', 'latvian', 'lau', 'lava', 'lavender', 'laverbread', 'lax', 'layer', 'layered', 'layers', 'lazy', 'lazybones', 'le', 'leaf', 'lean', 'leather', 'leaves', 'lebanese', 'lee', 'leek', 'leekie', 'leeks', 'leftover', 'leg', 'legs', 'legume', 'leicester', 'lemak', 'lemon', 'lemonade', 'lemongrass', 'lemons', 'lemony', 'lentil', 'lentils', 'lepur', 'les', 'less', 'lettuce', 'leves', 'levi', 'libre', 'licorice', 'lid', 'light', 'lighter', 'lightly', 'lime', 'limed', 'limes', 'limoncello', 'lincolnshire', 'ling', 'lingonberries', 'linguine', 'linguini', 'linseed', 'linzertorte', 'lion', 'liqueur', 'liquor', 'liquorice', 'little', 'liver', 'livers', 'lo', 'loaf', 'loaves', 'lobscouse', 'lobster', 'log', 'logs', 'loin', 'lokshen', 'lollies', 'lollipop', 'lollipops', 'lomo', 'london', 'londonderry', 'long', 'longhorn', 'lorraine', 'lotus', 'loubet', 'louisiana', 'lovage', 'love', 'lovers', 'low', 'lucia', 'lucy', 'luilakbollen', 'lunchbox', 'lune', 'luscious', 'lush', 'luxe', 'luxury', 'lychee', 'lychees', 'lyonnaise', 'mac', 'macadamia', 'macanese', 'macaroni', 'macarons', 'macaroon', 'macaroons', 'macchi', 'macchiato', 'mace', 'macerated', 'machchi', 'macher', 'mackerel', 'madagascan', 'madame', 'made', 'madeira', 'madeleines', 'maderia', 'madras', 'magic', 'mai', 'mainstay', 'maize', 'make', 'maki', 'mala', 'malai', 'malay', 'malaysian', 'maldivian', 'mallard', 'malone', 'malt', 'maltaise', 'malted', 'malvern', 'maman', 'mammole', 'mamon', 'mamoosa', 'man', 'manchego', 'manchester', 'mandarin', 'mandioca', 'mandorle', 'maneesh', 'mangalorean', 'mange', 'mangetout', 'mango', 'mangsho', 'maple', 'marble', 'marbled', 'margarita', 'margherita', 'marie', 'marinade', 'marinara', 'marinated', 'marini', 'mariniere', 'marjolaine', 'marjoram', 'marmalade', 'marmitako', 'marnier', 'marrons', 'marrow', 'mars', 'marsala', 'marsh', 'marshmallow', 'marshmallows', 'martin', 'martini', 'martinique', 'maru', 'marula', 'mary', 'maryland', 'marzano', 'marzipan', 'masala', 'mascarpone', 'mash', 'mashed', 'mashwi', 'masour', 'massaman', 'matcha', 'matchstick', 'matchsticks', 'matrimony', 'mature', 'may', 'mayan', 'mayo', 'mayonnaise', 'mcsingh', 'mead', 'meal', 'mealie', 'meat', 'meatball', 'meatballs', 'meatloaf', 'meatzza', 'meculin', 'medallions', 'medieval', 'mediterranean', 'medium', 'medjool', 'medlars', 'medley', 'mee', 'meen', 'megrim', 'mein', 'melagrana', 'melanzane', 'melba', 'mele', 'melon', 'melt', 'melted', 'melting', 'melton', 'melts', 'membrillo', 'men', 'menta', 'mer', 'merguez', 'meringue', 'meringues', 'merluza', 'merrett', 'merry', 'mess', 'meuni', 'mex', 'mexican', 'meze', 'mezze', 'mice', 'micro', 'microwave', 'middle', 'midnight', 'migas', 'milanese', 'miles', 'milk', 'milkshake', 'milkshakes', 'millefeuille', 'millefeuilles', 'millionaire', 'mince', 'minced', 'mincemeat', 'minervois', 'minestrone', 'mini', 'miniature', 'mint', 'minted', 'minty', 'minute', 'miracle', 'mirch', 'mirin', 'mirror', 'mirza', 'miso', 'mississippi', 'misto', 'mitzuna', 'mix', 'mixed', 'mizuna', 'mkawra', 'mo', 'mocha', 'mock', 'moist', 'mojito', 'mojo', 'mokatines', 'molasses', 'molcajete', 'mole', 'molee', 'molle', 'molly', 'molten', 'mom', 'moments', 'mongolian', 'monkey', 'monkfish', 'monmouth', 'monsieur', 'monster', 'mont', 'monte', 'montenebro', 'monterey', 'monts', 'mooli', 'moorish', 'morecambe', 'morel', 'morels', 'mornay', 'morning', 'moroccan', 'morocco', 'morrocan', 'morroccan', 'morston', 'morteau', 'moscito', 'mother', 'moules', 'moussaka', 'mousse', 'mousseline', 'mousseron', 'mouthwatering', 'mowbray', 'mozzarella', 'mr', 'mtland', 'mu', 'mud', 'muek', 'muesli', 'muffin', 'muffins', 'mug', 'muhallabi', 'mujaddarah', 'mulberry', 'mulled', 'mullet', 'mulligatawny', 'multi', 'mum', 'murg', 'murgh', 'murghi', 'murgi', 'muscat', 'muscovado', 'mushroom', 'mushrooms', 'mushy', 'musical', 'mussakhan', 'mussel', 'mussels', 'mustard', 'mustia', 'mutton', 'myoga', 'na', 'naan', 'naanwich', 'nachos', 'nadiya', 'nage', 'napoleons', 'napoletana', 'nargis', 'nasi', 'nasturtium', 'nasu', 'natalizio', 'navarin', 'nduja', 'neapolitan', 'neck', 'nectar', 'nectarine', 'nectarines', 'neeps', 'negra', 'neige', 'neopolitan', 'nero', 'nest', 'nests', 'nettle', 'nettles', 'never', 'new', 'ngd', 'ni', 'nibbles', 'nibs', 'nice', 'nicky', 'nigel', 'night', 'nimish', 'noci', 'nog', 'noise', 'noisette', 'noisettes', 'non', 'nonya', 'noodle', 'noodles', 'norfolk', 'north', 'northumberland', 'norwegian', 'nose', 'nougat', 'nourishing', 'novelty', 'nuggets', 'nuoc', 'nut', 'nutmeg', 'nuts', 'nutty', 'oat', 'oatcake', 'oatcakes', 'oatmeal', 'oats', 'oaty', 'ocopa', 'octopus', 'oeufs', 'offal', 'oil', 'oilves', 'oise', 'oisin', 'ojingeo', 'okra', 'old', 'oli', 'olive', 'olives', 'olly', 'oloroso', 'omelette', 'omlek', 'one', 'onglet', 'onion', 'onions', 'oolong', 'oops', 'oozing', 'open', 'opera', 'orange', 'oranges', 'orangey', 'orchid', 'oregano', 'organic', 'oriental', 'original', 'orleans', 'orzo', 'oscietra', 'osso', 'ossobuco', 'ottoman', 'outstanding', 'oven', 'overnight', 'ox', 'oxford', 'oxtail', 'oyster', 'oysters', 'pa', 'package', 'pad', 'padr', 'padron', 'paella', 'pagne', 'pain', 'pak', 'pakora', 'pakoras', 'palak', 'palm', 'palmiers', 'paloise', 'pampushki', 'pan', 'panacotta', 'panang', 'pancake', 'pancakes', 'pancetta', 'panchporan', 'pandan', 'pandhi', 'pandoro', 'pane', 'paneed', 'paneer', 'panettone', 'panettones', 'panforte', 'pangrattato', 'panhaggerty', 'panko', 'panna', 'pannacotta', 'pansotti', 'panzanella', 'papardelle', 'papaya', 'paper', 'papeta', 'papillote', 'papillotte', 'pappa', 'pappardelle', 'paprika', 'parachute', 'paradise', 'paratha', 'parathas', 'parcel', 'parcels', 'pardina', 'parfait', 'paris', 'parisienne', 'parkin', 'parma', 'parmentier', 'parmesan', 'parmigiana', 'parmo', 'parnsip', 'parsee', 'parsley', 'parsnip', 'parsnips', 'partridge', 'partridges', 'party', 'pasanda', 'pasilla', 'pasquale', 'pasqualina', 'passata', 'passion', 'passionfruit', 'pasta', 'paste', 'pastiera', 'pasties', 'pastilla', 'pastis', 'pastrami', 'pastries', 'pastry', 'pasty', 'pat', 'pata', 'patagonian', 'patatas', 'patate', 'pate', 'patia', 'patties', 'paul', 'paupiette', 'pauvre', 'pavlova', 'pavlovas', 'paw', 'payasam', 'paysanne', 'pe', 'pea', 'peach', 'peaches', 'peachy', 'peacock', 'peanut', 'peanuts', 'pear', 'pearl', 'pears', 'peas', 'pease', 'pebbles', 'pecan', 'pecans', 'pecorino', 'peel', 'peeling', 'pegs', 'peking', 'pembrokeshire', 'penang', 'pencil', 'penne', 'people', 'pepe', 'peperonata', 'peperoncino', 'peperoni', 'pepper', 'peppercorn', 'peppercorns', 'peppered', 'peppermint', 'pepperoni', 'pepperpot', 'peppers', 'peppery', 'perch', 'perdu', 'perfect', 'perfectly', 'peri', 'perky', 'perry', 'persian', 'persillade', 'peruvian', 'pes', 'pesce', 'peshwari', 'pesto', 'petal', 'petalberry', 'petit', 'petits', 'pheasant', 'pho', 'pi', 'piadina', 'picada', 'piccalilli', 'pici', 'pickle', 'pickled', 'pickles', 'pickling', 'picnic', 'pico', 'pie', 'pieces', 'piedmontese', 'pierogi', 'pies', 'pig', 'pigeon', 'pigeons', 'pigs', 'pikelets', 'pilaf', 'pilaff', 'pilau', 'piment', 'pimenton', 'pimiento', 'pimientos', 'pimms', 'pin', 'pina', 'pinchos', 'pine', 'pineapple', 'pinenuts', 'pink', 'pinwheel', 'pinwheels', 'piperade', 'piquillo', 'piri', 'pisco', 'piselli', 'pisellini', 'pissaladi', 'pistaches', 'pistachio', 'pistachios', 'pistou', 'pithivier', 'pithiviers', 'pitta', 'pittas', 'pittsburgh', 'pizza', 'pizzaiola', 'pizzas', 'pizzette', 'pizzoccheri', 'pla', 'plaice', 'plait', 'plaited', 'plaits', 'plancha', 'plantain', 'plantains', 'plate', 'plateau', 'platter', 'plot', 'ploughdue', 'ploughman', 'plum', 'pluma', 'plums', 'po', 'poached', 'poacher', 'pockets', 'pog', 'poha', 'poire', 'pois', 'poivre', 'poke', 'polenta', 'polish', 'pollack', 'pollen', 'pollo', 'pollock', 'polonaise', 'polpettone', 'poly', 'pomegranate', 'pomegranates', 'pomme', 'pommes', 'pomodoro', 'pond', 'ponzu', 'pop', 'popcorn', 'poppadoms', 'poppadum', 'poppy', 'poppyseed', 'pops', 'porchetta', 'porcini', 'poriyal', 'pork', 'porridge', 'port', 'portobello', 'portuguesa', 'portuguese', 'posh', 'posset', 'pot', 'potato', 'potatoes', 'pots', 'potsticker', 'potted', 'pouilly', 'poulet', 'pound', 'poussin', 'poussins', 'poutine', 'povitica', 'powder', 'power', 'pra', 'praline', 'prawn', 'prawns', 'prepare', 'present', 'preserve', 'preserved', 'pressed', 'pressure', 'pretzels', 'primavera', 'prime', 'prince', 'prinsesst', 'profiterole', 'profiteroles', 'proof', 'proper', 'prosciutto', 'proscuitto', 'prosecco', 'prosperity', 'proven', 'provence', 'prune', 'prunes', 'pudding', 'puddings', 'puff', 'puffball', 'puffed', 'puffs', 'pugliese', 'pul', 'pulao', 'pullao', 'pulled', 'pulpo', 'pumpkin', 'punch', 'punjab', 'punjabi', 'pur', 'pure', 'puri', 'purple', 'purpose', 'purses', 'purslane', 'puttanesca', 'puy', 'px', 'pyramid', 'pyramids', 'pyrizhky', 'pytt', 'quackers', 'quadruple', 'quail', 'quails', 'queen', 'queijo', 'quenelles', 'quesada', 'quesadilla', 'quesadillas', 'quiche', 'quiches', 'quick', 'quickalilli', 'quince', 'quinces', 'quinoa', 'quorn', 'raan', 'rabbit', 'rack', 'racks', 'raclette', 'radicchio', 'radish', 'radishes', 'rag', 'rago', 'ragu', 'rahmbraten', 'rainbow', 'raised', 'raisin', 'raisins', 'raita', 'raj', 'rajma', 'ram', 'ramen', 'rancheros', 'range', 'rao', 'rapeseed', 'rarebit', 'ras', 'rasayana', 'rascals', 'raspberries', 'raspberry', 'ratatouille', 'ratte', 'ravioli', 'raviolo', 'raw', 'raymond', 'razor', 'real', 'really', 'reblochon', 'red', 'redcurrant', 'redcurrants', 'reduced', 'reduction', 'reflection', 'refried', 'refrigerator', 'regina', 'reindeer', 'religieuse', 'relish', 'remoulade', 'rendang', 'res', 'return', 'reuben', 'rhubarb', 'rib', 'ribbon', 'ribbons', 'ribs', 'rice', 'rich', 'rick', 'rico', 'ricotta', 'riesling', 'rieslingspaschteit', 'rillettes', 'rillons', 'rimmer', 'rinforzo', 'ring', 'rings', 'rioja', 'ripieni', 'ripple', 'risotto', 'rissoles', 'river', 'riz', 'road', 'roased', 'roast', 'roasted', 'roasties', 'roasting', 'robins', 'rock', 'rockefeller', 'rocket', 'rockpool', 'rocky', 'roe', 'rogan', 'rojak', 'roll', 'rolled', 'rolls', 'roly', 'romaine', 'roman', 'romana', 'romanoff', 'romero', 'romesco', 'ronique', 'root', 'roots', 'roquamole', 'roquefort', 'rosace', 'rosato', 'roscoff', 'rose', 'rosemary', 'rosette', 'rosewater', 'rosse', 'rossini', 'rosti', 'rostis', 'rota', 'roti', 'rotolo', 'rouge', 'rough', 'rouille', 'roulade', 'roux', 'rowies', 'royal', 'royale', 'royals', 'rta', 'rub', 'rubbed', 'ruby', 'rucola', 'rudolph', 'rum', 'rumbledethumps', 'rump', 'rumtopf', 'runner', 'russe', 'russian', 'rustic', 'rustico', 'rye', 'saag', 'saas', 'sabayon', 'sabl', 'sable', 'sables', 'sachertorte', 'saddle', 'sadza', 'saffron', 'saganaki', 'sage', 'saikyo', 'saint', 'salad', 'salade', 'salame', 'salami', 'sali', 'salmi', 'salmon', 'salmoriglio', 'salsa', 'salsify', 'salt', 'saltado', 'salted', 'saltfish', 'saltimbocca', 'salut', 'salzburg', 'sambar', 'sambuca', 'samosa', 'samosas', 'samphire', 'san', 'sand', 'sandwich', 'sandwiches', 'sangchae', 'sangria', 'sansho', 'sao', 'saor', 'sapeur', 'sarde', 'sardine', 'sardines', 'sardinian', 'sarnie', 'sarnies', 'sarsaparilla', 'sashimi', 'satay', 'satsumas', 'saturday', 'sauce', 'sauces', 'saucisson', 'sauer', 'sauerkraut', 'sausage', 'sausagemeat', 'sausages', 'saut', 'sauternes', 'savarin', 'savoury', 'savoy', 'scad', 'scales', 'scallion', 'scallop', 'scallops', 'scampi', 'scandinavian', 'scapece', 'scapes', 'scarlet', 'scary', 'scented', 'schichttorte', 'schnecken', 'schnitzel', 'schnitzels', 'schokogugelhupf', 'scone', 'scones', 'scorched', 'scotch', 'scottish', 'scouse', 'scrambled', 'scraps', 'scratchings', 'scrolls', 'scrumpy', 'sea', 'seabass', 'seafood', 'seared', 'seashell', 'seaside', 'season', 'seasonal', 'seasoned', 'seasoning', 'seaweed', 'second', 'secret', 'seed', 'seeded', 'seeds', 'seedy', 'seek', 'seekh', 'seera', 'segments', 'selection', 'semifreddo', 'semolina', 'senegalese', 'serrano', 'served', 'sesame', 'seven', 'seville', 'sgombro', 'shahi', 'shake', 'shaken', 'shakshuka', 'shallot', 'shallots', 'sham', 'shami', 'shank', 'shanks', 'shaped', 'shards', 'share', 'sharing', 'sharlotka', 'sharp', 'shashlik', 'shatkora', 'shaved', 'shavings', 'shawarma', 'sheek', 'sheep', 'sheeps', 'sheera', 'sheftalia', 'shell', 'shellfish', 'shells', 'shepherd', 'sherbet', 'sherry', 'shetland', 'shichimi', 'shiitake', 'shikar', 'shimeji', 'shin', 'shinni', 'shirley', 'shish', 'shiso', 'shoestring', 'shoot', 'shoots', 'shorshe', 'short', 'shortbread', 'shortbreads', 'shortcake', 'shortcakes', 'shortcrust', 'shortcut', 'shots', 'shoulder', 'showstopper', 'showstoppers', 'shredded', 'shreds', 'shrikand', 'shrikhand', 'shrimp', 'shrimps', 'shropshire', 'shu', 'si', 'sichuan', 'sicilian', 'silk', 'simmered', 'simnel', 'simon', 'simple', 'sin', 'singapore', 'sioh', 'sirloin', 'siu', 'sized', 'sizzler', 'sizzling', 'skate', 'skewer', 'skewered', 'skewers', 'skin', 'skink', 'skinless', 'skinned', 'skinny', 'skinnylicious', 'skins', 'skirlie', 'skirt', 'skolebr', 'skordalia', 'skye', 'slater', 'slaw', 'slice', 'sliced', 'slices', 'sliders', 'slightly', 'sling', 'slipcote', 'sloe', 'sloppy', 'slow', 'small', 'smashed', 'smiles', 'smoked', 'smokie', 'smoky', 'smooth', 'smoothie', 'smothered', 'snail', 'snake', 'snap', 'snapper', 'snaps', 'snazzy', 'snickers', 'snow', 'snowman', 'soaked', 'soba', 'socca', 'soda', 'soffritto', 'soft', 'softened', 'softly', 'sogliola', 'soi', 'soil', 'soldiers', 'sole', 'somerset', 'sorbet', 'sorrel', 'souchong', 'souffl', 'soup', 'sour', 'sourdough', 'soured', 'soused', 'south', 'southern', 'soutzoukakia', 'souvlaki', 'souvlakia', 'soy', 'soya', 'sp', 'space', 'spada', 'spaghetti', 'spaghettini', 'spanakopita', 'spanische', 'spanish', 'spare', 'spargel', 'sparkle', 'sparkles', 'sparkling', 'spatchcock', 'spatchcocked', 'spatzle', 'spears', 'special', 'speck', 'speculaas', 'speedy', 'spelt', 'spice', 'spiced', 'spices', 'spicy', 'spider', 'spigola', 'spillers', 'spinach', 'spinaci', 'spirals', 'splashed', 'splendidly', 'split', 'splits', 'sponge', 'sponges', 'spotted', 'spread', 'spring', 'sprinkles', 'spritzer', 'sprout', 'sprouting', 'sprouts', 'spruced', 'spun', 'squab', 'squares', 'squash', 'squashes', 'squeak', 'squid', 'squirrel', 'sri', 'ssamjang', 'st', 'stack', 'stacks', 'staffordshire', 'stained', 'stamped', 'star', 'stargazey', 'starter', 'stawberry', 'steak', 'steaks', 'steamed', 'steeped', 'stefan', 'stein', 'stem', 'stems', 'step', 'stew', 'stewed', 'sti', 'sticks', 'sticky', 'stillton', 'stilton', 'stinging', 'stinking', 'stir', 'stis', 'stock', 'stollen', 'stone', 'store', 'stout', 'stovies', 'strand', 'strata', 'straw', 'strawberries', 'strawberry', 'straws', 'streaky', 'stress', 'streusel', 'strips', 'stroganoff', 'stromboli', 'strudel', 'strudels', 'struffoli', 'studded', 'stuffed', 'stuffin', 'stuffing', 'stuffings', 'style', 'succotash', 'succulent', 'suckling', 'suet', 'suffolk', 'sugar', 'sugared', 'suissesse', 'sukiyaki', 'sukka', 'sultan', 'sultana', 'sultanas', 'sum', 'sumac', 'sumiko', 'summer', 'summerberry', 'summery', 'sumptuous', 'sun', 'sunblush', 'sundae', 'sundaes', 'sunflower', 'sunny', 'sunrise', 'sunshine', 'super', 'superfood', 'supper', 'supr', 'surf', 'suriani', 'surprise', 'sushi', 'sussex', 'suzette', 'swahili', 'swans', 'swede', 'swedish', 'sweet', 'sweetbread', 'sweetbreads', 'sweetcorn', 'sweetheart', 'swirl', 'swirls', 'swiss', 'swordfish', 'syllabub', 'syrian', 'syrup', 'tabbouleh', 'tablier', 'taboon', 'taboulleh', 'taco', 'tacos', 'tadka', 'tagine', 'tagliarini', 'tagliata', 'tagliatelle', 'taglierini', 'tagliolini', 'tags', 'tahini', 'tail', 'tails', 'taleggio', 'talla', 'tamago', 'tamale', 'tamarind', 'tandoori', 'tangerines', 'tangiers', 'tangy', 'tapas', 'tapenade', 'taquitos', 'tar', 'taramasalata', 'tarator', 'tarka', 'tarkari', 'tarragon', 'tart', 'tartar', 'tartare', 'tarte', 'tartiflette', 'tartlets', 'tarts', 'tasted', 'tasty', 'tatin', 'tatins', 'tattie', 'tatties', 'taukwa', 'tav', 'te', 'tea', 'teacakes', 'teacup', 'teal', 'tear', 'teau', 'teaux', 'tempered', 'temple', 'temptation', 'tempura', 'ten', 'tenderloin', 'tenderstem', 'tendon', 'tennis', 'tequila', 'teriyaki', 'terrine', 'teryaki', 'tex', 'texan', 'textured', 'thai', 'thermidor', 'thick', 'thigh', 'thighs', 'thin', 'thins', 'thoran', 'thousand', 'thread', 'three', 'throw', 'thyme', 'tian', 'ticada', 'tidy', 'tielle', 'tier', 'tiered', 'tiger', 'tikka', 'tilapia', 'timbale', 'timbales', 'time', 'tinned', 'tiny', 'tips', 'tipsy', 'tiramis', 'tiramisini', 'tiramisu', 'tissi', 'toad', 'toast', 'toasted', 'toastie', 'toasts', 'toasty', 'toffee', 'tofu', 'together', 'toise', 'tom', 'tomatillo', 'tomato', 'tomatoes', 'ton', 'tongue', 'tonic', 'tonkatsu', 'tonnato', 'tons', 'top', 'topped', 'topping', 'toppings', 'tops', 'topside', 'torbay', 'torched', 'toridashi', 'torn', 'torrese', 'torroncino', 'torrone', 'torta', 'torte', 'tortelli', 'tortellini', 'tortiglioni', 'tortilla', 'tortillas', 'toscana', 'toscano', 'tossed', 'tostadas', 'totally', 'toulouse', 'tournedos', 'tout', 'tower', 'towers', 'towey', 'town', 'traditional', 'tranche', 'transit', 'trapanese', 'travelling', 'tray', 'traybake', 'traybaked', 'treacle', 'treats', 'tree', 'trencher', 'triangle', 'triangles', 'tricolor', 'trifle', 'trifolati', 'trimmed', 'trimmings', 'trinity', 'trio', 'tripe', 'triple', 'tripolium', 'tropical', 'trotter', 'trout', 'truffle', 'truffled', 'truffles', 'tso', 'tsoureki', 'tubetti', 'tuile', 'tuiles', 'tuille', 'tumbet', 'tuna', 'tunis', 'turbigo', 'turbot', 'turf', 'turkey', 'turkish', 'turmeric', 'turnip', 'turnips', 'turnover', 'turnovers', 'tuscan', 'tusha', 'tutti', 'twice', 'twist', 'twister', 'twists', 'twisty', 'two', 'txistorra', 'tzatziki', 'tzle', 'udon', 'ulster', 'ultimate', 'umami', 'umeboshi', 'unbelievable', 'underground', 'union', 'ups', 'upside', 'urad', 'vacherin', 'valencian', 'valensole', 'valentine', 'vanilla', 'vanille', 'variations', 'various', 'veal', 'veg', 'vegan', 'vegetable', 'vegetables', 'vegetarian', 'veggie', 'veggies', 'velodrome', 'velout', 'veloute', 'velvet', 'velvety', 'venetian', 'veneziana', 'venison', 'vents', 'verbena', 'verde', 'verdure', 'verjus', 'vermicelli', 'vermouth', 'veronique', 'verte', 'vevichathu', 'via', 'vichy', 'vichyssoise', 'victoria', 'viennese', 'vierge', 'vietnamese', 'vignarola', 'vignotte', 'vin', 'vinaigre', 'vinaigrette', 'vincisgrassi', 'vindaloo', 'vine', 'vinegar', 'violet', 'virgin', 'vitello', 'vodka', 'vol', 'volcanoes', 'vongole', 'wafer', 'wafers', 'waffle', 'waffles', 'wagyu', 'wakame', 'waldorf', 'wallbanger', 'walnut', 'walnuts', 'warm', 'warmed', 'wasabi', 'wastenot', 'water', 'watercress', 'watermelon', 'way', 'ways', 'wedding', 'wedges', 'wee', 'weeping', 'wellington', 'wellingtons', 'welsh', 'wensleydale', 'wenslydale', 'west', 'westie', 'wet', 'wheat', 'wheaten', 'wheel', 'whip', 'whipped', 'whips', 'whirl', 'whirls', 'whiskey', 'whisky', 'white', 'whitebait', 'whiting', 'whitstable', 'whole', 'wholegrain', 'wholemeal', 'wholewheat', 'whoopie', 'wight', 'wild', 'william', 'wilted', 'window', 'windtorte', 'wine', 'wing', 'wings', 'winter', 'witchill', 'wobbly', 'wok', 'wontons', 'wood', 'worcestershire', 'woven', 'wrap', 'wrapped', 'wraps', 'wreath', 'wreck', 'xeo', 'xo', 'yaan', 'yaki', 'yakitori', 'yam', 'yarg', 'yasai', 'yellow', 'yoghurt', 'yolk', 'york', 'yorkshire', 'yorkshires', 'young', 'yukhoe', 'yule', 'yum', 'yung', 'yunnanese', 'yusheng', 'yuzu', 'za', 'zabaglione', 'zampone', 'zanzibar', 'zemlovka', 'zest', 'zesty', 'zingy', 'zoe', 'zombie', 'zucca', 'zucchero', 'zucchini', 'zuccotto', 'zupfe']\n"
     ]
    }
   ],
   "source": [
    "tf_vectorizer = TfidfVectorizer(stop_words=stopwords_nltk)\n",
    "titles_tfidf = tf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(tf_vectorizer.get_feature_names()) #resulting tokens very similar to above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA on CtVec titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:02.319412Z",
     "start_time": "2019-12-09T00:32:01.899755Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11877882891003717"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_10 = TruncatedSVD(10)  #Dimensionality reduction using truncated SVD (aka LSA).\n",
    "#10 topics\n",
    "t_lsa_10 = lsa_10.fit_transform(titles_ct)\n",
    "lsa_10.explained_variance_ratio_.sum()\n",
    "#explained variance ratio too low? .sum() only 12%\n",
    "#only 46% for 100 topics! need to clean further to get higher variance ration with a reasonable number of topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:02.340315Z",
     "start_time": "2019-12-09T00:32:02.330930Z"
    }
   },
   "outputs": [],
   "source": [
    "#from proj4 and topic modelling/LSA/NMF lecture\n",
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:02.368202Z",
     "start_time": "2019-12-09T00:32:02.346565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "sauce, salad, chicken, fried, roasted, tomato, potato, cheese, chilli, roast\n",
      "\n",
      "Topic  1\n",
      "sauce, chocolate, cream, wine, pork, fried, pudding, ice, cake, roast\n",
      "\n",
      "Topic  2\n",
      "chicken, roast, fried, rice, curry, stuffed, stir, pie, garlic, potatoes\n",
      "\n",
      "Topic  3\n",
      "chocolate, chicken, cream, salad, cake, ice, pudding, orange, white, mousse\n",
      "\n",
      "Topic  4\n",
      "cream, chocolate, roasted, potato, lamb, cheese, cake, sweet, pudding, soup\n",
      "\n",
      "Topic  5\n",
      "fried, pan, chocolate, deep, egg, cream, stir, rice, chilli, salmon\n",
      "\n",
      "Topic  6\n",
      "cheese, potato, goats, tomato, soup, sweet, cream, blue, onion, baked\n",
      "\n",
      "Topic  7\n",
      "cheese, tomato, lamb, roasted, chocolate, goats, potatoes, stuffed, garlic, tomatoes\n",
      "\n",
      "Topic  8\n",
      "chocolate, potato, soup, sweet, chilli, red, beef, lamb, tomato, onion\n",
      "\n",
      "Topic  9\n",
      "pork, cheese, roasted, apple, chocolate, goats, belly, fried, salad, cabbage\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa_10, ct_vectorizer.get_feature_names(), 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:02.374589Z",
     "start_time": "2019-12-09T00:32:02.371299Z"
    }
   },
   "outputs": [],
   "source": [
    "#topics all over the place, and most have chocolate!\n",
    "\n",
    "#try TF-IDF, NMF, *LDA and spacy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA on titles vectorized by TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:02.487295Z",
     "start_time": "2019-12-09T00:32:02.379642Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0046158 , 0.00769671, 0.00703018, 0.00662758, 0.00643933,\n",
       "       0.00570901, 0.00548669, 0.00539797, 0.00516311, 0.00497743])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_10 = TruncatedSVD(10)  #10 topics\n",
    "t_tf_lsa_10 = lsa_10.fit_transform(titles_tfidf)\n",
    "lsa_10.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:02.518068Z",
     "start_time": "2019-12-09T00:32:02.496765Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "salad, chicken, sauce, fried, tomato, potato, roasted, chilli, cheese, soup\n",
      "\n",
      "Topic  1\n",
      "chocolate, sauce, cake, cream, pudding, orange, ice, mousse, white, raspberry\n",
      "\n",
      "Topic  2\n",
      "chicken, roast, lemon, pie, curry, sauce, rice, fried, chocolate, stir\n",
      "\n",
      "Topic  3\n",
      "soup, potato, sweet, sauce, pork, roast, leek, pea, roasted, red\n",
      "\n",
      "Topic  4\n",
      "soup, chicken, chocolate, potato, cake, pie, salad, cheese, cream, leek\n",
      "\n",
      "Topic  5\n",
      "cheese, goats, roast, sauce, stuffed, lamb, roasted, pie, souffl, potatoes\n",
      "\n",
      "Topic  6\n",
      "tomato, sauce, soup, chicken, chocolate, salad, pasta, spinach, basil, white\n",
      "\n",
      "Topic  7\n",
      "cheese, fried, goats, chilli, souffl, rice, pan, stir, egg, blue\n",
      "\n",
      "Topic  8\n",
      "pudding, tomato, apple, bread, butter, pie, soup, toffee, cream, rice\n",
      "\n",
      "Topic  9\n",
      "lamb, chilli, roasted, garlic, chocolate, potatoes, roast, soup, cake, squash\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa_10, tf_vectorizer.get_feature_names(), 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:02.525924Z",
     "start_time": "2019-12-09T00:32:02.522571Z"
    }
   },
   "outputs": [],
   "source": [
    "#Topic 1 is dessert, still messy though - bi-grams? or use instructions instead - lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:02.603338Z",
     "start_time": "2019-12-09T00:32:02.529583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00461168, 0.00769694, 0.00703141, 0.00663835, 0.00642146])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5 topics?\n",
    "lsa_5 = TruncatedSVD(5)  #5 topics\n",
    "t_tf_lsa_5 = lsa_5.fit_transform(titles_tfidf)\n",
    "lsa_5.explained_variance_ratio_\n",
    "#lsa_5.explained_variance_ratio_.sum() #only 3% of variance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:02.629017Z",
     "start_time": "2019-12-09T00:32:02.605952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "salad, chicken, sauce, fried, potato, tomato, roasted, chilli, cheese, soup\n",
      "\n",
      "Topic  1\n",
      "chocolate, sauce, cake, cream, pudding, orange, ice, mousse, white, raspberry\n",
      "\n",
      "Topic  2\n",
      "chicken, curry, lemon, roast, pie, chocolate, rice, stir, fried, sauce\n",
      "\n",
      "Topic  3\n",
      "soup, sauce, potato, sweet, fried, pork, roast, red, leek, roasted\n",
      "\n",
      "Topic  4\n",
      "soup, chicken, potato, chocolate, cheese, cake, cream, pudding, leek, sweet\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa_5, tf_vectorizer.get_feature_names(), 10) #fewer topics, cutoff earlier, same as top 5 topics in 10 topic case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CtVec+ NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:03.754102Z",
     "start_time": "2019-12-09T00:32:02.635482Z"
    }
   },
   "outputs": [],
   "source": [
    "nmf_10 = NMF(10) \n",
    "t_nmf_10 = nmf_10.fit_transform(titles_ct) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:03.791601Z",
     "start_time": "2019-12-09T00:32:03.765304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "sauce, tomato, wine, red, apple, fillet, steak, butter, roast, pork, mushroom, poached, duck, wild, vierge, pasta, mustard, ed, toffee, potatoes\n",
      "\n",
      "Topic  1\n",
      "salad, dressing, warm, tomato, salmon, bean, herb, smoked, spinach, green, chilli, fennel, thai, grilled, noodle, rocket, cucumber, beef, style, watercress\n",
      "\n",
      "Topic  2\n",
      "chicken, roast, curry, rice, lemon, stuffed, pie, chilli, breast, thai, liver, chorizo, spiced, spicy, mushroom, garlic, honey, style, stir, potatoes\n",
      "\n",
      "Topic  3\n",
      "chocolate, cake, orange, white, mousse, pudding, raspberry, hot, cheesecake, caramel, souffl, tart, chilli, brownies, fondant, coffee, baked, bread, dark, sponge\n",
      "\n",
      "Topic  4\n",
      "roasted, pork, tomatoes, potatoes, red, pepper, apple, chilli, stuffed, lemon, cabbage, garlic, honey, belly, vegetables, sea, slow, onion, squash, duck\n",
      "\n",
      "Topic  5\n",
      "fried, pan, egg, deep, stir, rice, chilli, salmon, sea, bass, mushrooms, beef, tomato, spicy, onion, salsa, broccoli, wild, noodles, courgette\n",
      "\n",
      "Topic  6\n",
      "cheese, goats, tomato, blue, onion, baked, spinach, souffl, stuffed, ham, beetroot, cauliflower, tart, wrapped, chutney, toast, mushrooms, salsa, parma, bacon\n",
      "\n",
      "Topic  7\n",
      "potato, sweet, soup, red, sour, mash, beef, onion, spinach, pie, leek, rosti, smoked, spicy, egg, bacon, poached, salmon, cakes, wedges\n",
      "\n",
      "Topic  8\n",
      "cream, ice, pudding, vanilla, lemon, strawberry, cake, apple, tart, fruit, spiced, salmon, clotted, ginger, sponge, rhubarb, orange, pistachio, smoked, honey\n",
      "\n",
      "Topic  9\n",
      "lamb, roast, potatoes, garlic, spiced, rosemary, crusted, herb, shoulder, beans, slow, mint, grilled, braised, rack, chilli, chops, stuffed, saut, red\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_10, ct_vectorizer.get_feature_names(), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset, NMF topics seem to make more sense (somewhat divided) than LSA ones (chocolate in every topic). \n",
    "Can still improve before vectorization and dim reduction though\n",
    "\n",
    "CtVec + NMF: \n",
    "* Topic 0 steak, pasta, roast\n",
    "* Topic 1 salad, vegetables\n",
    "* Topic 2 curry, rice\n",
    "* Topic 3 desserts, chocolate, coffee (darker colour?)\n",
    "* Topic 4 roast\n",
    "* Topic 5 pan-fried, eggs and fish\n",
    "* Topic 6 cheese, baked\n",
    "* Topic 7 potatoe, soup\n",
    "* Topic 8 desserts, ice cream, tart, cake (lighter colours)\n",
    "* Topic 9 lamb roast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF+ NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:05.853636Z",
     "start_time": "2019-12-09T00:32:03.838009Z"
    }
   },
   "outputs": [],
   "source": [
    "nmf_10 = NMF(10) \n",
    "t_tf_nmf_10 = nmf_10.fit_transform(titles_tfidf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:05.880407Z",
     "start_time": "2019-12-09T00:32:05.856217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "salad, dressing, warm, salmon, bean, smoked, herb, spinach, tomato, noodle, bacon, cucumber, fennel, mango, green, thai, rocket, orange, mackerel, style\n",
      "\n",
      "Topic  1\n",
      "chocolate, cake, cream, orange, mousse, white, ice, raspberry, tart, cheesecake, souffl, hot, lemon, caramel, strawberry, vanilla, brownies, fruit, coffee, hazelnut\n",
      "\n",
      "Topic  2\n",
      "chicken, curry, lemon, pie, thai, roast, liver, rice, chorizo, mushroom, stuffed, casserole, style, breast, skewers, korma, coconut, jerk, spicy, noodles\n",
      "\n",
      "Topic  3\n",
      "sauce, tomato, pasta, wine, mushroom, basil, meatballs, red, poached, steak, pancakes, vierge, cherry, apple, fillet, creamy, wild, fish, cream, bass\n",
      "\n",
      "Topic  4\n",
      "soup, pea, spinach, leek, noodle, cream, mushroom, vegetable, broccoli, squash, butternut, mint, cro, creamy, tons, stilton, poached, lentil, parsnip, sweetcorn\n",
      "\n",
      "Topic  5\n",
      "fried, pan, chilli, stir, egg, salmon, rice, deep, beef, fry, smoked, broccoli, sea, noodles, prawns, poached, bass, mushrooms, spicy, squid\n",
      "\n",
      "Topic  6\n",
      "potato, sweet, pie, sour, mash, leek, rosti, curry, spicy, wedges, beef, cakes, gratin, fish, mashed, chips, onion, sausage, stew, cake\n",
      "\n",
      "Topic  7\n",
      "cheese, goats, souffl, blue, onion, baked, cauliflower, stuffed, spinach, ham, beetroot, tart, toast, mushrooms, wrapped, pizza, parma, bacon, walnut, macaroni\n",
      "\n",
      "Topic  8\n",
      "pudding, rice, bread, butter, apple, toffee, sponge, sticky, christmas, cream, coconut, black, steamed, custard, vanilla, ginger, caramelised, ice, yorkshire, pineapple\n",
      "\n",
      "Topic  9\n",
      "roasted, lamb, roast, pork, potatoes, garlic, stuffed, spiced, red, lemon, tomatoes, rosemary, honey, pepper, saut, slow, ed, cabbage, braised, shoulder\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_10, tf_vectorizer.get_feature_names(), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For TFIDF and NMF, topics 1 and 8 are desserts, Topic 4 is a bit strange (salmon and meringue)\n",
    "\n",
    "* Topic 0 salad\n",
    "* Topic 1 dessert, chocolate\n",
    "* Topic 2 curry\n",
    "* Topics 3 soup, vegetarian\n",
    "* Topic 4 dessert? (except for sole and salmon)\n",
    "* Topic 5 pan-fried, eggs and fish, noodles\n",
    "* Topic 6 roast - pork, lamb, steak, duck\n",
    "* Topic 7 cheese, roasted, baked, pizza\n",
    "* Topic 8 dessert, rice pudding, toffee, sponge cake (non-chocolate)\n",
    "* Topic 9 potatoes, meat pies and stews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:05.888532Z",
     "start_time": "2019-12-09T00:32:05.883773Z"
    }
   },
   "outputs": [],
   "source": [
    "#try LDA later?\n",
    "#better cleaning and lemmatization?\n",
    "#or can use these for recommendations already?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:05.898369Z",
     "start_time": "2019-12-09T00:32:05.892045Z"
    }
   },
   "outputs": [],
   "source": [
    "#how to group topics for filtering? cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize recipe instructions\n",
    "\n",
    "Now it seems it would make more sense to lemmatize 'instructions' column and use those for topics (include both ingredients and cooking process, which can imply on the course of meal (boil, pan-fried less likely to be desserts than bake);\n",
    "\n",
    "instructions give the most information, contains ingredients, and can imply the course of meal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:05.916091Z",
     "start_time": "2019-12-09T00:32:05.902229Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Preheat the oven to 200C/400F/Gas 6., For the...\n",
       "1    [Cook the pasta in a pan of boiling salted wat...\n",
       "2    [To make the basic dough, line a baking tray w...\n",
       "3    [Preheat the oven to 220C/200C Fan/Gas 7., For...\n",
       "4    [Place the rib-eye of beef into a large non-me...\n",
       "Name: instructions, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use tfidf + NMF (most clearly divided)\n",
    "instructions = bbcdata['instructions']\n",
    "instructions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean text with regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:05.923497Z",
     "start_time": "2019-12-09T00:32:05.919837Z"
    }
   },
   "outputs": [],
   "source": [
    "#clean text with regex, ftn defined above\n",
    "# def regex_nodigits(s):\n",
    "#     '''use regex to clean string: \n",
    "#     get rid of numbers, punctuations and capitalized letters''' \n",
    "#     s = re.sub(r'[\\d]','',str(s).lower())#convert to lower case, replace digits with empty str\n",
    "#     s = re.sub(r'[\\W]','',str(s))#replace puctuations with empty str\n",
    "#     return s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:09.432123Z",
     "start_time": "2019-12-09T00:32:05.927749Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      preheat the oven to c f gas      for the piz...\n",
       "1      cook the pasta in a pan of boiling salted wa...\n",
       "2      to make the basic dough  line a baking tray ...\n",
       "3      preheat the oven to c c fan gas      for the...\n",
       "4      place the rib eye of beef into a large non m...\n",
       "Name: instructions, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cld_ins=instructions.apply(regex_nodigits)\n",
    "cld_ins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:09.471778Z",
     "start_time": "2019-12-09T00:32:09.443192Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  preheat the oven to c f gas      for the pizza base  place the flour  oil  water and salt into a food processor and blend together until a dough is formed  tip out onto a floured work surface and knead  shape into a round base about cm in wide     place into a frying pan over a high heat and brown the base  then using a mini blowtorch  crisp the top of the pizza   alternatively you can do this under the grill      for the topping  spread tomato pur e over the top of the base     fry the mushrooms in a dry frying pan then scatter over the tomato pur e  arrange the prosciutto and cheese on top     crack an egg into the middle  then place into the oven for five minutes to finish cooking     serve on a large plate  and slice into wedges to serve   '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cld_ins[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF + NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:14.080108Z",
     "start_time": "2019-12-09T00:32:09.474858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'abaking', 'abandon', 'abd', 'ability', 'able', 'aboutcm', 'absolute', 'absolutely', 'absorb', 'absorbed', 'absorbency', 'absorbent', 'absorbing', 'absorbs', 'absorption', 'abundant', 'aburaage', 'accelerate', 'acceptable', 'accessible', 'accidentally', 'accommodate', 'accompainment', 'accompanied', 'accompaniment', 'accompaniments', 'accompany', 'accord', 'according', 'accordingly', 'accorning', 'account', 'accross', 'accumulate', 'accumulated', 'accumulates', 'accurate', 'accurately', 'acetate', 'achieve', 'achieved', 'achieves', 'acid', 'acidic', 'acidity', 'acidulated', 'ackee', 'acm', 'acorn', 'acorns', 'acquire', 'acrid', 'across', 'act', 'acting', 'action', 'activate', 'activated', 'active', 'activity', 'actual', 'actually', 'ad', 'add', 'adddrop', 'added', 'adding', 'addition', 'additional', 'additions', 'adds', 'addtablespoons', 'addtbsp', 'addthe', 'adhere', 'adheres', 'adhering', 'adhesive', 'aditional', 'adjacent', 'adjoining', 'adjust', 'adjusting', 'adjustments', 'admittedly', 'adobo', 'adorn', 'aduki', 'adult', 'adults', 'advance', 'advisable', 'advocaat', 'aer', 'aerate', 'aerated', 'aesthetic', 'aesthetically', 'affect', 'afford', 'afraid', 'afterwards', 'agar', 'agave', 'age', 'aged', 'agents', 'aggressively', 'agitate', 'agitating', 'agretti', 'agur', 'ahead', 'aid', 'aim', 'aiming', 'aioli', 'air', 'airily', 'airing', 'airtight', 'airy', 'aj', 'ajar', 'aji', 'ajo', 'ajowan', 'ajwain', 'al', 'alarmed', 'alaska', 'albondigas', 'albumen', 'albumina', 'alcohol', 'alcoholic', 'ale', 'alegar', 'alexander', 'alfalfa', 'algin', 'alginate', 'alight', 'alla', 'allow', 'allowed', 'allowing', 'allows', 'allspice', 'almond', 'almonds', 'almost', 'alone', 'along', 'alongside', 'aloo', 'aloow', 'aloud', 'alphabet', 'already', 'alsatian', 'also', 'alter', 'alternate', 'alternately', 'alternating', 'alternative', 'alternatively', 'although', 'altogether', 'alumimium', 'aluminium', 'aluminum', 'always', 'amalgamate', 'amalgamated', 'amaranth', 'amaretti', 'amaretto', 'amarillo', 'amber', 'amchur', 'amel', 'american', 'among', 'amongst', 'amoretti', 'amoul', 'amount', 'amounts', 'ample', 'anaemic', 'ancho', 'anchoiade', 'anchovade', 'anchovies', 'anchovy', 'angel', 'angeli', 'angelica', 'anglais', 'anglaise', 'angle', 'angled', 'angles', 'anglesey', 'angling', 'angostura', 'animal', 'anise', 'aniseed', 'anolini', 'another', 'ans', 'anti', 'antipasti', 'antlers', 'anty', 'anything', 'anyway', 'anywhere', 'apart', 'apex', 'apostles', 'appear', 'appearance', 'appeared', 'appearing', 'appears', 'appetising', 'appetites', 'appetizing', 'apple', 'apples', 'applicable', 'applied', 'applies', 'apply', 'applying', 'apprimately', 'approach', 'approaches', 'approaching', 'appropriate', 'appropriately', 'approx', 'approximate', 'approximately', 'apricot', 'apricots', 'aprons', 'aquavit', 'arancina', 'arancine', 'arancini', 'arborio', 'arbroath', 'arch', 'arched', 'arches', 'architectural', 'arctic', 'area', 'areas', 'argan', 'arm', 'armagnac', 'arms', 'arnaise', 'aroma', 'aromas', 'aromatic', 'aromatics', 'arome', 'around', 'arrabbiata', 'arrabiata', 'arrange', 'arranged', 'arrangement', 'arrangements', 'arranging', 'arrive', 'arrowroot', 'arroz', 'artichoke', 'artichokes', 'asafoetida', 'ash', 'ashen', 'ashes', 'asian', 'asid', 'aside', 'ask', 'askew', 'asparagus', 'assemble', 'assembled', 'assembling', 'assembly', 'assess', 'asset', 'assist', 'assisted', 'assorted', 'assortment', 'assuming', 'aster', 'astor', 'astringent', 'atar', 'atmosphere', 'atomiser', 'atop', 'atsina', 'attach', 'attached', 'attaching', 'attachment', 'attack', 'attempt', 'attempting', 'attention', 'attractive', 'attractively', 'au', 'aubergine', 'aubergines', 'auspicious', 'austere', 'australian', 'auternes', 'authentic', 'auto', 'autumn', 'available', 'average', 'avocado', 'avocadoes', 'avocados', 'avoid', 'avoiding', 'avour', 'avruga', 'aware', 'away', 'baba', 'babas', 'babking', 'baby', 'back', 'backbone', 'background', 'backing', 'backs', 'backward', 'backwards', 'bacon', 'bacteria', 'bad', 'bag', 'bagel', 'bagels', 'baggy', 'bagna', 'bags', 'baguette', 'baguettes', 'baharat', 'bain', 'bak', 'bake', 'baked', 'bakee', 'bakes', 'bakestone', 'bakeware', 'bakewell', 'bakign', 'baking', 'baklava', 'baklavas', 'balance', 'balanced', 'balck', 'ball', 'baller', 'balloon', 'ballotine', 'balls', 'balm', 'balsamic', 'balti', 'bamboo', 'bambrack', 'banana', 'bananas', 'band', 'bands', 'bang', 'bangers', 'banging', 'bangladeshi', 'bangs', 'banh', 'banish', 'banneton', 'banoffee', 'bantam', 'bao', 'bap', 'bar', 'bara', 'barbados', 'barbecue', 'barbecued', 'barbecues', 'barbecuing', 'barbeque', 'barberries', 'barbie', 'bard', 'bare', 'barely', 'barest', 'bark', 'barley', 'barmbrack', 'barnacles', 'barnsley', 'barquette', 'barquillo', 'barquillos', 'barramundi', 'barrel', 'barrels', 'bars', 'bas', 'base', 'based', 'bases', 'bash', 'bashed', 'bashing', 'basic', 'basically', 'basil', 'basin', 'basins', 'basis', 'basket', 'baskets', 'basmati', 'basque', 'bass', 'baste', 'basted', 'basting', 'bat', 'batch', 'batches', 'bath', 'baton', 'batons', 'batter', 'battered', 'batters', 'battlements', 'bauble', 'baubles', 'baulk', 'bavarois', 'bavette', 'bay', 'bayleaf', 'bayleaves', 'bbq', 'bbring', 'beacomes', 'beads', 'beak', 'beaks', 'bean', 'beancurd', 'beans', 'beansprouts', 'beany', 'bear', 'beard', 'beards', 'bearing', 'bearnaise', 'beast', 'beat', 'beaten', 'beater', 'beaters', 'beating', 'beauteous', 'beauties', 'beautiful', 'beautifully', 'beauty', 'bechamel', 'bechemel', 'become', 'becomes', 'becoming', 'bed', 'bee', 'beef', 'beefsteaks', 'beefy', 'beehive', 'beer', 'beers', 'bees', 'beet', 'beetroot', 'beetroots', 'beets', 'beforehand', 'beggars', 'begin', 'beginning', 'begins', 'begun', 'behave', 'behind', 'beige', 'beignet', 'beignets', 'bell', 'bellies', 'belly', 'ben', 'bench', 'bend', 'bending', 'bendy', 'beneath', 'benedict', 'beneficial', 'bent', 'bergamot', 'berries', 'berry', 'besan', 'beside', 'bessara', 'best', 'better', 'betwee', 'beurre', 'beware', 'bewreen', 'beyond', 'bhaji', 'bhajis', 'biber', 'bicarb', 'bicarbonate', 'bichamel', 'bicycle', 'bierocks', 'big', 'bigarade', 'bigger', 'biggest', 'bikers', 'billowy', 'bin', 'bind', 'binding', 'binds', 'birch', 'bird', 'birds', 'biro', 'birthday', 'biryani', 'biscotti', 'biscuit', 'biscuits', 'bisque', 'bit', 'bite', 'bites', 'bitesized', 'bits', 'bitten', 'bitter', 'bitterness', 'bitters', 'bitto', 'black', 'blackbeans', 'blackberries', 'blackberry', 'blackbird', 'blackcurrant', 'blackcurrants', 'blacked', 'blacken', 'blackened', 'blackening', 'blackstrap', 'blade', 'bladed', 'blades', 'blanc', 'blanch', 'blanched', 'blanching', 'blancmange', 'blancs', 'blanket', 'blankets', 'blanquette', 'blast', 'blasts', 'bleed', 'blemished', 'blemishes', 'blend', 'blended', 'blender', 'blending', 'blends', 'bleu', 'blind', 'blini', 'blinis', 'blip', 'blister', 'blistered', 'blisters', 'blitz', 'blitzing', 'blob', 'blobs', 'block', 'blocks', 'blonde', 'blondies', 'blood', 'bloodline', 'bloody', 'bloomer', 'blossom', 'blot', 'blow', 'blown', 'blowtorch', 'blue', 'blueberries', 'blueberry', 'blunt', 'blush', 'blushed', 'blusher', 'boar', 'board', 'boarder', 'boards', 'boat', 'boater', 'boats', 'bob', 'bobble', 'bodies', 'body', 'boeuf', 'boil', 'boiled', 'boiler', 'boiling', 'boils', 'bois', 'bok', 'bold', 'boletus', 'bolognese', 'bombay', 'bon', 'bonbons', 'bone', 'boned', 'boneless', 'bones', 'boning', 'bonito', 'bonnet', 'bons', 'bony', 'book', 'boost', 'booze', 'boozy', 'borage', 'bordelaise', 'border', 'borders', 'boreks', 'borlotti', 'boston', 'bot', 'bother', 'bottarga', 'bottle', 'bottled', 'bottles', 'bottom', 'bottomed', 'bottomless', 'bottoms', 'boudoir', 'boudran', 'bought', 'bouillabaisse', 'bouillion', 'bouillon', 'boulang', 'boule', 'bounce', 'bounces', 'bouncy', 'bound', 'boundaries', 'bouquet', 'bouquets', 'bourbon', 'bourbons', 'bourguignon', 'bourride', 'bow', 'bowl', 'bowlful', 'bowls', 'bown', 'bows', 'box', 'boxes', 'boxing', 'boxties', 'boxty', 'br', 'braciole', 'brad', 'braeburn', 'braid', 'braided', 'braiding', 'braids', 'brains', 'braise', 'braised', 'braising', 'bramble', 'brambles', 'bramley', 'bran', 'branch', 'branches', 'brandade', 'brandied', 'brands', 'brandy', 'brava', 'bravas', 'brazil', 'brazils', 'bread', 'breadcrumb', 'breadcrumbed', 'breadcrumbs', 'breaded', 'breadfruit', 'breads', 'breadstick', 'breadsticks', 'break', 'breakages', 'breakdown', 'breakfast', 'breaking', 'breaks', 'bream', 'breast', 'breastbone', 'breastbones', 'breasts', 'breath', 'bresaola', 'brew', 'brick', 'bricks', 'bridge', 'brie', 'brief', 'briefly', 'bright', 'brighter', 'brightly', 'brik', 'brill', 'brilliant', 'brim', 'brine', 'brined', 'bring', 'bringing', 'brings', 'brining', 'brioche', 'brioches', 'brisk', 'brisket', 'briskly', 'brith', 'british', 'brittle', 'broad', 'broadbeans', 'broccoli', 'brocolli', 'broken', 'bronze', 'bronzed', 'broom', 'broth', 'brought', 'brow', 'brown', 'browned', 'brownie', 'brownies', 'browning', 'brownish', 'browns', 'browny', 'bruise', 'bruised', 'bruises', 'bruising', 'brul', 'brulee', 'brulees', 'brulie', 'bruschetta', 'bruschettas', 'brush', 'brushed', 'brushing', 'brussels', 'brutal', 'brute', 'bubble', 'bubbled', 'bubbles', 'bubbling', 'bubbly', 'bucatini', 'buccaneer', 'buchi', 'bucket', 'buckles', 'buckwheat', 'bud', 'buds', 'buerre', 'buffalo', 'buffet', 'bugs', 'build', 'building', 'built', 'bulb', 'bulbous', 'bulbs', 'bulgar', 'bulge', 'bulghur', 'bulging', 'bulgur', 'buljol', 'bulked', 'bull', 'bumps', 'bumpy', 'bun', 'bunch', 'bunched', 'bunches', 'bundle', 'bundles', 'bundt', 'bunnies', 'buns', 'bunting', 'buratta', 'burger', 'burgers', 'burgundy', 'burn', 'burned', 'burner', 'burners', 'burning', 'burnished', 'burns', 'burnt', 'burrata', 'burrida', 'burrito', 'burritos', 'burst', 'bursting', 'bursts', 'bury', 'burying', 'business', 'butcher', 'butchers', 'buter', 'butter', 'butterbean', 'butterbeans', 'buttercream', 'buttercreams', 'buttercup', 'buttered', 'butterflied', 'butterfly', 'butterflying', 'butterhead', 'buttering', 'buttermilk', 'butternut', 'butters', 'butterscotch', 'buttery', 'butties', 'button', 'buttons', 'butty', 'buy', 'buzz', 'byzantine', 'cabbage', 'cabbages', 'cabernet', 'cacao', 'cachumba', 'cachumber', 'cacio', 'caerfully', 'caerphilly', 'caesar', 'caf', 'caff', 'cage', 'cajun', 'cake', 'cakes', 'cakey', 'calamari', 'calasparra', 'calavados', 'calculate', 'calculated', 'calculating', 'calendula', 'calf', 'call', 'callaloo', 'called', 'calmed', 'calvados', 'calvero', 'calves', 'calypso', 'calzone', 'calzones', 'camembert', 'camomile', 'camouflage', 'camp', 'campden', 'canap', 'candied', 'candle', 'candles', 'candy', 'cane', 'canelle', 'canister', 'canned', 'cannellini', 'cannelloni', 'canning', 'cannot', 'canoes', 'canola', 'cans', 'cantonese', 'cantuccini', 'cap', 'capacious', 'capacity', 'cape', 'caper', 'caperberries', 'caperberry', 'capers', 'caponata', 'cappellacci', 'cappuccino', 'cappucino', 'caps', 'capsicum', 'capturing', 'caradmom', 'caramalise', 'caramalised', 'caramel', 'caramelisation', 'caramelise', 'caramelised', 'caramelises', 'caramelising', 'caramelize', 'caramelized', 'caramels', 'carameslised', 'caraway', 'carbonara', 'carbonated', 'carcass', 'carcasses', 'carcinogenic', 'card', 'cardamom', 'cardamoms', 'cardamon', 'cardamons', 'cardboard', 'cards', 'care', 'careful', 'carefully', 'carefuly', 'carlo', 'carmelise', 'carmelised', 'carne', 'carom', 'carpaccio', 'carpet', 'carrier', 'carries', 'carrot', 'carrots', 'carry', 'carrying', 'cars', 'cartilage', 'cartilaginous', 'carton', 'cartons', 'cartouche', 'cartridges', 'carve', 'carved', 'carving', 'cascade', 'case', 'cases', 'cashel', 'cashew', 'cashews', 'casing', 'casings', 'cassava', 'casserole', 'cassia', 'cassis', 'cassoulet', 'cast', 'caster', 'castor', 'catch', 'catches', 'catching', 'cats', 'caught', 'caul', 'cauliflower', 'cauliflowers', 'cauls', 'cause', 'caused', 'causes', 'caution', 'cautious', 'cautiously', 'cava', 'cavalo', 'cavatelli', 'cavello', 'cavelo', 'caviar', 'cavities', 'cavity', 'cavolo', 'cawl', 'cayenne', 'caynenne', 'cazuela', 'ceam', 'celariac', 'celeriac', 'celery', 'cellophane', 'celtic', 'cement', 'cent', 'center', 'centimetre', 'centimetres', 'central', 'centrally', 'centre', 'centres', 'cep', 'ceps', 'ceramic', 'cereal', 'certain', 'certainly', 'ceviche', 'cf', 'ch', 'chaat', 'chaffing', 'chafing', 'chair', 'chairs', 'chalkiness', 'chalky', 'cham', 'chamel', 'champ', 'champagne', 'chana', 'chance', 'change', 'changed', 'changes', 'changing', 'channel', 'chantenay', 'chanterelle', 'chanterelles', 'chantilly', 'chantillys', 'chapati', 'chapatis', 'chapatti', 'chapattis', 'chappatis', 'char', 'character', 'charcoal', 'chard', 'chardonnay', 'chards', 'charge', 'chargers', 'chargrill', 'chargrilled', 'charlotte', 'charlottes', 'charm', 'charred', 'charring', 'chars', 'chartreuse', 'chasseur', 'chat', 'chateaubriand', 'chausson', 'chaussons', 'che', 'cheat', 'cheats', 'check', 'checked', 'checker', 'checking', 'cheddar', 'cheek', 'cheeks', 'cheeky', 'cheerful', 'cheese', 'cheeseburger', 'cheesecake', 'cheesecakes', 'cheesecloth', 'cheeseis', 'cheeses', 'cheesy', 'chef', 'cheffy', 'chefs', 'chelsea', 'chequerboard', 'chequered', 'chermoula', 'cherries', 'cherry', 'chervil', 'cheshire', 'chestnut', 'chestnuts', 'chew', 'chewed', 'chewing', 'chewy', 'chia', 'chick', 'chicken', 'chickens', 'chickpea', 'chickpeas', 'chicks', 'chickweed', 'chicories', 'chicory', 'chiffon', 'child', 'children', 'chili', 'chilies', 'chill', 'chilled', 'chilles', 'chilli', 'chillies', 'chilling', 'chills', 'chimichurri', 'chimicurri', 'chimney', 'chimneys', 'china', 'chine', 'chinese', 'chinkiang', 'chinois', 'chip', 'chipattis', 'chipolata', 'chipolatas', 'chipotle', 'chipped', 'chippings', 'chips', 'chiu', 'chive', 'chives', 'chlorophyll', 'cho', 'choc', 'chocolat', 'chocolate', 'chocolates', 'chocolatey', 'chocolatiness', 'choi', 'choice', 'choke', 'chokes', 'choking', 'choose', 'choosing', 'chop', 'chopped', 'chopper', 'chopping', 'chops', 'chopstick', 'chopsticks', 'choriceros', 'chorizo', 'chorizos', 'choron', 'chosen', 'choucroute', 'choux', 'chow', 'chowder', 'choy', 'christmas', 'christophine', 'chuck', 'chucked', 'chuff', 'chump', 'chunk', 'chunkily', 'chunks', 'chunky', 'churn', 'churned', 'churning', 'churro', 'churros', 'chutney', 'chutneys', 'ciabatta', 'ciabattas', 'cider', 'cidery', 'cigar', 'cigarillos', 'cigars', 'cinder', 'cinderella', 'cinnamon', 'circle', 'circles', 'circling', 'circular', 'circulate', 'circumference', 'cired', 'cirlce', 'citrate', 'citric', 'citron', 'citrus', 'clafoutis', 'claggy', 'clair', 'clairs', 'clam', 'clamp', 'clams', 'clap', 'clapshot', 'clarification', 'clarified', 'clarify', 'classic', 'clatter', 'clatteringly', 'claw', 'claws', 'clay', 'clean', 'cleaned', 'cleaner', 'cleaning', 'cleanly', 'clear', 'clearer', 'clearing', 'clearly', 'clears', 'cleaver', 'clef', 'cleft', 'clementine', 'clementines', 'clench', 'clenched', 'clenching', 'clever', 'click', 'cling', 'clingfilm', 'clinging', 'clings', 'clip', 'clips', 'clock', 'clockwise', 'clootie', 'close', 'closed', 'closely', 'closer', 'closest', 'closing', 'clot', 'cloth', 'clothes', 'clothing', 'clotted', 'cloud', 'clouds', 'cloudy', 'clove', 'clover', 'cloves', 'clown', 'club', 'clump', 'clumping', 'clumps', 'clumpy', 'cluster', 'clusters', 'cm', 'cms', 'cmx', 'cmxcm', 'cmxcmx', 'cmxmm', 'co', 'coagulate', 'coagulated', 'coal', 'coals', 'coarse', 'coarsely', 'coarser', 'coasted', 'coat', 'coated', 'coating', 'coatings', 'coats', 'coax', 'cob', 'cobbled', 'cobbler', 'cobblers', 'cobbles', 'cobbling', 'cobnut', 'cobnuts', 'cobs', 'cock', 'cockle', 'cockles', 'cocktail', 'cocktails', 'coco', 'cocoa', 'coconut', 'coconuts', 'cocotte', 'cocount', 'cod', 'coddled', 'coffee', 'cognac', 'coil', 'coiled', 'coiling', 'coin', 'coins', 'cointreau', 'coking', 'cola', 'colander', 'colatura', 'colcannon', 'cold', 'colder', 'coleslaw', 'coley', 'collagen', 'collapse', 'collapsed', 'collapsing', 'collar', 'collarette', 'collars', 'collect', 'collected', 'collecting', 'collects', 'color', 'colour', 'coloured', 'colourful', 'colouring', 'colourings', 'colourless', 'colours', 'columns', 'coluring', 'com', 'comb', 'combination', 'combine', 'combined', 'combines', 'combining', 'come', 'comes', 'comfortable', 'comfortably', 'coming', 'comma', 'command', 'commercial', 'comp', 'compact', 'compacted', 'competely', 'complement', 'complete', 'completed', 'completely', 'completeness', 'completes', 'completing', 'completly', 'complex', 'component', 'components', 'compote', 'compress', 'compressing', 'comprises', 'comt', 'con', 'concass', 'concasse', 'concave', 'conceal', 'concentrate', 'concentrated', 'concentration', 'concentric', 'concerned', 'concertina', 'cond', 'condensed', 'condenses', 'condiments', 'condition', 'cone', 'conents', 'coner', 'cones', 'confectionary', 'confident', 'confidently', 'confines', 'confit', 'conical', 'conmtinue', 'connect', 'connected', 'connecting', 'conserve', 'considerable', 'considerably', 'consideration', 'consistence', 'consistency', 'consistent', 'consistently', 'consisting', 'consomm', 'constancy', 'constant', 'constantly', 'construct', 'consume', 'contact', 'contain', 'contained', 'container', 'containers', 'containing', 'contains', 'contaminate', 'contantly', 'content', 'contents', 'continously', 'continual', 'continually', 'continue', 'continues', 'continuing', 'continune', 'continuous', 'continuously', 'contiuously', 'contours', 'contrast', 'contrasting', 'control', 'convection', 'convenient', 'conventional', 'converted', 'convincing', 'cooing', 'cook', 'cooked', 'cooker', 'cookie', 'cookies', 'cooking', 'cooks', 'cool', 'cooled', 'cooler', 'coolest', 'cooling', 'cools', 'copper', 'coq', 'cor', 'coral', 'corals', 'cord', 'cordial', 'cordon', 'core', 'cored', 'corer', 'cores', 'coriander', 'coring', 'cork', 'corks', 'corkscrew', 'corn', 'cornbread', 'corned', 'corner', 'cornered', 'corners', 'cornflake', 'cornflakes', 'cornflour', 'cornflower', 'cornichon', 'cornichons', 'cornish', 'cornmeal', 'corns', 'cornstarch', 'correct', 'correctly', 'corresponding', 'cos', 'cotija', 'cotta', 'cottage', 'cottas', 'cotton', 'cou', 'couche', 'cougette', 'could', 'coulibiac', 'coulis', 'counter', 'country', 'couple', 'coupler', 'courgesttes', 'courgette', 'courgettes', 'courgetti', 'couronne', 'course', 'court', 'cous', 'couscous', 'cover', 'coverage', 'covered', 'coveredl', 'covering', 'covers', 'cox', 'cque', 'cr', 'crab', 'crabcake', 'crabcakes', 'crabmeat', 'crabmeats', 'crabs', 'crack', 'cracked', 'cracker', 'crackers', 'cracking', 'crackle', 'crackled', 'crackling', 'cracklings', 'crackly', 'cracks', 'craft', 'crains', 'cramped', 'cranachan', 'cranachans', 'cranberries', 'cranberry', 'crank', 'crannies', 'cranny', 'crater', 'craters', 'crayfish', 'crazy', 'cream', 'creama', 'creamed', 'creamier', 'creamif', 'creamily', 'creaminess', 'creaming', 'creams', 'creamy', 'crease', 'creases', 'creasing', 'create', 'created', 'creates', 'creating', 'creatures', 'credit', 'crema', 'cremat', 'creme', 'cremona', 'cremosa', 'creole', 'crepe', 'crepes', 'crepinette', 'crescent', 'cress', 'cresses', 'crest', 'crimp', 'crimped', 'crimping', 'crimps', 'crinkle', 'crinkled', 'crinkles', 'criolla', 'crisp', 'crispbreads', 'crisped', 'crispen', 'crisper', 'crispier', 'crispies', 'crispiness', 'crisping', 'crispness', 'crisps', 'crispy', 'criss', 'crisscross', 'critical', 'cro', 'crocodile', 'croissant', 'croissants', 'croque', 'croquembouche', 'croquet', 'croqueta', 'croquetas', 'croquette', 'croquettes', 'cross', 'crossed', 'crosses', 'crosshatch', 'crossing', 'crossways', 'crosswise', 'crostini', 'croustade', 'croustades', 'croustilliant', 'croute', 'crouton', 'croutons', 'crowd', 'crowded', 'crowdie', 'crowding', 'crown', 'crowns', 'cru', 'crucial', 'cruda', 'crudely', 'crudit', 'crumb', 'crumbed', 'crumble', 'crumbled', 'crumbles', 'crumbling', 'crumbly', 'crumbs', 'crumpet', 'crumpets', 'crumple', 'crumpled', 'crunch', 'crunched', 'crunchiness', 'crunchy', 'crush', 'crushed', 'crushing', 'crushto', 'crust', 'crusted', 'crusting', 'crusts', 'crusty', 'crystal', 'crystalise', 'crystalised', 'crystalized', 'crystallise', 'crystallised', 'crystallises', 'crystallising', 'crystals', 'cube', 'cubed', 'cubes', 'cubesafter', 'cucmbers', 'cucumber', 'cucumbers', 'cuit', 'cumberland', 'cumbrian', 'cumin', 'cup', 'cupboard', 'cupboards', 'cupcake', 'cupcakes', 'cupful', 'cupfuls', 'cupid', 'cuplike', 'cups', 'curd', 'curdle', 'curdled', 'curdles', 'curdling', 'curds', 'cure', 'cured', 'cures', 'curing', 'curl', 'curled', 'curlicues', 'curling', 'curls', 'curly', 'currant', 'currants', 'curried', 'curries', 'currimbhoy', 'currry', 'curry', 'curst', 'curve', 'curved', 'curves', 'curving', 'custard', 'custards', 'cut', 'cutlery', 'cutlet', 'cutlets', 'cuts', 'cutter', 'cutters', 'cutting', 'cuttle', 'cuttlefish', 'cycle', 'cyclist', 'cylinder', 'cylinders', 'cylindrical', 'cypriot', 'daal', 'dab', 'dabbing', 'dabs', 'dacquoise', 'dahl', 'daikon', 'daily', 'dairy', 'daisies', 'daisy', 'dal', 'dally', 'damage', 'damaged', 'damp', 'damped', 'dampen', 'dampened', 'dampfnudel', 'dampfnudels', 'damson', 'damsons', 'dance', 'dandelion', 'dangerous', 'dangle', 'dangling', 'danish', 'danishes', 'dappled', 'dare', 'dariole', 'dark', 'darken', 'darkened', 'darkens', 'darker', 'darkly', 'darne', 'darnes', 'daroile', 'dash', 'dashes', 'dashi', 'date', 'dates', 'daube', 'dauphinoise', 'daurade', 'day', 'days', 'de', 'dead', 'deal', 'debeard', 'debris', 'decant', 'decanted', 'decanting', 'decent', 'decide', 'deck', 'decorate', 'decorated', 'decorating', 'decoration', 'decorations', 'decorative', 'decoratively', 'decrease', 'decreasing', 'deduct', 'deep', 'deepens', 'deeper', 'deepest', 'deeply', 'deer', 'defined', 'deflate', 'deflated', 'deflates', 'defrost', 'defrosted', 'defrosts', 'deglaze', 'deglazed', 'deglazing', 'degree', 'degrees', 'dehydrate', 'dehydrated', 'dehydrator', 'delay', 'delectably', 'deliberately', 'delicate', 'delicately', 'delice', 'delicious', 'deliciously', 'deliciousness', 'delight', 'delis', 'demarara', 'demerara', 'demerera', 'demi', 'den', 'dense', 'dent', 'dente', 'depend', 'dependent', 'depending', 'depends', 'depression', 'depressions', 'depth', 'derry', 'des', 'descending', 'describe', 'described', 'description', 'deseed', 'deseeded', 'deseeding', 'desiccated', 'design', 'designs', 'desirably', 'desire', 'desired', 'desires', 'despair', 'dessert', 'desserts', 'dessertspoon', 'dessertspoonful', 'dessertspoonfuls', 'dessertspoons', 'dessicated', 'detach', 'detached', 'details', 'deteriorate', 'detract', 'devein', 'develop', 'developed', 'develops', 'devilled', 'devils', 'devoured', 'devouring', 'dhal', 'di', 'diagonal', 'diagonally', 'diagonals', 'diakon', 'diamater', 'diameter', 'diameters', 'diamond', 'diamonds', 'diane', 'dice', 'diced', 'dicing', 'dick', 'die', 'died', 'dies', 'difference', 'different', 'differently', 'difficult', 'difficulty', 'diffuser', 'dig', 'digestive', 'digestives', 'digital', 'dijon', 'dill', 'dillisk', 'dilute', 'diluted', 'dim', 'dimensions', 'diminish', 'diminished', 'dimple', 'dimpled', 'dimples', 'diner', 'diners', 'dinner', 'dip', 'dippable', 'dipped', 'dippers', 'dipping', 'dips', 'direct', 'directed', 'direction', 'directions', 'directly', 'dirt', 'dirty', 'disappear', 'disappeared', 'disappears', 'disappointingly', 'disc', 'discard', 'discarded', 'discarding', 'disco', 'discoloration', 'discolour', 'discolouration', 'discolouring', 'discover', 'discs', 'disguise', 'dish', 'dishes', 'dishing', 'dishwasher', 'disintegrate', 'disintegrated', 'disk', 'disks', 'dislodge', 'disolved', 'disperse', 'dispersed', 'displacing', 'display', 'displayed', 'disposable', 'dissloved', 'dissolve', 'dissolved', 'dissolves', 'dissolving', 'distance', 'distances', 'distinct', 'distinctive', 'distribute', 'distributed', 'distributing', 'distribution', 'disturb', 'disturbing', 'disused', 'dive', 'divide', 'divided', 'divider', 'dividing', 'division', 'diy', 'dock', 'docker', 'dog', 'dolcelatte', 'dollop', 'dollops', 'dolly', 'dome', 'domed', 'domes', 'domestic', 'dominating', 'done', 'doneness', 'dont', 'donut', 'donuts', 'door', 'doors', 'dopiaza', 'dory', 'dose', 'dot', 'dots', 'dotted', 'dotting', 'double', 'doubled', 'doubles', 'doubt', 'dough', 'doughball', 'doughballs', 'doughnut', 'doughnuts', 'doughs', 'doughy', 'douse', 'dove', 'dover', 'dowel', 'doweling', 'dowelling', 'dowels', 'downward', 'downwards', 'dozen', 'drab', 'drafts', 'drag', 'dragged', 'dragging', 'dragon', 'dragoncella', 'drags', 'drain', 'drained', 'draining', 'dramatic', 'dramatically', 'drambuie', 'drape', 'drapes', 'draping', 'draught', 'draughts', 'draw', 'drawer', 'drawing', 'drawn', 'draws', 'dream', 'dredge', 'dredging', 'dregs', 'dress', 'dressed', 'dressing', 'dressingwhisk', 'dribble', 'dribbles', 'dribbling', 'dried', 'drier', 'dries', 'drink', 'drinking', 'drinks', 'drip', 'dripped', 'dripping', 'drippings', 'drips', 'drive', 'driven', 'drizzle', 'drizzled', 'drizzles', 'drizzling', 'drop', 'droplets', 'dropped', 'dropping', 'drops', 'drown', 'drum', 'drumstick', 'drumsticks', 'drunk', 'drunken', 'dry', 'dryer', 'drying', 'dublin', 'duchess', 'duck', 'duckling', 'ducks', 'due', 'dug', 'duja', 'dukka', 'dukkah', 'dulce', 'dull', 'dulse', 'duly', 'dump', 'dumpling', 'dumplings', 'dunk', 'dunked', 'dunking', 'duo', 'duration', 'dusky', 'dust', 'dusted', 'duster', 'dusting', 'dusty', 'dutch', 'duty', 'duxelle', 'duxelles', 'dwarf', 'dye', 'eactute', 'ear', 'earl', 'earlier', 'early', 'ears', 'earth', 'earthenware', 'ease', 'easier', 'easiest', 'easilly', 'easily', 'easing', 'easter', 'easy', 'eat', 'eaten', 'eating', 'eau', 'ebene', 'eccles', 'eclairs', 'ed', 'edam', 'edamame', 'edge', 'edged', 'edges', 'edible', 'edward', 'eeda', 'eel', 'eels', 'effect', 'efficient', 'effort', 'eg', 'egg', 'eggcup', 'eggs', 'eggshell', 'eggwash', 'eggy', 'eight', 'eighteen', 'eighth', 'eighths', 'either', 'el', 'elaborate', 'elapsed', 'elastic', 'elasticated', 'elbow', 'elderberries', 'elderflower', 'elderflowers', 'electric', 'electrical', 'elegant', 'elegantly', 'element', 'elements', 'eleven', 'eliminate', 'ell', 'elongate', 'elongated', 'else', 'elusive', 'emanates', 'embers', 'emboss', 'embossed', 'emerge', 'emerged', 'emergency', 'emmental', 'emmenthal', 'empanada', 'empanadas', 'emphasise', 'emptied', 'empty', 'emulsifed', 'emulsified', 'emulsifies', 'emulsify', 'emulsifying', 'emulsion', 'en', 'enable', 'enables', 'enamel', 'enamelled', 'encase', 'encased', 'encases', 'encasing', 'enchiladas', 'encircling', 'enclose', 'enclosed', 'enclosing', 'encourage', 'encouraging', 'encrusted', 'end', 'ended', 'ending', 'endive', 'endives', 'ends', 'energetically', 'english', 'enhances', 'enjoy', 'enjoyable', 'enlarge', 'enoki', 'enormous', 'enough', 'enrich', 'ensure', 'ensures', 'ensuring', 'enter', 'enters', 'entire', 'entirely', 'entrails', 'entrie', 'envelope', 'enzymes', 'equal', 'equally', 'equator', 'equipment', 'equivalent', 'err', 'eryngii', 'es', 'escabeche', 'escalope', 'escalopes', 'escape', 'escapes', 'escaping', 'escarole', 'escovitch', 'especially', 'espelette', 'espresso', 'espuma', 'essence', 'essential', 'essex', 'et', 'etc', 'evaporate', 'evaporated', 'evaporates', 'evaporating', 'eveb', 'even', 'evening', 'evenly', 'evens', 'eventually', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ewes', 'exact', 'exactly', 'examine', 'example', 'exceed', 'excellent', 'excep', 'except', 'exception', 'excess', 'excessive', 'exchange', 'exciting', 'excluding', 'expand', 'expanded', 'expands', 'expansion', 'expect', 'expel', 'expelled', 'experience', 'experienced', 'explode', 'exploding', 'expose', 'exposed', 'exposing', 'expression', 'extend', 'extends', 'exterior', 'external', 'extinguish', 'extra', 'extract', 'extracted', 'extracting', 'extractor', 'extracts', 'extras', 'extravagant', 'extreme', 'extremely', 'exuded', 'eye', 'eyebrows', 'eyed', 'eyes', 'fabric', 'fabulous', 'fabulously', 'face', 'faces', 'facial', 'facilitate', 'facing', 'factor', 'fade', 'fading', 'faggots', 'fagioli', 'fail', 'failed', 'faintest', 'faintly', 'fair', 'fairly', 'fairy', 'fajita', 'fajitas', 'falafel', 'falafels', 'fall', 'fallen', 'falling', 'falls', 'family', 'fan', 'fancies', 'fancy', 'fanned', 'fanning', 'fans', 'far', 'faraizi', 'farce', 'farl', 'farls', 'farthest', 'fashion', 'fashioned', 'fashions', 'fast', 'fasten', 'fastening', 'faster', 'fastest', 'fat', 'fatless', 'fats', 'fatter', 'fattest', 'fattiness', 'fatty', 'fava', 'favourite', 'fazool', 'fear', 'feather', 'feathered', 'feathering', 'feathers', 'feathery', 'features', 'fecule', 'fed', 'fee', 'feed', 'feeder', 'feeding', 'feel', 'feeling', 'feels', 'feet', 'feijoada', 'fence', 'fennel', 'fenugreek', 'ferment', 'fermentation', 'fermented', 'fermenting', 'fern', 'ferociously', 'festive', 'feta', 'fettuccine', 'fettuccini', 'feuille', 'feuilletine', 'fewer', 'fgas', 'fibers', 'fibres', 'fibrous', 'fiddle', 'fiddly', 'fideu', 'field', 'fierce', 'fiercely', 'fieriness', 'fiery', 'fifteen', 'fifth', 'fifths', 'fifty', 'fig', 'fights', 'figs', 'figurines', 'filing', 'fill', 'filled', 'fillet', 'filleted', 'filleting', 'fillets', 'filling', 'fillings', 'fills', 'filly', 'film', 'filmed', 'filo', 'filter', 'fin', 'final', 'finally', 'financier', 'financiers', 'find', 'finding', 'finds', 'fine', 'finely', 'finest', 'finger', 'fingered', 'fingernail', 'fingernails', 'fingerprints', 'fingers', 'fingertip', 'fingertips', 'finial', 'finish', 'finished', 'finishes', 'finishing', 'fins', 'fior', 'fiori', 'fir', 'fire', 'firecracker', 'firm', 'firmed', 'firmer', 'firmest', 'firmly', 'firmness', 'firms', 'first', 'firstly', 'fish', 'fishcake', 'fishcakes', 'fishes', 'fishmonger', 'fishy', 'fist', 'fistful', 'fit', 'fita', 'fits', 'fitted', 'fitting', 'five', 'fivers', 'fix', 'fixed', 'fixing', 'fixture', 'fixtures', 'fizz', 'fizzes', 'fizzy', 'fl', 'flag', 'flageolet', 'flags', 'flake', 'flaked', 'flakes', 'flakiness', 'flaking', 'flaky', 'flamb', 'flame', 'flamed', 'flameproof', 'flames', 'flaming', 'flammable', 'flan', 'flans', 'flaounes', 'flap', 'flapjack', 'flapjacks', 'flaps', 'flare', 'flares', 'flash', 'flat', 'flatbeads', 'flatbread', 'flatbreads', 'flatfish', 'flatleaf', 'flats', 'flatten', 'flattened', 'flattening', 'flattens', 'flatter', 'flattish', 'flavoring', 'flavour', 'flavoured', 'flavourful', 'flavouring', 'flavourings', 'flavourless', 'flavours', 'flavoursome', 'flax', 'flecked', 'flecks', 'fleeces', 'flesh', 'fleshed', 'fleshy', 'fleur', 'fleurs', 'flex', 'flexible', 'flick', 'flicked', 'flicking', 'flip', 'flipped', 'flipping', 'fll', 'float', 'floated', 'floating', 'floats', 'flood', 'flooding', 'floor', 'floppy', 'florentine', 'florentines', 'florets', 'florist', 'floss', 'flour', 'floured', 'flouring', 'flourish', 'flours', 'floury', 'flow', 'flower', 'flowering', 'flowerpots', 'flowers', 'flowing', 'floz', 'fluff', 'fluffier', 'fluffing', 'fluffly', 'fluffy', 'fluid', 'fluidity', 'fluids', 'flush', 'flute', 'fluted', 'flutes', 'flying', 'fminutes', 'fo', 'foam', 'foamed', 'foaming', 'foams', 'foamy', 'focaccia', 'foie', 'foil', 'foiled', 'fold', 'folded', 'folding', 'folds', 'foliage', 'folk', 'follow', 'followed', 'following', 'follows', 'fondant', 'fondants', 'fondue', 'fonio', 'fontina', 'food', 'foodprocessor', 'foods', 'fool', 'foot', 'football', 'foraged', 'force', 'forceful', 'forcefully', 'forcemeat', 'forces', 'forcing', 'fore', 'forearms', 'forefinger', 'forest', 'foresti', 'forget', 'forgiving', 'forhours', 'fork', 'forkful', 'forking', 'forks', 'form', 'forma', 'formation', 'formed', 'forming', 'forminutes', 'forms', 'forno', 'forth', 'fortune', 'fortunes', 'forty', 'forward', 'forwards', 'fougasse', 'found', 'foundation', 'four', 'fours', 'fourteen', 'fourth', 'fowl', 'fowls', 'fr', 'fra', 'fragile', 'fragments', 'fragrance', 'fragrances', 'fragrant', 'fraiche', 'frais', 'fraise', 'fraisier', 'framboise', 'frame', 'frames', 'frangipane', 'frankfurter', 'free', 'freehand', 'freekeh', 'freely', 'freestanding', 'freezable', 'freeze', 'freezeable', 'freezer', 'freezerproof', 'freezes', 'freezing', 'fregola', 'french', 'frequently', 'fresh', 'freshen', 'fresher', 'freshly', 'freshness', 'friache', 'fricass', 'fricassee', 'fridge', 'fried', 'friendly', 'friends', 'fries', 'frijoles', 'frikadeller', 'frill', 'frilly', 'fringe', 'fris', 'fritas', 'frites', 'frittata', 'frittatas', 'fritter', 'fritters', 'fritti', 'fritto', 'frm', 'fromage', 'fromf', 'frond', 'fronds', 'front', 'frost', 'frosted', 'frosting', 'froth', 'frothing', 'frothy', 'frozen', 'fruit', 'fruitiness', 'fruits', 'fruity', 'frutti', 'fry', 'fryer', 'frying', 'frym', 'ft', 'fu', 'fudge', 'fudgy', 'full', 'fuller', 'fully', 'fume', 'fumes', 'fun', 'function', 'funnel', 'furiously', 'furry', 'furthest', 'furthur', 'fuses', 'fusilli', 'futher', 'future', 'gable', 'gables', 'gai', 'gain', 'galangal', 'galette', 'galettes', 'galinha', 'galliano', 'gallo', 'game', 'gamekeeper', 'gammon', 'ganache', 'ganoush', 'gap', 'gaping', 'gapping', 'gaps', 'garage', 'garam', 'garden', 'gariguettes', 'garlands', 'garlic', 'garlicky', 'garnet', 'garni', 'garnish', 'garnished', 'garnishes', 'garnishing', 'garnising', 'garnsih', 'garri', 'gas', 'gask', 'gastrique', 'gate', 'gateau', 'gather', 'gathered', 'gathering', 'gathers', 'gauge', 'gazpacho', 'gear', 'geishas', 'gel', 'gelantine', 'gelatin', 'gelatine', 'gelato', 'geletine', 'gem', 'gemolata', 'gems', 'general', 'generally', 'generate', 'generates', 'generous', 'generously', 'genoise', 'gentle', 'gentleman', 'gentlemen', 'gentlest', 'gently', 'george', 'georgian', 'germ', 'german', 'get', 'gets', 'getting', 'gg', 'ghee', 'gherkin', 'gherkins', 'ghoogras', 'ghosts', 'giant', 'giblets', 'gift', 'gifts', 'gilded', 'gill', 'gills', 'gilt', 'gin', 'ginger', 'gingerbread', 'gingernut', 'gingernuts', 'gingers', 'gingery', 'girolle', 'girolles', 'give', 'given', 'gives', 'giving', 'gizzard', 'gizzards', 'gl', 'glac', 'glace', 'glad', 'glands', 'glass', 'glasses', 'glassy', 'glaze', 'glazed', 'glazes', 'glazing', 'gleam', 'gleaming', 'glide', 'glistening', 'glitter', 'glittery', 'globe', 'gloopy', 'glorious', 'gloriously', 'glory', 'gloss', 'glossy', 'glove', 'gloves', 'glowing', 'glowingly', 'gluco', 'glucose', 'glue', 'glued', 'glug', 'glugs', 'gluten', 'glutens', 'glutinous', 'glycerine', 'gnerates', 'gnocchi', 'gnudi', 'go', 'goat', 'goats', 'gobo', 'gochujang', 'goes', 'going', 'goji', 'gold', 'golde', 'golden', 'golf', 'golfball', 'gondola', 'gone', 'gong', 'good', 'goodness', 'gooey', 'gooeyness', 'goose', 'gooseberries', 'gooseberry', 'gore', 'goreng', 'gorgeous', 'gorgeously', 'gorgonzola', 'got', 'goug', 'goujons', 'goulash', 'gr', 'grab', 'grade', 'gradually', 'graffiti', 'grain', 'grained', 'grains', 'grainy', 'gram', 'grams', 'granadilla', 'grand', 'granita', 'granite', 'granny', 'granola', 'granular', 'granulated', 'granules', 'grape', 'grapefruit', 'grapefruits', 'grapefuit', 'grapes', 'grapeseed', 'grappa', 'gras', 'grasp', 'grasping', 'grass', 'grate', 'grated', 'gratefully', 'grater', 'graters', 'gratin', 'gratinated', 'grating', 'gratings', 'gravad', 'gravadlax', 'gravel', 'gravlax', 'gravy', 'grddle', 'grease', 'greased', 'greaseproof', 'greaseproofed', 'greases', 'greasing', 'greasproof', 'greasy', 'great', 'greatly', 'greek', 'green', 'greener', 'greengage', 'greengages', 'greenish', 'greens', 'gremolata', 'grenadine', 'gressingham', 'grey', 'gribiche', 'grid', 'griddle', 'griddled', 'griddling', 'gril', 'grill', 'grilled', 'griller', 'grilling', 'grills', 'grin', 'grind', 'grinder', 'grinding', 'grindings', 'grinds', 'griottine', 'grip', 'grissini', 'gristle', 'gristly', 'grit', 'grits', 'grittiness', 'gritty', 'grlll', 'groove', 'grooves', 'ground', 'grounded', 'groundnut', 'group', 'grouse', 'groves', 'grow', 'growing', 'grown', 'gruy', 'gruyere', 'gruyhre', 'guacamole', 'guajillo', 'guanciale', 'guard', 'guava', 'guavas', 'guernsey', 'guest', 'guests', 'guidance', 'guide', 'guidelines', 'guides', 'guinea', 'guinnes', 'guinness', 'gulab', 'gum', 'gumbo', 'gun', 'gungo', 'gurnard', 'gusto', 'gut', 'guts', 'gutted', 'gyoza', 'habanero', 'haddock', 'haf', 'haggerty', 'haggis', 'hair', 'hairy', 'hajikami', 'hake', 'half', 'halfway', 'halibut', 'halloumi', 'halloween', 'halt', 'halve', 'halved', 'halves', 'halving', 'halwa', 'ham', 'hamburger', 'hammer', 'hamper', 'hams', 'hand', 'handed', 'handful', 'handfuls', 'handheld', 'handing', 'handle', 'handled', 'handles', 'handling', 'handmixer', 'hands', 'handwhisk', 'handy', 'hang', 'hanger', 'hanging', 'hangs', 'hanout', 'hape', 'haphazardly', 'happen', 'happened', 'happening', 'happens', 'happily', 'happy', 'hard', 'hardboil', 'harden', 'hardened', 'hardening', 'hardens', 'harder', 'hardest', 'hardly', 'haricot', 'harina', 'harissa', 'harrisa', 'harrissa', 'harsh', 'harshness', 'harvest', 'harvester', 'hash', 'hasselback', 'hat', 'hatch', 'hatching', 'hats', 'havent', 'hay', 'hazard', 'haze', 'hazelnut', 'hazelnuts', 'hazes', 'hazy', 'head', 'heading', 'heads', 'heal', 'healthy', 'heap', 'heaped', 'heaping', 'heaps', 'hear', 'heart', 'hearts', 'hearty', 'heat', 'heated', 'heather', 'heating', 'heatproof', 'heats', 'heatuntil', 'heavier', 'heavily', 'heavy', 'hedgehog', 'hedgerow', 'heel', 'hefty', 'height', 'heights', 'held', 'hell', 'help', 'helped', 'helpful', 'helping', 'helpings', 'helps', 'hemp', 'hen', 'henna', 'hens', 'herb', 'herbal', 'herbed', 'herbes', 'herbs', 'herby', 'heritage', 'herring', 'herrings', 'het', 'hexagon', 'hey', 'hidden', 'hide', 'higgledy', 'high', 'highball', 'higher', 'highest', 'highish', 'highly', 'hih', 'hijiki', 'hillocks', 'hind', 'hinder', 'hinges', 'hint', 'hispi', 'hiss', 'hit', 'hits', 'hitting', 'hive', 'hob', 'hoba', 'hock', 'hocks', 'hogget', 'hoi', 'hoisin', 'hoki', 'hold', 'holding', 'holds', 'hole', 'holes', 'holey', 'hollandaise', 'hollow', 'hollowed', 'hollowing', 'hollows', 'holly', 'hollywood', 'hologram', 'holy', 'home', 'homemade', 'hominy', 'homme', 'homogenise', 'homogenous', 'honey', 'honeycomb', 'honeycombe', 'honeyed', 'honor', 'hood', 'hook', 'hooks', 'hoop', 'hoops', 'hopefully', 'hopeless', 'horizonatally', 'horizontal', 'horizontally', 'horn', 'horns', 'horse', 'horseback', 'horseradish', 'horseshoe', 'hot', 'hotdog', 'hotdogs', 'hotplate', 'hotpot', 'hotter', 'hottest', 'hour', 'hours', 'house', 'however', 'hr', 'hrs', 'hued', 'huge', 'hull', 'hulled', 'humane', 'humanly', 'humid', 'humidity', 'hummus', 'humour', 'hundreds', 'hung', 'hunk', 'hunza', 'hurry', 'hurt', 'husk', 'husks', 'hydrate', 'hydrated', 'ib', 'iberico', 'ice', 'iceberg', 'iced', 'iche', 'icicles', 'icing', 'icings', 'icky', 'icy', 'idea', 'ideal', 'ideally', 'ideas', 'identical', 'identified', 'ie', 'ieeks', 'ignite', 'ignore', 'ills', 'ilm', 'image', 'imagination', 'imagine', 'imgredients', 'immediatedly', 'immediately', 'immerse', 'immersing', 'immersion', 'imminent', 'impact', 'impale', 'impart', 'imperative', 'imperfections', 'implement', 'important', 'importantly', 'impossible', 'impression', 'impressive', 'imprint', 'imprinted', 'improve', 'improves', 'impurities', 'ina', 'inactive', 'inand', 'inc', 'inch', 'inches', 'incision', 'incisions', 'include', 'includes', 'including', 'inconsistency', 'incorporate', 'incorporated', 'incorporating', 'increase', 'increased', 'increasing', 'increasingly', 'increments', 'indeed', 'indent', 'indentation', 'indentations', 'indents', 'index', 'indian', 'indicate', 'indicates', 'indicating', 'indigestion', 'individual', 'individually', 'indoor', 'indulgently', 'industrial', 'inedible', 'inexplicable', 'inflate', 'inform', 'informal', 'information', 'infrequently', 'infuse', 'infused', 'infuses', 'infusing', 'infusion', 'ing', 'inglese', 'ingredient', 'ingredients', 'inhaled', 'initial', 'initially', 'inject', 'injector', 'ink', 'innards', 'inner', 'insalata', 'insert', 'inserted', 'inserting', 'inside', 'insides', 'insipid', 'insist', 'inspect', 'instant', 'instantly', 'instead', 'instruct', 'instructed', 'instruction', 'instructions', 'instuctions', 'insure', 'insuring', 'intact', 'intend', 'intended', 'intense', 'intensely', 'intensified', 'intensifies', 'intensify', 'intensity', 'interesting', 'interfere', 'interior', 'interleave', 'intermittently', 'internal', 'internet', 'intersect', 'interspersed', 'interspersing', 'intervals', 'interweaving', 'intestinal', 'intestine', 'intestines', 'intolarge', 'intp', 'intricate', 'intricately', 'introduce', 'introduces', 'introduction', 'intto', 'invert', 'inverted', 'inverting', 'invisible', 'inward', 'inwards', 'inwide', 'inxin', 'inzimino', 'ion', 'iplace', 'iquidise', 'irish', 'iron', 'irregular', 'irregularly', 'irritant', 'iscooked', 'ish', 'island', 'islands', 'israeli', 'isremoved', 'italian', 'italy', 'item', 'itemized', 'items', 'ithe', 'ivory', 'jack', 'jacket', 'jackets', 'jacob', 'jade', 'jaffa', 'jagged', 'jaggery', 'jalape', 'jalapeno', 'jalebi', 'jalfrezi', 'jalousie', 'jam', 'jamaican', 'jamless', 'jammy', 'jamon', 'jamun', 'japanese', 'jar', 'jarred', 'jars', 'jasmine', 'jaune', 'jay', 'jellied', 'jellies', 'jellified', 'jelly', 'jerez', 'jerk', 'jersey', 'jerusalem', 'jewels', 'jhal', 'jicama', 'jigsaw', 'jim', 'job', 'joconde', 'joe', 'joffol', 'john', 'join', 'joined', 'joining', 'joins', 'joint', 'joints', 'jollof', 'jowl', 'joy', 'joyously', 'juces', 'jucies', 'judge', 'judus', 'jug', 'jugs', 'juice', 'juiced', 'juicer', 'juices', 'juicier', 'juicily', 'juiciness', 'juicy', 'julienne', 'julienned', 'jumbled', 'jumbo', 'jump', 'jumping', 'juniper', 'jus', 'kachori', 'kachumber', 'kadaifi', 'kadhai', 'kadhi', 'kaffir', 'kai', 'kalamata', 'kale', 'karahi', 'karara', 'karhai', 'kasha', 'kashk', 'kashmiri', 'katakuriko', 'katsu', 'kavaab', 'kebab', 'kebabs', 'kecap', 'kedgeree', 'keep', 'keeping', 'keeps', 'kefir', 'kefta', 'kegs', 'kelewele', 'kelp', 'kept', 'kerala', 'keralan', 'kernel', 'kernels', 'ketchup', 'ketchups', 'ketjap', 'kettle', 'kewra', 'key', 'keys', 'kg', 'khao', 'khichri', 'kibbled', 'kick', 'kid', 'kidney', 'kidneys', 'kids', 'kiev', 'kievs', 'kill', 'kilner', 'kilo', 'kilogram', 'kimchi', 'kind', 'kinds', 'king', 'kipper', 'kippers', 'kirsch', 'kirsh', 'kisses', 'kitchen', 'kitchenware', 'kite', 'kites', 'kiwi', 'klippfisk', 'knead', 'kneadable', 'kneaded', 'kneading', 'kneads', 'kneed', 'knickerbocker', 'knickerbockerglory', 'knife', 'knifel', 'knifepoint', 'knives', 'knob', 'knobs', 'knock', 'knocked', 'knocking', 'knot', 'knots', 'knotted', 'knotting', 'know', 'known', 'knuckle', 'knuckles', 'kofta', 'koftas', 'kofte', 'kohlrabi', 'koji', 'kokum', 'kombu', 'konbu', 'korean', 'korma', 'krachai', 'kracthai', 'kransekake', 'kransekakes', 'krispies', 'kulfi', 'kumquat', 'kumquats', 'kuzu', 'kvass', 'la', 'label', 'labelled', 'labneh', 'lace', 'laced', 'laces', 'lacey', 'lacking', 'lacquered', 'lacy', 'ladder', 'ladel', 'ladle', 'ladled', 'ladleful', 'ladlefuls', 'ladles', 'ladling', 'lady', 'ladybird', 'ladyfinger', 'lager', 'laid', 'laksa', 'lamb', 'lambs', 'lamination', 'lan', 'lancashire', 'lanes', 'langoustine', 'langoustines', 'langue', 'langues', 'lankan', 'laotian', 'lapping', 'lapsang', 'larb', 'lard', 'larder', 'larding', 'lardo', 'lardons', 'lardy', 'large', 'larger', 'largest', 'largish', 'lasagne', 'lashings', 'last', 'lastly', 'latch', 'late', 'later', 'latke', 'latkes', 'latte', 'lattice', 'lattices', 'lava', 'lave', 'lavender', 'laver', 'laverbread', 'lavosh', 'lax', 'lay', 'layer', 'layered', 'layering', 'layers', 'laying', 'layout', 'lazy', 'lb', 'lboz', 'lbs', 'lea', 'leaf', 'leafage', 'leafs', 'leafy', 'leak', 'leakage', 'leakages', 'leaking', 'leakproof', 'leaks', 'lean', 'leaning', 'least', 'leat', 'leather', 'leathery', 'leave', 'leavers', 'leaves', 'leaving', 'leavingenough', 'leche', 'lecithin', 'leek', 'leeks', 'left', 'leftover', 'leftovers', 'leg', 'legged', 'legless', 'legs', 'leicester', 'leisure', 'lemon', 'lemonade', 'lemongrass', 'lemons', 'lemony', 'lenghtways', 'length', 'lengthen', 'lengthened', 'lengths', 'lengthways', 'lengthwise', 'lenthways', 'lentil', 'lentils', 'less', 'lessen', 'let', 'lets', 'letsimmer', 'letter', 'letters', 'letting', 'lettuce', 'lettuces', 'level', 'levelled', 'levelling', 'levels', 'lever', 'leverage', 'liable', 'liberal', 'liberally', 'lices', 'licorice', 'lid', 'lidded', 'lids', 'lie', 'life', 'lifeless', 'lift', 'lifted', 'lifting', 'lighlty', 'light', 'lighted', 'lighten', 'lightened', 'lighter', 'lightest', 'lighting', 'lightl', 'lightly', 'lightness', 'lights', 'like', 'liked', 'likely', 'likewise', 'liking', 'lilac', 'limbs', 'lime', 'limejuice', 'limes', 'limoncello', 'limp', 'lin', 'lincolnshire', 'linconshire', 'line', 'lined', 'linen', 'liner', 'liners', 'lines', 'ling', 'lingonberries', 'lingonberry', 'linguine', 'linguini', 'lining', 'linseed', 'linseeds', 'lion', 'lip', 'lipped', 'lipstick', 'liqour', 'liquefied', 'liquer', 'liqueur', 'liquid', 'liquidise', 'liquidiser', 'liquidizer', 'liquids', 'liquor', 'liquorice', 'liquour', 'list', 'listed', 'listening', 'lit', 'literally', 'lithuanian', 'litre', 'litres', 'little', 'live', 'lively', 'liver', 'livers', 'llasagne', 'llow', 'lml', 'load', 'loads', 'loaf', 'loaves', 'lobes', 'lobscouse', 'lobster', 'lobsters', 'locate', 'located', 'lock', 'log', 'logo', 'logs', 'loin', 'loins', 'lollies', 'lollipop', 'lollipops', 'lolly', 'london', 'long', 'longer', 'longest', 'longish', 'look', 'looking', 'looks', 'loop', 'looping', 'loops', 'loose', 'loosed', 'loosely', 'loosen', 'loosened', 'loosening', 'loosens', 'looser', 'looses', 'lose', 'loses', 'losing', 'loss', 'lost', 'lot', 'lots', 'lotus', 'lovage', 'love', 'lovely', 'low', 'lower', 'lowered', 'lowering', 'lowers', 'lowest', 'lowish', 'lternatively', 'lthen', 'lubricate', 'lubrication', 'lucky', 'lug', 'lukewarm', 'lump', 'lumpfish', 'lumpiness', 'lumps', 'lumpy', 'lunch', 'lunchbox', 'lunchboxes', 'lunchtime', 'lungs', 'lurk', 'lurking', 'luscious', 'lusciously', 'luster', 'lustre', 'luxe', 'luxurious', 'luxury', 'lychee', 'lychees', 'lying', 'lyonnaise', 'maca', 'macadamia', 'macadamias', 'macanese', 'macaron', 'macaroni', 'macarons', 'macaroon', 'macaroons', 'mace', 'macerate', 'macerated', 'macerating', 'machine', 'mackerel', 'mackerels', 'mad', 'made', 'madeira', 'madeleine', 'madeleines', 'madeline', 'madiera', 'madras', 'magazine', 'magic', 'magma', 'mahlab', 'mahlepi', 'mahogany', 'main', 'mainly', 'maintain', 'maintaining', 'maize', 'major', 'majoram', 'majority', 'make', 'maker', 'makes', 'maki', 'making', 'malai', 'malay', 'malaysian', 'malden', 'maldon', 'mallard', 'malleable', 'mallet', 'mallow', 'malt', 'maltaise', 'malted', 'maltodextrin', 'mamoosa', 'man', 'manage', 'manageable', 'manchego', 'mandarin', 'mandolin', 'mandoline', 'mane', 'mange', 'mangetout', 'mango', 'mangoes', 'mani', 'manie', 'manipulate', 'manis', 'manner', 'manoeuvre', 'manual', 'manually', 'manufacturer', 'manufacturers', 'many', 'maple', 'maraschino', 'marble', 'marbled', 'marbling', 'march', 'margarine', 'margin', 'maria', 'marie', 'marinade', 'marinara', 'marinate', 'marinated', 'marinating', 'mariniere', 'mariniers', 'marizpan', 'marjoram', 'mark', 'marked', 'market', 'marking', 'markings', 'marks', 'marmalade', 'marmitako', 'marmite', 'marnier', 'marquise', 'marries', 'marron', 'marrons', 'marrow', 'marrowbone', 'marrows', 'marrrow', 'marry', 'mars', 'marsala', 'marscapone', 'marscarpone', 'marsh', 'marshmallow', 'marshmallows', 'marshmallowy', 'martini', 'marula', 'mary', 'maryland', 'marzipan', 'masa', 'masala', 'mascarpone', 'mash', 'mashed', 'masher', 'mashing', 'mashwith', 'masking', 'mass', 'massa', 'massage', 'massaging', 'massala', 'massaman', 'masses', 'massive', 'massively', 'master', 'mastic', 'mat', 'match', 'matchbox', 'matches', 'matching', 'matchstick', 'matchsticks', 'material', 'materials', 'matrimony', 'mats', 'matt', 'matter', 'mature', 'matures', 'matzo', 'mauve', 'maximise', 'maximises', 'maximum', 'may', 'mayan', 'maybe', 'mayo', 'mayonaise', 'mayonnaise', 'mead', 'meal', 'mealie', 'mealy', 'mean', 'meaning', 'meanly', 'means', 'meant', 'meantime', 'meanwhile', 'measure', 'measured', 'measurement', 'measures', 'measuring', 'measurung', 'meat', 'meatball', 'meatballs', 'meatloaf', 'meats', 'meaty', 'medallion', 'medallions', 'medals', 'mediterranean', 'medium', 'medlars', 'medley', 'meet', 'meeting', 'meets', 'megrim', 'mein', 'melba', 'melds', 'mellow', 'mellowed', 'melon', 'melons', 'melt', 'melted', 'melting', 'meltingly', 'melts', 'members', 'membrane', 'membranes', 'membrillo', 'men', 'mend', 'menthe', 'mer', 'merely', 'merge', 'mergeuz', 'merging', 'merguez', 'merigues', 'meringue', 'meringues', 'merinuges', 'mesh', 'meshed', 'mess', 'message', 'messy', 'metal', 'metallic', 'methi', 'method', 'metre', 'metrexcm', 'mexican', 'mezze', 'mi', 'mic', 'micro', 'microleaf', 'microplane', 'microvave', 'microwavable', 'microwave', 'microwaveable', 'microwavem', 'microwaving', 'mid', 'middle', 'middles', 'midlle', 'midpoint', 'midway', 'migas', 'might', 'miix', 'milanese', 'mild', 'milk', 'milks', 'milkshake', 'milkshakes', 'milky', 'mill', 'mille', 'milled', 'millefeuille', 'millefeuilles', 'millet', 'millimetres', 'millionaire', 'mimic', 'mimicking', 'min', 'mince', 'minceand', 'minced', 'mincemeat', 'mincer', 'mincing', 'mind', 'mine', 'mineral', 'minerals', 'minestrone', 'mingle', 'mingled', 'mini', 'miniature', 'minimal', 'minimise', 'minimum', 'mins', 'mint', 'minted', 'mintues', 'minus', 'minuscule', 'minute', 'minutely', 'minutes', 'minutesm', 'minutesto', 'minutesutes', 'mirapois', 'mirepoix', 'mirin', 'mirror', 'miscalculated', 'miso', 'missed', 'misshapen', 'misshaping', 'missing', 'mist', 'mister', 'misto', 'mitzuna', 'mix', 'mixed', 'mixer', 'mixes', 'mixing', 'mixtuer', 'mixture', 'mixtures', 'mixure', 'mizing', 'mizuna', 'ml', 'mlfl', 'mm', 'mnutes', 'mocha', 'mock', 'mode', 'moderate', 'moderately', 'modern', 'modestly', 'moist', 'moisten', 'moistened', 'moistening', 'moisture', 'mojo', 'moka', 'molasses', 'molcajete', 'mold', 'molds', 'mole', 'molten', 'moment', 'momentarily', 'moments', 'money', 'monger', 'mongolian', 'monitor', 'monitoring', 'monk', 'monkey', 'monkfish', 'monkish', 'monks', 'monsieur', 'monster', 'mont', 'monte', 'month', 'months', 'monts', 'mooli', 'moon', 'moong', 'moons', 'mop', 'mopping', 'morel', 'morello', 'morels', 'mornay', 'morning', 'moroccan', 'morsels', 'mort', 'mortadella', 'mortal', 'mortar', 'morteau', 'moss', 'mostarda', 'mostly', 'motion', 'motions', 'motor', 'mottled', 'mould', 'mouldable', 'moulding', 'moulds', 'moules', 'mouli', 'moulis', 'mound', 'mounds', 'mountain', 'moussaka', 'mousse', 'mousseline', 'mousseron', 'mousses', 'moussey', 'mouth', 'mouthed', 'mouton', 'move', 'moved', 'movement', 'movements', 'moves', 'moving', 'mozzarella', 'mr', 'much', 'mud', 'muddle', 'muesli', 'muffin', 'muffins', 'mug', 'mugfuls', 'mugs', 'mugwith', 'mulberries', 'mulberry', 'mulch', 'mulled', 'mullet', 'multi', 'mummies', 'mung', 'murg', 'muscat', 'muscavado', 'muscle', 'muscles', 'muscovado', 'muscovardo', 'mush', 'mushroom', 'mushrooms', 'mushy', 'music', 'musical', 'muslin', 'mussel', 'mussells', 'mussels', 'must', 'mustaches', 'mustard', 'mustards', 'mustardy', 'mustia', 'muted', 'mutton', 'myoga', 'naan', 'naans', 'nachos', 'nad', 'nage', 'nail', 'naked', 'nam', 'napkin', 'napkins', 'napoleon', 'nargis', 'narrow', 'narrower', 'narrowest', 'narrowing', 'nasi', 'nasturtium', 'natural', 'naturally', 'navarin', 'navy', 'nb', 'ndjua', 'nduja', 'ne', 'near', 'nearby', 'nearer', 'nearest', 'nearly', 'neat', 'neaten', 'neater', 'neatest', 'neatly', 'neatness', 'necessary', 'neck', 'necks', 'nectar', 'nectarine', 'nectarines', 'need', 'needed', 'needle', 'needles', 'needs', 'neeps', 'negra', 'neither', 'nero', 'nest', 'nestle', 'nestled', 'nestling', 'nests', 'net', 'nettle', 'nettles', 'neutralising', 'never', 'new', 'newspaper', 'next', 'ngredients', 'nibbed', 'nibble', 'nibbles', 'nibs', 'nice', 'nicely', 'nicer', 'nicest', 'nick', 'nigella', 'night', 'nigiri', 'nine', 'ninety', 'nip', 'nitro', 'nobody', 'noddle', 'noddles', 'nog', 'noise', 'noisette', 'noisettes', 'noissette', 'non', 'none', 'nonstick', 'nonstop', 'noodle', 'noodles', 'nook', 'nooks', 'noose', 'nori', 'normal', 'normally', 'nose', 'noses', 'notch', 'note', 'notebook', 'notes', 'nothing', 'notice', 'noting', 'nougat', 'nougatine', 'nowhere', 'nozzle', 'nozzled', 'nozzles', 'nthe', 'nubbly', 'nudge', 'nudges', 'nugget', 'nuggets', 'number', 'numbers', 'nuoc', 'nut', 'nutmeg', 'nutritious', 'nuts', 'nuttiness', 'nutty', 'nylon', 'oak', 'oaked', 'oasis', 'oat', 'oatcake', 'oatcakes', 'oatflakes', 'oatmeal', 'oats', 'object', 'objects', 'oblique', 'oblong', 'obsession', 'obtain', 'obtained', 'obvious', 'obviously', 'occasion', 'occasional', 'occasionally', 'occassionally', 'ocopa', 'octopus', 'odd', 'oelek', 'offal', 'offcuts', 'offer', 'offers', 'officiously', 'offset', 'often', 'ofthe', 'ogleshield', 'oil', 'oiled', 'oilin', 'oill', 'oils', 'oilseed', 'oily', 'ok', 'okay', 'okra', 'old', 'oli', 'olive', 'olives', 'oloroso', 'omelette', 'omelettes', 'omit', 'omitting', 'onced', 'one', 'ones', 'onglet', 'onion', 'onions', 'oniony', 'online', 'onsistency', 'onto', 'ontop', 'oolong', 'ooze', 'oozes', 'oozing', 'oozy', 'opaque', 'open', 'opened', 'opener', 'opening', 'openings', 'opens', 'opera', 'operated', 'opposed', 'opposing', 'opposite', 'optimal', 'option', 'optional', 'ora', 'orach', 'orange', 'oranges', 'orangey', 'order', 'ordinary', 'ordination', 'oregano', 'oreo', 'oriental', 'original', 'orthodox', 'oruntil', 'orzo', 'os', 'ossi', 'ot', 'otherend', 'others', 'otherside', 'otherwise', 'ottoman', 'outdoor', 'outdoors', 'outequally', 'outer', 'outing', 'outline', 'outside', 'outsides', 'outto', 'outward', 'outwards', 'oval', 'ovals', 'ove', 'oveb', 'oven', 'ovenproof', 'ovenrpoof', 'ovenrproof', 'ovens', 'ovenware', 'overbake', 'overbeat', 'overbrown', 'overcomes', 'overcook', 'overcooked', 'overcooking', 'overcrowd', 'overcrowding', 'overdo', 'overdone', 'overfill', 'overflow', 'overhanding', 'overhang', 'overhanging', 'overhangs', 'overheat', 'overlap', 'overlapped', 'overlapping', 'overlaps', 'overlaying', 'overload', 'overloading', 'overly', 'overmix', 'overnight', 'overpower', 'overpowering', 'overproof', 'overripe', 'overspills', 'overwhip', 'overwhisk', 'overwork', 'overworking', 'ower', 'ox', 'oxen', 'oxford', 'oxidise', 'oxidising', 'oxidization', 'oxtail', 'oyster', 'oysters', 'oz', 'ozif', 'pac', 'pace', 'pack', 'package', 'packages', 'packaging', 'packed', 'packer', 'packet', 'packets', 'packing', 'pacojet', 'pad', 'paddle', 'paddling', 'padr', 'padron', 'paella', 'pages', 'pain', 'paint', 'paintbrush', 'painted', 'painting', 'pair', 'paired', 'pairing', 'pairs', 'paisley', 'pak', 'pakora', 'pakoras', 'palatable', 'palate', 'palatte', 'palce', 'pale', 'paleness', 'paler', 'palette', 'pallet', 'pallete', 'pallette', 'palm', 'palmier', 'palmiers', 'palms', 'paloise', 'palourde', 'pan', 'panacotta', 'pancake', 'pancakes', 'pancetta', 'panch', 'panchetta', 'pandan', 'pandora', 'pandoro', 'pane', 'paneer', 'panels', 'panettone', 'panettones', 'panfry', 'pangrattato', 'panhaggerty', 'panhandle', 'panic', 'panini', 'panko', 'panna', 'pannacotta', 'pannetone', 'pans', 'pansies', 'pansotti', 'panzanella', 'papardelle', 'papaya', 'papayas', 'paper', 'papers', 'papery', 'papeta', 'papillote', 'pappa', 'pappardelle', 'papper', 'paprika', 'paprikas', 'par', 'parallel', 'paratha', 'parathas', 'parboil', 'parboiled', 'parboiling', 'parcel', 'parcels', 'parchment', 'pare', 'parfait', 'parfaits', 'paring', 'paris', 'parisian', 'parisienne', 'park', 'parkin', 'parlsey', 'parma', 'parmentier', 'parmesan', 'parmigiana', 'parnsips', 'parsee', 'parsely', 'parsley', 'parsnip', 'parsnips', 'part', 'partially', 'particles', 'particular', 'particularly', 'partly', 'partridge', 'partridges', 'parts', 'party', 'pasilla', 'pasrty', 'pass', 'passata', 'passed', 'passes', 'passing', 'passion', 'passionfruit', 'passionfruits', 'past', 'pasta', 'paste', 'pasted', 'pastel', 'pastes', 'pasteurised', 'pastie', 'pasties', 'pastilla', 'pastillas', 'pastille', 'pastis', 'pastrami', 'pastries', 'pastry', 'pastryo', 'pasty', 'pat', 'pata', 'patatas', 'patch', 'patched', 'patches', 'patchily', 'patching', 'patchwork', 'pate', 'patia', 'patient', 'patiently', 'patisserie', 'patissi', 'patissiere', 'patted', 'patter', 'pattern', 'patterned', 'patterns', 'pattie', 'patties', 'patting', 'patty', 'paul', 'pause', 'pauvre', 'pav', 'pavlova', 'pavlovas', 'paw', 'pay', 'payasam', 'pe', 'pea', 'peach', 'peaches', 'peachy', 'peacock', 'peak', 'peaked', 'peaking', 'peaks', 'peanut', 'peanuts', 'pear', 'pearl', 'pearlescent', 'pearls', 'pearly', 'pears', 'peas', 'pease', 'peashoots', 'pebbles', 'pecan', 'pecans', 'pecorino', 'pecornio', 'pectin', 'peek', 'peel', 'peeled', 'peeler', 'peelers', 'peeling', 'peelings', 'peels', 'peeping', 'peg', 'pegs', 'pellets', 'pen', 'pence', 'pencil', 'pencilled', 'penetrate', 'penne', 'pennette', 'penny', 'pennywort', 'penultimate', 'people', 'pepe', 'peper', 'peperonata', 'peperoncini', 'peperoncino', 'peppadew', 'peppadews', 'peppe', 'pepper', 'peppercorn', 'peppercorns', 'peppered', 'peppermill', 'peppermint', 'pepperoni', 'pepperpot', 'peppers', 'peppery', 'pequillo', 'per', 'percent', 'perch', 'percorino', 'perdu', 'perfect', 'perfection', 'perfectly', 'perforated', 'perform', 'performed', 'perfumed', 'perhaps', 'perheated', 'peri', 'perilla', 'perimeter', 'period', 'periodically', 'periphery', 'perk', 'perky', 'permeate', 'permeating', 'permits', 'pernod', 'perrins', 'perry', 'perseverance', 'persian', 'persillade', 'persistent', 'person', 'personal', 'peruvian', 'pes', 'peshawari', 'pestle', 'pesto', 'petal', 'petalled', 'petals', 'petit', 'petits', 'pf', 'pheasant', 'pheasants', 'phoran', 'phrases', 'phwoar', 'physalis', 'piano', 'picada', 'piccalilli', 'pice', 'pices', 'pici', 'pick', 'picked', 'pickers', 'picking', 'pickle', 'pickled', 'pickles', 'pickling', 'picks', 'picnic', 'picnics', 'pico', 'picture', 'pictured', 'pide', 'pie', 'piece', 'pieced', 'pieces', 'pied', 'piedmont', 'pierce', 'pierced', 'piercing', 'pierogi', 'pies', 'pig', 'pigeon', 'pigeons', 'piggledy', 'pigs', 'pilaf', 'pilaff', 'pilau', 'pile', 'piled', 'piles', 'piling', 'pillar', 'pillars', 'pillow', 'pillowcase', 'pillows', 'pillowy', 'piment', 'pimento', 'pimenton', 'pimentos', 'pimiento', 'pimientos', 'pimm', 'pimms', 'pin', 'pincers', 'pinch', 'pinched', 'pinches', 'pinching', 'pinchos', 'pine', 'pineapple', 'pineapples', 'pinenuts', 'ping', 'pinhead', 'pink', 'pinkness', 'pins', 'pinstripe', 'pint', 'pinto', 'pints', 'pinwheel', 'pinwheels', 'pip', 'pipe', 'pipeable', 'piped', 'pipette', 'piping', 'pips', 'piquant', 'piquillo', 'pirags', 'pirate', 'piri', 'pisco', 'pissaladi', 'pistachio', 'pistachios', 'pistil', 'pistou', 'pit', 'pita', 'pitcher', 'pith', 'pithivier', 'pithiviers', 'pitta', 'pittas', 'pitted', 'pizza', 'pizzas', 'pizzetta', 'pizzette', 'pizzoccheri', 'pizzs', 'pla', 'place', 'placea', 'placed', 'placeon', 'places', 'placing', 'plack', 'plaice', 'plain', 'plainer', 'plait', 'plaited', 'plaits', 'plan', 'plank', 'planks', 'planning', 'plant', 'plantain', 'plantains', 'plasters', 'plastic', 'plate', 'plateful', 'plates', 'platform', 'plating', 'platter', 'platters', 'play', 'pleasant', 'pleasantly', 'please', 'pleasing', 'pleasingly', 'pleasure', 'pleat', 'pleated', 'pleating', 'pleats', 'plenty', 'pletely', 'pliable', 'pliant', 'pliers', 'plop', 'pluck', 'plug', 'plum', 'pluma', 'plume', 'plump', 'plumped', 'plums', 'plunge', 'plunging', 'plus', 'plushes', 'pn', 'poach', 'poached', 'poacher', 'poaches', 'poaching', 'poblano', 'pocked', 'pocket', 'pockets', 'pod', 'podded', 'podding', 'pods', 'poha', 'point', 'pointed', 'pointier', 'pointing', 'pointless', 'points', 'pointy', 'poire', 'pois', 'poke', 'poked', 'pokes', 'poking', 'polenta', 'polish', 'pollack', 'pollen', 'pollo', 'pollock', 'polonaise', 'polpettone', 'poly', 'polystyrene', 'polythene', 'pomace', 'pomegranate', 'pomegranates', 'pomelo', 'pomelos', 'pomme', 'pommes', 'pomodoro', 'pond', 'pong', 'pontefract', 'ponzu', 'pool', 'pooled', 'pools', 'poor', 'pop', 'popcorn', 'poppadoms', 'poppadum', 'poppadums', 'popped', 'popping', 'poppy', 'pops', 'porchetta', 'porcini', 'pores', 'poriyal', 'pork', 'porridge', 'port', 'portabello', 'portion', 'portioned', 'portions', 'portobello', 'portuguesa', 'posh', 'position', 'positioned', 'positioning', 'positions', 'posset', 'possets', 'possible', 'postage', 'pot', 'potaotes', 'potato', 'potatoes', 'potator', 'potentially', 'potions', 'pots', 'potted', 'potting', 'pouch', 'pouches', 'pouchong', 'poultry', 'pound', 'pounded', 'pounding', 'pour', 'pourable', 'poured', 'pourer', 'pouring', 'poussin', 'poussins', 'povitica', 'powder', 'powdered', 'powders', 'powdery', 'power', 'powered', 'powerful', 'practice', 'praline', 'prawn', 'prawns', 'pre', 'preapred', 'precious', 'precise', 'precook', 'precooked', 'precooking', 'precut', 'prefer', 'preferable', 'preferably', 'prefered', 'preference', 'preferred', 'preferring', 'preheat', 'preheated', 'preheating', 'preparation', 'prepare', 'prepared', 'preparing', 'prepated', 'present', 'presentable', 'presentation', 'presenting', 'preserve', 'preserved', 'preserves', 'preserving', 'press', 'pressed', 'pressing', 'pressure', 'presto', 'prettiest', 'pretty', 'pretzel', 'pretzels', 'prevent', 'prevents', 'previous', 'previously', 'prick', 'pricked', 'prickly', 'primavera', 'prime', 'primrose', 'primroses', 'printing', 'prints', 'prior', 'prise', 'prising', 'probably', 'probe', 'problem', 'procedure', 'proceed', 'proceedings', 'process', 'processco', 'processed', 'processer', 'processing', 'processor', 'processors', 'prod', 'prodded', 'prodding', 'produce', 'produced', 'produces', 'producing', 'product', 'production', 'professional', 'profiterole', 'profiteroles', 'progress', 'progresses', 'prongs', 'pronounced', 'pronto', 'proof', 'prop', 'propelling', 'proper', 'properly', 'proportion', 'proportionately', 'proportions', 'propped', 'propping', 'props', 'prosciuto', 'prosciutto', 'proscuitto', 'prosecco', 'prosperity', 'protect', 'protecting', 'protection', 'protective', 'protects', 'proteins', 'protrude', 'protruding', 'proudly', 'prove', 'proved', 'proven', 'provencal', 'provence', 'proves', 'provide', 'provided', 'providing', 'proving', 'provolone', 'prs', 'prune', 'prunes', 'pry', 'psyllium', 'pt', 'ptfl', 'pu', 'pudding', 'puddings', 'puddle', 'puff', 'puffball', 'puffed', 'puffing', 'puffs', 'puffy', 'pugliese', 'pul', 'pulao', 'pull', 'pullao', 'pulled', 'pulling', 'pulls', 'pulp', 'pulped', 'pulpy', 'pulse', 'pulses', 'pulsing', 'pummel', 'pummelling', 'pumpernickel', 'pumpkin', 'pumpkins', 'punch', 'punching', 'pungent', 'punnet', 'punnets', 'puntarella', 'pupils', 'puppy', 'pur', 'pure', 'puree', 'pureed', 'purer', 'puri', 'purie', 'purple', 'purplish', 'purpose', 'purposes', 'purse', 'purslane', 'push', 'pushed', 'pushes', 'pushing', 'put', 'puttanesca', 'putting', 'putty', 'puy', 'puzzle', 'pyramid', 'pyramids', 'pyrex', 'quadrupled', 'quail', 'quails', 'qualities', 'quality', 'quantities', 'quantity', 'quark', 'quarter', 'quartered', 'quarters', 'quatre', 'queen', 'quenelle', 'quenelles', 'quesada', 'quesadilla', 'quesadillas', 'question', 'quiche', 'quiches', 'quick', 'quicker', 'quickest', 'quickly', 'quietly', 'quill', 'quilting', 'quince', 'quinces', 'quinoa', 'quinoas', 'quite', 'quorn', 'rabbit', 'rack', 'racket', 'rackets', 'racks', 'radiating', 'radiator', 'radicchio', 'radish', 'radishes', 'raffia', 'rag', 'ragged', 'rago', 'ragout', 'rags', 'ragu', 'rainbow', 'raining', 'raise', 'raised', 'raises', 'raisin', 'raising', 'raisins', 'raita', 'ram', 'ramekin', 'ramekins', 'ramen', 'ranch', 'rancid', 'random', 'randomly', 'range', 'rao', 'rape', 'rapeseed', 'rapid', 'rapidly', 'rare', 'rarebit', 'rarebits', 'ras', 'rasayana', 'rascal', 'rascals', 'rasher', 'rashers', 'raspberries', 'raspberry', 'ratafia', 'ratafias', 'ratatouille', 'rate', 'rather', 'ratio', 'ratte', 'ravioli', 'raviolo', 'raw', 'rawness', 'razor', 'rds', 'reach', 'reached', 'reaches', 'reachesc', 'reaching', 'react', 'reacting', 'reaction', 'reactive', 'read', 'readily', 'reading', 'reads', 'ready', 'real', 'realise', 'realistic', 'really', 'reamining', 'rear', 'rearrange', 'reason', 'reasonable', 'reasonably', 'reassemble', 'reblechon', 'reblochon', 'reboil', 'rec', 'receive', 'recently', 'receptacle', 'recipe', 'recipes', 'recommend', 'recommended', 'recommends', 'reconstitute', 'reconstruct', 'recover', 'rectangle', 'rectangles', 'rectangular', 'red', 'redcurrant', 'redcurrants', 'redistribute', 'reduce', 'reduced', 'reduces', 'reducing', 'reducrrant', 'reduction', 'refer', 'reference', 'refill', 'refolding', 'reform', 'refreeze', 'refresh', 'refreshed', 'refreshing', 'refridgerate', 'refried', 'refrigerate', 'refrigerated', 'refrigerating', 'refrigerator', 'refuse', 'registers', 'regular', 'regularly', 'reheat', 'reheated', 'reheating', 'reheats', 'rehydrate', 'rehydrated', 'reindeer', 'rejuvenator', 'reknead', 'relative', 'relatively', 'relax', 'relaxed', 'relaxes', 'release', 'released', 'releases', 'releasing', 'relevant', 'religieuse', 'relish', 'rellish', 'reluctant', 'remain', 'remainder', 'remained', 'remaing', 'remaining', 'remainins', 'remainng', 'remains', 'remaning', 'remember', 'remembering', 'reminaing', 'reminder', 'remnants', 'remoulade', 'removable', 'remove', 'removed', 'remover', 'removes', 'removing', 'rendang', 'render', 'rendered', 'rendering', 'renders', 'rending', 'renmaining', 'rennet', 'reopen', 'repair', 'repeat', 'repeated', 'repeatedly', 'repeating', 'repeats', 'replace', 'replacing', 'replenish', 'replenishing', 'represent', 'reproduce', 'require', 'required', 'requires', 'reroll', 'rerolled', 'rerolling', 'res', 'rescue', 'reseal', 'resealable', 'resemble', 'resembles', 'resembling', 'reserve', 'reserved', 'reserving', 'reshape', 'reshaping', 'residual', 'residue', 'residues', 'resist', 'resistance', 'resistant', 'resists', 'respective', 'rest', 'rested', 'resting', 'restrain', 'rests', 'result', 'resulting', 'results', 'resume', 'retain', 'retained', 'retaining', 'retains', 'retracted', 'return', 'returned', 'returning', 'returns', 'reusable', 'reused', 'reusing', 'reveal', 'revealed', 'revealing', 'reverse', 'revert', 'reward', 'rewrap', 'rg', 'rhubarb', 'rib', 'ribbed', 'ribbon', 'ribbons', 'ribcage', 'ribs', 'ricard', 'rice', 'riced', 'ricer', 'rices', 'rich', 'richer', 'richly', 'richness', 'rico', 'ricotta', 'rid', 'ridged', 'ridges', 'ridiculously', 'rie', 'riesling', 'rigatoni', 'right', 'righted', 'rigid', 'rigidity', 'rigorous', 'rigorously', 'rillette', 'rillettes', 'rillons', 'rim', 'rimmed', 'rims', 'rind', 'rinds', 'rinforzo', 'ring', 'ringlets', 'rings', 'rinse', 'rinsed', 'rinsing', 'rioja', 'rip', 'ripe', 'ripeness', 'ripped', 'ripple', 'rippled', 'ripples', 'rise', 'risen', 'rises', 'rising', 'risk', 'risotto', 'risottos', 'rissoles', 'river', 'road', 'roast', 'roasted', 'roasting', 'roasts', 'robin', 'robust', 'rock', 'rockefeller', 'rocket', 'rocking', 'rocks', 'rocky', 'rod', 'rods', 'roe', 'roes', 'rogue', 'rojak', 'roll', 'rollable', 'rolled', 'roller', 'rollers', 'rolling', 'rollout', 'rolls', 'roly', 'romaine', 'romana', 'romanesco', 'romano', 'romanoff', 'romero', 'romesco', 'roof', 'roofs', 'rooftops', 'room', 'roomy', 'root', 'roots', 'rope', 'ropes', 'roquefort', 'rosace', 'rosaces', 'rose', 'rosemary', 'roses', 'rosette', 'rosettes', 'rosewater', 'rossini', 'rosti', 'rostis', 'rosy', 'rotary', 'rotate', 'rotating', 'rotation', 'roti', 'rotis', 'rotollo', 'rotolo', 'rough', 'roughen', 'roughened', 'roughly', 'rouille', 'roulade', 'roulades', 'round', 'rounded', 'rounder', 'roundness', 'rounds', 'roux', 'rover', 'row', 'rowan', 'rows', 'royal', 'royale', 'royals', 'rub', 'rubbed', 'rubber', 'rubbery', 'rubbing', 'rubble', 'ruby', 'ruffle', 'ruffles', 'rugby', 'ruin', 'ruined', 'rule', 'ruler', 'rum', 'rumoured', 'rump', 'rumps', 'rums', 'run', 'runner', 'runnier', 'running', 'runny', 'runs', 'rush', 'russe', 'russet', 'russian', 'rustic', 'rustle', 'rye', 'saag', 'sabayon', 'sabl', 'sable', 'sables', 'sac', 'sacher', 'sachet', 'sachets', 'sack', 'sacs', 'saddle', 'sadly', 'sadza', 'saeson', 'safe', 'safely', 'safer', 'safest', 'saffron', 'sag', 'sage', 'saikyo', 'sail', 'sails', 'saint', 'sak', 'sake', 'salad', 'salads', 'salady', 'salame', 'salami', 'salata', 'salmon', 'salmoriglio', 'salsa', 'salsify', 'salt', 'salted', 'saltfish', 'saltimbocca', 'saltimboccas', 'saltiness', 'salting', 'salts', 'salty', 'salut', 'sambal', 'sambuca', 'samosa', 'samosas', 'samphire', 'sancho', 'sand', 'sandpaper', 'sandwhich', 'sandwich', 'sandwiched', 'sandwiches', 'sandwiching', 'sandy', 'sangrita', 'sansho', 'santa', 'santo', 'sapeur', 'sarde', 'sardine', 'sardines', 'sarnies', 'sarsaparilla', 'sashimi', 'sat', 'satay', 'satiny', 'satisfaction', 'satisfied', 'satsuma', 'satsumas', 'satumas', 'saturated', 'sau', 'sauce', 'sauceboat', 'sauceboats', 'saucepan', 'saucepanful', 'saucepans', 'saucer', 'saucers', 'sauces', 'saucisson', 'saucy', 'sauerkraut', 'sausage', 'sausagemeat', 'sausagement', 'sausagemet', 'sausages', 'sausges', 'saut', 'saute', 'sauternes', 'sauvignon', 'savarin', 'save', 'saved', 'saving', 'savoiardi', 'savory', 'savoury', 'savoy', 'saw', 'sawing', 'say', 'saying', 'says', 'sbord', 'scad', 'scald', 'scalded', 'scalding', 'scalds', 'scale', 'scaled', 'scales', 'scallion', 'scallop', 'scalloped', 'scallops', 'scalpel', 'scamorza', 'scampi', 'scant', 'scape', 'scapes', 'scared', 'scatter', 'scattered', 'scattering', 'scent', 'scented', 'scewer', 'schedule', 'schicimi', 'schnapps', 'schnitzel', 'schnitzels', 'scissors', 'scollop', 'scone', 'scones', 'scoop', 'scooped', 'scooping', 'scoops', 'scoot', 'scorch', 'scorched', 'scorching', 'score', 'scored', 'scores', 'scoring', 'scorings', 'scotch', 'scourer', 'scouse', 'scraggy', 'scramble', 'scrambled', 'scrambling', 'scrape', 'scraped', 'scraper', 'scraping', 'scrapings', 'scrapingup', 'scrapping', 'scraps', 'scratch', 'scratchings', 'screen', 'screw', 'screwdriver', 'screwing', 'screwpine', 'scribed', 'scroll', 'scrub', 'scrubbed', 'scrubbing', 'scruffy', 'scrum', 'scrumpy', 'scrunch', 'scrunched', 'scrunching', 'scrupulously', 'scuff', 'sculpt', 'scum', 'sea', 'seabass', 'seafood', 'seal', 'sealable', 'sealed', 'sealer', 'sealing', 'seals', 'seam', 'seame', 'seamless', 'seams', 'sear', 'seared', 'searing', 'searingly', 'seashell', 'season', 'seasonal', 'seasoned', 'seasoning', 'seasonings', 'seated', 'seawater', 'seaweed', 'sechuan', 'second', 'seconds', 'secret', 'secreto', 'section', 'sections', 'secure', 'secured', 'securely', 'securing', 'sediment', 'sediments', 'see', 'seed', 'seeded', 'seeding', 'seedless', 'seedlings', 'seeds', 'seedy', 'seem', 'seems', 'seen', 'seep', 'seeped', 'seeping', 'seera', 'segment', 'segmented', 'segmenting', 'segments', 'seive', 'seize', 'seizes', 'sel', 'select', 'selected', 'selection', 'self', 'semi', 'semicircle', 'semicircles', 'semicircular', 'semifreddo', 'semifreddos', 'semolina', 'separate', 'separated', 'separately', 'separates', 'separating', 'separator', 'seperate', 'sequence', 'serenely', 'series', 'serious', 'serrano', 'serrated', 'serve', 'served', 'service', 'serving', 'servings', 'sesame', 'sessions', 'set', 'sets', 'setting', 'settings', 'settle', 'settled', 'settles', 'seve', 'seven', 'several', 'seville', 'sew', 'shade', 'shadow', 'shaggy', 'shake', 'shaken', 'shaker', 'shakes', 'shaking', 'shallot', 'shallots', 'shallow', 'shank', 'shanks', 'shaoxing', 'shape', 'shaped', 'shapes', 'shaping', 'shard', 'shards', 'share', 'shared', 'sharing', 'sharon', 'sharp', 'sharpen', 'sharpening', 'sharpest', 'sharply', 'sharpness', 'shatkora', 'shatter', 'shattering', 'shave', 'shaved', 'shaves', 'shaving', 'shavings', 'shears', 'sheek', 'sheen', 'sheep', 'sheeps', 'sheet', 'sheets', 'sheftalia', 'shelf', 'shell', 'shelled', 'shellfish', 'shells', 'shellsfish', 'shelves', 'shepherd', 'sherbet', 'sherry', 'shichimi', 'shiitake', 'shimeji', 'shimmer', 'shimmering', 'shimmers', 'shin', 'shine', 'shininess', 'shining', 'shins', 'shiny', 'shio', 'shirt', 'shisho', 'shiso', 'shoestring', 'shonka', 'shoot', 'shoots', 'shop', 'shops', 'short', 'shortbread', 'shortbreads', 'shortcake', 'shortcakes', 'shortcrust', 'shortcut', 'shorten', 'shortening', 'shorter', 'shortest', 'shortly', 'shot', 'shots', 'shoulder', 'shoulders', 'show', 'shower', 'showing', 'shown', 'shows', 'shoyu', 'shred', 'shredded', 'shredder', 'shredding', 'shreddy', 'shreds', 'shrikhand', 'shrimp', 'shrimps', 'shrink', 'shrinkage', 'shrinking', 'shrinks', 'shrivel', 'shrivelled', 'shropshire', 'shrunk', 'shuck', 'shucked', 'shucker', 'shut', 'shy', 'sichuan', 'sicilia', 'sicilian', 'side', 'sideboard', 'sided', 'sides', 'sideways', 'sieve', 'sieved', 'sieving', 'sift', 'sifted', 'sifting', 'sign', 'signature', 'significantly', 'signs', 'silhouette', 'silicon', 'silicone', 'silken', 'silkier', 'silky', 'silt', 'silver', 'silverskin', 'silvery', 'similar', 'similarly', 'simmer', 'simmered', 'simmering', 'simmers', 'simon', 'simple', 'simpler', 'simply', 'sin', 'since', 'sinew', 'sinews', 'sinewy', 'singapore', 'singe', 'singed', 'single', 'singly', 'sink', 'sinkful', 'sinking', 'sinks', 'sip', 'siphon', 'sir', 'sirloin', 'sit', 'site', 'sits', 'sitting', 'six', 'sixteen', 'sixth', 'sixths', 'sizable', 'size', 'sized', 'sizes', 'sizzle', 'sizzles', 'sizzling', 'skate', 'skeleton', 'skeletons', 'sketching', 'skewer', 'skewered', 'skewering', 'skewers', 'skilled', 'skillet', 'skillets', 'skim', 'skimmed', 'skimming', 'skimp', 'skin', 'skinless', 'skinned', 'skinning', 'skinny', 'skins', 'skip', 'skirlie', 'skirt', 'skirts', 'skordalia', 'sky', 'slab', 'slabs', 'slacken', 'slackens', 'slams', 'slant', 'slanted', 'slanting', 'slap', 'slapping', 'slash', 'slashed', 'slashes', 'slashing', 'slate', 'slates', 'slather', 'slathered', 'slathering', 'slaw', 'sleep', 'sleeve', 'sleeves', 'slender', 'sli', 'slice', 'sliced', 'slicer', 'slices', 'slicing', 'slick', 'slide', 'slides', 'slidethe', 'sliding', 'slighly', 'slight', 'slightest', 'slightly', 'slim', 'slip', 'slippery', 'slipping', 'slips', 'slit', 'slither', 'slithers', 'slits', 'sliver', 'slivers', 'sloe', 'sloes', 'slope', 'sloping', 'sloppy', 'slot', 'slots', 'slotted', 'slow', 'slower', 'slowest', 'slowly', 'slug', 'slurps', 'slush', 'slushy', 'sm', 'small', 'smaller', 'smallest', 'smallish', 'smalll', 'smarter', 'smash', 'smashed', 'smashing', 'smear', 'smearing', 'smell', 'smelling', 'smells', 'smidge', 'smile', 'smiles', 'smith', 'smiths', 'smoke', 'smoked', 'smoker', 'smokes', 'smokey', 'smokie', 'smokier', 'smokies', 'smoking', 'smoky', 'smoosh', 'smooth', 'smoother', 'smoothie', 'smoothies', 'smoothing', 'smoothish', 'smoothly', 'smoothness', 'smother', 'smothered', 'smoulder', 'smouldering', 'snack', 'snacking', 'snacks', 'snail', 'snails', 'snake', 'snap', 'snapped', 'snapper', 'snaps', 'sneaky', 'sneeze', 'snickers', 'snip', 'snipped', 'snipping', 'snow', 'snowcapped', 'snowflake', 'snowflakes', 'snows', 'snug', 'snuggly', 'snugly', 'soak', 'soaked', 'soaking', 'soaks', 'soapy', 'soba', 'socca', 'soda', 'sodium', 'soffritto', 'soft', 'softball', 'soften', 'softened', 'softenened', 'softening', 'softens', 'softer', 'softest', 'softish', 'softly', 'softness', 'soggy', 'soi', 'soil', 'sold', 'soldiers', 'sole', 'soles', 'solid', 'solidified', 'solidifies', 'solidify', 'solidifying', 'solids', 'solution', 'somen', 'someone', 'something', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sooner', 'sorbet', 'sorbets', 'sorrel', 'sort', 'souchong', 'souff', 'souffl', 'souffle', 'souffles', 'souffli', 'sound', 'sounds', 'soup', 'soups', 'soupy', 'sour', 'source', 'sourdough', 'soured', 'sourer', 'sous', 'soused', 'sousing', 'southern', 'souvlakia', 'soy', 'soya', 'sp', 'space', 'spaced', 'spacer', 'spacers', 'spaces', 'spacing', 'spaghetti', 'spaghettini', 'span', 'spanakopita', 'spanish', 'spanking', 'spare', 'spareribs', 'sparing', 'sparingly', 'sparkle', 'sparkling', 'spatchcock', 'spatchcocked', 'spatchcocking', 'spatter', 'spatula', 'spatulas', 'spatzle', 'speak', 'spear', 'spearing', 'spears', 'spec', 'special', 'specialist', 'speck', 'speckle', 'speckled', 'specks', 'spectacle', 'spectacles', 'speed', 'speeding', 'speeds', 'speedy', 'spell', 'spelt', 'sphere', 'spheres', 'spherical', 'spice', 'spiced', 'spices', 'spiciness', 'spicy', 'spider', 'spike', 'spikes', 'spiky', 'spill', 'spillage', 'spilled', 'spillers', 'spilling', 'spills', 'spin', 'spinach', 'spine', 'spinkle', 'spinner', 'spinning', 'spiral', 'spiraling', 'spiralised', 'spiraliser', 'spiralized', 'spiralizer', 'spiralled', 'spirals', 'spirit', 'spirlised', 'spit', 'spits', 'spitting', 'splash', 'splashed', 'splashes', 'splashing', 'splater', 'splatter', 'splattering', 'splay', 'splendid', 'splinters', 'split', 'splits', 'splitting', 'splutter', 'spoil', 'spoilage', 'spoilt', 'spokes', 'sponge', 'sponges', 'spongey', 'sponginess', 'spongy', 'spooky', 'spoon', 'spoonable', 'spooned', 'spoonful', 'spoonfuls', 'spooning', 'spoons', 'spot', 'spotlessly', 'spots', 'spotted', 'spout', 'spouted', 'sprat', 'sprats', 'spray', 'sprayed', 'spraying', 'spread', 'spreadable', 'spreading', 'spreads', 'sprig', 'sprigs', 'spring', 'springclip', 'springform', 'springfrom', 'springing', 'springs', 'springy', 'sprinke', 'sprinked', 'sprinkle', 'sprinkled', 'sprinkles', 'sprinkling', 'sprinklings', 'spritz', 'sprout', 'sprouted', 'sprouting', 'sprouts', 'spud', 'spuds', 'spun', 'spurt', 'spurting', 'spurts', 'squab', 'square', 'squared', 'squares', 'squaring', 'squash', 'squashed', 'squashes', 'squashing', 'squashy', 'squeak', 'squeaky', 'squeeze', 'squeezed', 'squeezing', 'squeezy', 'squelch', 'squid', 'squidge', 'squidging', 'squidgy', 'squidy', 'squiggle', 'squiggles', 'squirrel', 'squirt', 'squirting', 'squish', 'squished', 'squishing', 'squodge', 'sream', 'sri', 'sriracha', 'ssamjang', 'st', 'stab', 'stabbing', 'stabilise', 'stable', 'stack', 'stacked', 'stacking', 'stacks', 'stage', 'stages', 'staggered', 'stain', 'stained', 'staining', 'stainless', 'stale', 'stalk', 'stalks', 'stall', 'stamens', 'stamp', 'stand', 'standard', 'standing', 'stands', 'stanley', 'stapler', 'star', 'starch', 'starches', 'starchy', 'staring', 'stars', 'start', 'startch', 'started', 'starter', 'starting', 'starts', 'state', 'stated', 'station', 'stationer', 'stay', 'stays', 'stead', 'steadily', 'steady', 'steak', 'steaks', 'steam', 'steamed', 'steamer', 'steamers', 'steaming', 'steams', 'steel', 'steelrings', 'steep', 'steeped', 'stem', 'stemmed', 'stems', 'stencil', 'stenciled', 'stencils', 'step', 'steps', 'sterile', 'sterilise', 'sterilised', 'sterilize', 'sterilized', 'sterlise', 'sterlised', 'stern', 'stevia', 'stew', 'stewed', 'stewing', 'stewpan', 'stews', 'sti', 'stick', 'sticker', 'stickier', 'stickiness', 'sticking', 'sticks', 'sticky', 'stiff', 'stiffen', 'stiffened', 'stiffens', 'stiffer', 'stiffly', 'still', 'stilton', 'sting', 'stir', 'stiring', 'stirred', 'stirring', 'stirs', 'stis', 'stitch', 'stock', 'stockpot', 'stocks', 'stodgy', 'stollen', 'stomach', 'stone', 'stones', 'stood', 'stool', 'stop', 'stopped', 'stopper', 'stoppers', 'stopping', 'stops', 'storage', 'store', 'stored', 'stores', 'storing', 'story', 'stout', 'stove', 'stovetop', 'stovies', 'straight', 'straightaway', 'straighten', 'straighter', 'straightway', 'strain', 'strained', 'strainer', 'straining', 'strand', 'strands', 'strata', 'straw', 'strawberries', 'strawberry', 'straws', 'stray', 'streaks', 'streaky', 'stream', 'strength', 'strengthen', 'stressing', 'stretch', 'stretched', 'stretches', 'stretching', 'stretchy', 'streusel', 'strew', 'strewing', 'strewn', 'stri', 'strictly', 'string', 'strings', 'stringy', 'strip', 'stripe', 'striped', 'stripes', 'stripey', 'stripped', 'strips', 'strirring', 'stroganoff', 'strokes', 'stromboli', 'strong', 'stronger', 'strongly', 'structure', 'strudel', 'strudels', 'struffoli', 'struggle', 'struggling', 'strwberries', 'stubble', 'stubborn', 'stubby', 'stuck', 'stud', 'studded', 'stuff', 'stuffed', 'stuffing', 'stun', 'stupid', 'sturdy', 'style', 'stylish', 'submerge', 'submerged', 'submerging', 'subsequent', 'subside', 'subsided', 'subsides', 'substitute', 'substituting', 'subtle', 'subtlety', 'successful', 'succotash', 'succulent', 'suck', 'sucked', 'suckling', 'sucr', 'suddenly', 'suet', 'suffers', 'sufficient', 'sufficiently', 'sugar', 'sugared', 'sugarpaste', 'sugars', 'sugarsnap', 'sugary', 'suggest', 'suggested', 'suggestion', 'suggestions', 'suit', 'suitable', 'suitably', 'sukiyaki', 'sultana', 'sultanas', 'sum', 'sumac', 'summer', 'sumptuously', 'sun', 'sunbeam', 'sunblush', 'sundae', 'sundaes', 'sunday', 'sundried', 'sunflower', 'sunk', 'sunlight', 'sunny', 'super', 'superb', 'superlight', 'supermarket', 'supermarkets', 'supervise', 'supervised', 'supple', 'support', 'supports', 'supposed', 'supreme', 'supremes', 'sure', 'surface', 'surfaces', 'surplus', 'surprise', 'surround', 'surrounded', 'surrounding', 'sushi', 'suspend', 'suspended', 'suspending', 'suspension', 'suspicious', 'sussex', 'swahili', 'swamp', 'swan', 'swap', 'swapping', 'sweat', 'sweated', 'sweating', 'swede', 'swedish', 'sweep', 'sweet', 'sweetbread', 'sweetbreads', 'sweetcorn', 'sweeten', 'sweetened', 'sweetener', 'sweetening', 'sweeter', 'sweetness', 'sweets', 'swell', 'swells', 'swift', 'swiftly', 'swill', 'swimming', 'swipe', 'swirl', 'swirled', 'swirling', 'swirls', 'swirly', 'swish', 'swishing', 'swiss', 'switch', 'switched', 'switching', 'swivel', 'swizzle', 'swollen', 'swordfish', 'syllabub', 'syllabubs', 'symbolises', 'symmetrical', 'syringe', 'syrup', 'syrups', 'syrupy', 'szechuan', 'tab', 'tabasco', 'tabblouah', 'tabbouleh', 'tabespoon', 'table', 'tablecloth', 'tablemat', 'tablepsoon', 'tablespoon', 'tablespoonful', 'tablespoonfuls', 'tablespoons', 'tablespoonsful', 'tablets', 'tablier', 'tabliers', 'tablspoons', 'tack', 'tacked', 'tacky', 'taco', 'tacos', 'tact', 'tag', 'tagilatelle', 'tagine', 'tagliarini', 'tagliatelle', 'taglierini', 'tags', 'tahdig', 'tahigurt', 'tahini', 'tahoon', 'tail', 'tailed', 'tailpieces', 'tails', 'taint', 'taiwanese', 'take', 'taken', 'takes', 'taking', 'tale', 'taleggio', 'tall', 'tallegio', 'tally', 'tamale', 'tamales', 'tamari', 'tamarind', 'tamarinds', 'tammy', 'tandoor', 'tandoori', 'tang', 'tangerine', 'tangerines', 'tangle', 'tangled', 'tangy', 'tannins', 'tap', 'tapas', 'tape', 'tapenade', 'taper', 'tapered', 'tapering', 'tapioca', 'tapped', 'tapping', 'taquitos', 'tar', 'taramasalata', 'tarator', 'targets', 'tarka', 'tarpaulin', 'tarragon', 'tarst', 'tart', 'tartar', 'tartare', 'tarte', 'tartes', 'tartiflette', 'tartlet', 'tartlets', 'tarts', 'task', 'taste', 'tastes', 'tasting', 'tasty', 'tatin', 'tatins', 'tattie', 'tatties', 'taught', 'taut', 'tava', 'tawa', 'tb', 'tbsp', 'tbspof', 'te', 'tea', 'teabag', 'teabags', 'teacake', 'teacakes', 'teacloth', 'teacup', 'teacups', 'teal', 'teapot', 'tear', 'teardrop', 'tearing', 'tears', 'tease', 'teasing', 'teaspoon', 'teaspoonful', 'teaspoonfuls', 'teaspoons', 'teaspoonsful', 'teau', 'technique', 'tedious', 'teeth', 'tell', 'temp', 'temper', 'temperature', 'temperatures', 'tempered', 'template', 'templates', 'temptation', 'tempted', 'tempting', 'temptingly', 'tempura', 'ten', 'tend', 'tendency', 'tender', 'tenderer', 'tenderise', 'tenderiser', 'tenderize', 'tenderizer', 'tenderloin', 'tenderloins', 'tenderness', 'tenderstem', 'tending', 'tendon', 'tendons', 'tends', 'tennis', 'tension', 'tent', 'tentacle', 'tentacles', 'tepid', 'tequila', 'teriyaki', 'terms', 'terracotta', 'terrine', 'terrines', 'tes', 'test', 'tested', 'tester', 'testing', 'texture', 'textured', 'textures', 'th', 'thai', 'thanks', 'thaw', 'thawed', 'thecm', 'thee', 'thelemon', 'therapeutic', 'thereabouts', 'thereby', 'therefore', 'thermal', 'thermidor', 'thermo', 'thermometer', 'thermomix', 'thermostat', 'thick', 'thicken', 'thickened', 'thickeness', 'thickening', 'thickenremove', 'thickens', 'thicker', 'thickest', 'thickish', 'thickly', 'thickness', 'thicktest', 'thigh', 'thighs', 'thin', 'thing', 'things', 'think', 'thinly', 'thinned', 'thinner', 'thinnest', 'thinning', 'thins', 'third', 'thirds', 'thirty', 'thise', 'thmye', 'thorough', 'thoroughly', 'though', 'thousands', 'thr', 'thread', 'threaded', 'threading', 'threads', 'threatens', 'three', 'threes', 'throat', 'throughly', 'throughout', 'throw', 'throwing', 'ths', 'thumb', 'thumbnail', 'thumbs', 'thus', 'thyme', 'tia', 'tian', 'tians', 'tidy', 'tie', 'tied', 'tiede', 'tielle', 'tier', 'tiered', 'tiers', 'tiger', 'tight', 'tighten', 'tightens', 'tighter', 'tightly', 'tightness', 'tikka', 'til', 'tilapia', 'tile', 'tiles', 'till', 'tilt', 'tilting', 'timbale', 'timbales', 'time', 'timer', 'timers', 'times', 'timetable', 'timing', 'timings', 'tin', 'tines', 'tinge', 'tinged', 'tinges', 'tiniest', 'tinned', 'tins', 'tiny', 'tip', 'tipped', 'tipping', 'tips', 'tiramisu', 'tired', 'tiss', 'tisserie', 'tissi', 'tissue', 'toad', 'toast', 'toasted', 'toaster', 'toastie', 'toasting', 'toasts', 'toasty', 'toblerone', 'toc', 'toddy', 'toe', 'toffee', 'tofu', 'togarashi', 'together', 'togetherin', 'togetherto', 'toil', 'toise', 'tomaotes', 'tomatillo', 'tomatillos', 'tomato', 'tomatoes', 'tombstones', 'tomoatoes', 'ton', 'tone', 'tongs', 'tongue', 'tongues', 'tonic', 'tonkatsu', 'tonnato', 'tons', 'toohpick', 'took', 'tool', 'tools', 'tooth', 'toothbrush', 'toothpaste', 'toothpick', 'toothpicks', 'top', 'topof', 'topped', 'topping', 'toppings', 'toppling', 'tops', 'topside', 'torch', 'torched', 'torching', 'torn', 'torpedo', 'torpid', 'torta', 'torte', 'tortellini', 'tortilla', 'tortillas', 'toss', 'tossed', 'tossing', 'total', 'totalling', 'totally', 'touch', 'touched', 'touches', 'touching', 'tough', 'toughen', 'toughest', 'toulouse', 'tournedos', 'tout', 'toward', 'towards', 'towel', 'towels', 'tower', 'towers', 'towl', 'toy', 'toyour', 'trace', 'traces', 'tract', 'trademark', 'traditional', 'traditionally', 'trail', 'trailed', 'trailing', 'trails', 'tranfer', 'transfer', 'transferred', 'transferring', 'transform', 'translucent', 'transluscent', 'transparent', 'transport', 'transporting', 'trap', 'trapped', 'trapping', 'traps', 'trasnfer', 'tray', 'traybake', 'trays', 'treacle', 'treacly', 'treat', 'treating', 'trebled', 'tree', 'trees', 'trembles', 'trembling', 'trench', 'trencher', 'trhe', 'triangle', 'triangles', 'triangular', 'trick', 'trickle', 'trickling', 'tricky', 'tried', 'trifle', 'trim', 'trimmed', 'trimming', 'trimmings', 'trinity', 'tripe', 'triple', 'tripled', 'tripolium', 'triumphantly', 'trivet', 'trompets', 'trompette', 'trompettes', 'tropical', 'trotter', 'trotters', 'trouble', 'trout', 'trouts', 'true', 'truffle', 'truffled', 'truffles', 'trun', 'truning', 'trunk', 'trunks', 'truss', 'trussed', 'trussing', 'trust', 'try', 'trying', 'tsp', 'tsps', 'tuaca', 'tub', 'tube', 'tubes', 'tubs', 'tuck', 'tucked', 'tucking', 'tug', 'tugging', 'tuile', 'tuiles', 'tuille', 'tuilles', 'tumbet', 'tumble', 'tumbler', 'tumblers', 'tumbling', 'tumeric', 'tuna', 'tupperware', 'tur', 'turbot', 'tureen', 'turkey', 'turkish', 'turmeric', 'turn', 'turned', 'turning', 'turnip', 'turnips', 'turnover', 'turnovers', 'turns', 'turntable', 'turquoise', 'turskish', 'tuscan', 'tutti', 'twang', 'tweak', 'tweezers', 'twelve', 'twenty', 'twi', 'twice', 'twigs', 'twine', 'twinkle', 'twirl', 'twist', 'twisted', 'twisting', 'twists', 'twizzle', 'two', 'twofold', 'twominutes', 'twothirds', 'txistorra', 'tying', 'type', 'types', 'tzatsiki', 'tzatziki', 'tzle', 'udon', 'uffy', 'ultimate', 'ultimately', 'ultra', 'umani', 'umbrella', 'umeboshi', 'un', 'unable', 'unattended', 'unatttended', 'unblended', 'unbroken', 'unbuttered', 'unclip', 'uncoloured', 'unconscious', 'uncooked', 'uncover', 'uncovered', 'uncovering', 'unctuous', 'uncurl', 'uncut', 'undecorated', 'undeneath', 'undercook', 'undercooked', 'undercooking', 'undercrust', 'underdone', 'underground', 'undermix', 'underneath', 'underside', 'undersides', 'understated', 'underwater', 'underway', 'undissolved', 'undisturbed', 'undo', 'undone', 'uneaten', 'uneven', 'unevenly', 'unfilled', 'unfold', 'unfrozen', 'unfurl', 'unglazed', 'ungreased', 'uniced', 'uniform', 'union', 'unit', 'unite', 'unitl', 'unity', 'unless', 'unlined', 'unmalleable', 'unmanageable', 'unmanageably', 'unmarinaded', 'unmarked', 'unmelted', 'unmixed', 'unmould', 'unopened', 'unpeel', 'unpeeled', 'unpromising', 'unravel', 'unravelling', 'unroll', 'unrolled', 'unsalted', 'unscrunch', 'unset', 'unsightly', 'unskinned', 'unsmoked', 'unsoaked', 'unspring', 'unstick', 'unstrained', 'unsure', 'unti', 'untidily', 'untie', 'untill', 'untilt', 'untilthe', 'unto', 'untoasted', 'untouched', 'untreated', 'untrimmed', 'untruss', 'unusable', 'unused', 'unvarnished', 'unwanted', 'unwashed', 'unwelcome', 'unwrap', 'unwrapped', 'unwrapping', 'unyielding', 'upend', 'uplifting', 'upon', 'upper', 'uppermost', 'upright', 'upside', 'upturn', 'upturned', 'upward', 'upwards', 'urad', 'urge', 'usable', 'use', 'used', 'useful', 'using', 'usual', 'usually', 'utility', 'utons', 'utter', 'vac', 'vacant', 'vacherin', 'vacuum', 'vadouvan', 'vaguest', 'vanilla', 'vanish', 'variation', 'variations', 'varies', 'varieties', 'variety', 'various', 'vark', 'vary', 'varying', 'vase', 'vat', 'veal', 'veg', 'vegan', 'vegetable', 'vegetables', 'vegetarian', 'vegetarians', 'veggie', 'veggies', 'vegtables', 'vein', 'veins', 'velodrome', 'velout', 'veloute', 'velvety', 'venison', 'vent', 'ventilate', 'ventilated', 'ventilation', 'ventreche', 'vents', 'venue', 'verbena', 'verde', 'verge', 'verjus', 'vermicelli', 'vermout', 'vermouth', 'versa', 'versatile', 'version', 'verte', 'vertical', 'vertically', 'vetetable', 'vibrant', 'vibrantly', 'vice', 'vichy', 'victoria', 'vide', 'video', 'vie', 'vierge', 'vietnamese', 'vignarola', 'vigorous', 'vigorously', 'vin', 'vinaigrette', 'vinaigrettes', 'vindaloo', 'vine', 'vinegar', 'vinegars', 'vinegary', 'vines', 'vingar', 'viniagrette', 'vinyl', 'viola', 'violet', 'violets', 'virgin', 'virtually', 'viscous', 'visible', 'visibly', 'visual', 'visually', 'vital', 'vitamins', 'vivid', 'vocer', 'vodka', 'vokda', 'vol', 'volcanic', 'volcano', 'volume', 'voluminous', 'voluptuous', 'voluptuousness', 'vortex', 'vre', 'vreate', 'wad', 'wafer', 'wafers', 'waffle', 'waffles', 'wafting', 'wagyu', 'wait', 'waiting', 'wakame', 'wakami', 'waldorf', 'walk', 'wall', 'wallpaper', 'walls', 'walnut', 'walnuts', 'wamed', 'want', 'wanted', 'wanton', 'wantons', 'warm', 'warmed', 'warming', 'warmth', 'warn', 'warned', 'wasabi', 'wash', 'washed', 'washing', 'washy', 'waste', 'watch', 'watching', 'wate', 'water', 'watera', 'watercress', 'watered', 'wateriness', 'waterlogged', 'watermelon', 'waterproof', 'watertight', 'watery', 'watts', 'wave', 'wavy', 'wax', 'waxed', 'waxy', 'way', 'ways', 'weak', 'weapon', 'wear', 'wearing', 'weave', 'wedge', 'wedges', 'wedging', 'wee', 'week', 'weekly', 'weeks', 'weep', 'weigh', 'weighed', 'weighing', 'weighs', 'weight', 'weighted', 'weighting', 'weights', 'weighty', 'welcome', 'well', 'wellington', 'wellingtons', 'wells', 'welly', 'welsh', 'wensleydale', 'went', 'westie', 'wet', 'wetted', 'wetting', 'whack', 'whatever', 'whatsoever', 'wheat', 'wheaten', 'wheatgerm', 'wheel', 'whelk', 'whelks', 'whenever', 'whereas', 'whether', 'whey', 'whichever', 'whilse', 'whilst', 'whip', 'whipped', 'whipper', 'whipping', 'whips', 'whirl', 'whirlpool', 'whish', 'whisk', 'whisked', 'whiskers', 'whiskey', 'whisking', 'whisks', 'whisky', 'white', 'whitebait', 'whitecurrants', 'whiter', 'whites', 'whitesinto', 'whiting', 'whitish', 'whiz', 'whizz', 'whizzing', 'whole', 'wholegrain', 'wholemeal', 'wholesome', 'wholewheat', 'whoopie', 'whoopies', 'whoosh', 'whose', 'wide', 'widely', 'widened', 'wider', 'widest', 'width', 'widthways', 'wiggle', 'wiggly', 'wil', 'wild', 'willam', 'william', 'willow', 'wilt', 'wilted', 'wilting', 'wilton', 'wilts', 'wind', 'window', 'windowpane', 'windows', 'wine', 'wineglasses', 'wines', 'wing', 'winglet', 'winglets', 'wings', 'winkles', 'winter', 'wipe', 'wiped', 'wiping', 'wire', 'wirerack', 'wish', 'wishbone', 'wishbones', 'wished', 'wishy', 'wispy', 'witchill', 'witha', 'within', 'without', 'witth', 'wobble', 'wobbles', 'wobbling', 'wobbly', 'wodge', 'wok', 'wonderful', 'wonderfully', 'wonders', 'wonton', 'wontons', 'wood', 'wooden', 'woodiest', 'woody', 'worcester', 'worcestershire', 'worchestershire', 'word', 'words', 'work', 'worked', 'working', 'works', 'workspace', 'worksurface', 'worktop', 'worried', 'worries', 'worry', 'worth', 'would', 'woven', 'wrap', 'wrapped', 'wrapper', 'wrappers', 'wrapping', 'wraps', 'wreath', 'wreck', 'wring', 'wrinkle', 'wrinkled', 'wrinkles', 'wrinkly', 'wrist', 'wrists', 'write', 'writing', 'written', 'wrung', 'wth', 'xad', 'xantham', 'xanthan', 'xanthum', 'xaoxing', 'xbin', 'xcm', 'xcs', 'xd', 'xeo', 'xin', 'xml', 'xo', 'xre', 'xs', 'xt', 'xx', 'xxcm', 'xxin', 'yaki', 'yakitori', 'yam', 'yama', 'yams', 'yank', 'yarg', 'year', 'years', 'yeast', 'yeasted', 'yeasty', 'yellow', 'yellowing', 'yellowish', 'yelow', 'yerba', 'yet', 'yield', 'yielded', 'yields', 'yo', 'yoghurt', 'yoghurts', 'yogurt', 'yohurt', 'yold', 'yolk', 'yolks', 'yorkshire', 'yorkshires', 'young', 'yout', 'yucatan', 'yukari', 'yule', 'yum', 'yung', 'yuzu', 'za', 'zaatar', 'zabaglione', 'zag', 'zags', 'zest', 'zested', 'zester', 'zests', 'zesty', 'zig', 'zigzag', 'zingy', 'zip', 'zone', 'zucchini', 'zucchinis', 'zucotto']\n"
     ]
    }
   ],
   "source": [
    "corpus = cld_ins\n",
    "tf_vectorizer = TfidfVectorizer(stop_words=stopwords_nltk)\n",
    "instr_tfidf = tf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(tf_vectorizer.get_feature_names()) \n",
    "\n",
    "#too many repetitions, need stemming and lemmatization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:19.490333Z",
     "start_time": "2019-12-09T00:32:14.110737Z"
    }
   },
   "outputs": [],
   "source": [
    "nmf_10 = NMF(10) \n",
    "instr_tf_nmf_10 = nmf_10.fit_transform(instr_tfidf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:19.540600Z",
     "start_time": "2019-12-09T00:32:19.494603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "add, heat, minutes, cook, pan, stir, fry, simmer, rice, sauce, stock, boil, bring, garlic, onion, water, medium, oil, frying, saucepan\n",
      "\n",
      "Topic  1\n",
      "sugar, whisk, cream, mixture, chocolate, egg, place, bowl, milk, oven, pour, whites, vanilla, custard, heat, peaks, butter, fold, meringue, yolks\n",
      "\n",
      "Topic  2\n",
      "pastry, pie, tart, filling, oven, roll, brush, case, egg, edges, baking, cm, bake, beaten, surface, minutes, top, cut, tin, gas\n",
      "\n",
      "Topic  3\n",
      "place, pepper, freshly, black, ground, olive, season, salt, salad, oil, dressing, drizzle, oven, plate, pan, onto, cheese, bowl, serve, salmon\n",
      "\n",
      "Topic  4\n",
      "chicken, breast, breasts, cooked, marinade, pieces, juices, minutes, oven, thighs, skin, clear, sauce, rice, run, place, roast, side, add, roasting\n",
      "\n",
      "Topic  5\n",
      "dough, flour, surface, roll, knead, pasta, baking, floured, work, lightly, tray, yeast, bowl, mix, water, minutes, cm, together, place, oven\n",
      "\n",
      "Topic  6\n",
      "oil, deep, hot, kitchen, batter, unattended, dangerous, caution, drain, paper, slotted, golden, leave, fryer, brown, flour, fry, dip, fish, sizzles\n",
      "\n",
      "Topic  7\n",
      "pork, oven, belly, meat, apple, cabbage, pan, potatoes, sauce, place, duck, cook, roasting, minutes, skin, gas, hours, remove, heat, roast\n",
      "\n",
      "Topic  8\n",
      "lamb, oven, potatoes, meat, casserole, minutes, roasting, rosemary, pan, cover, roast, chops, dish, gas, vegetables, remove, hours, place, cook, add\n",
      "\n",
      "Topic  9\n",
      "cake, tin, icing, cool, beat, oven, cm, sugar, chocolate, line, cakes, tins, butter, bake, mixture, grease, skewer, eggs, rack, baking\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_10, tf_vectorizer.get_feature_names(), 20) \n",
    "#without stemming/lemmatization, topics dominated by baking instructions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Topic 0 rice stir fry\n",
    "* Topic 1 dessert - cream, chocolate, meringue\n",
    "* Topic 2 dessert - pastry/pie/tart\n",
    "* Topic 3 salmon, cheese and pepper? need more cleaning\n",
    "* Topic 4 chicken\n",
    "* Topic 5 dough, yeast, pasta\n",
    "* Topic 6 battered and fried (fish sticks?)\n",
    "* Topic 7 pork roast\n",
    "* Topic 8 lamb roast\n",
    "* Topic 9 cake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:19.565881Z",
     "start_time": "2019-12-09T00:32:19.556717Z"
    }
   },
   "outputs": [],
   "source": [
    "#more cleaning; \n",
    "#also - only use the recipes with images? purpose is to filter! (might get better topics?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focus on smaller set of recipes\n",
    "Only the 2225 recipes with images, to use as filter for image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:19.678361Z",
     "start_time": "2019-12-09T00:32:19.572660Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chef</th>\n",
       "      <th>chef_id</th>\n",
       "      <th>cooking_time_minutes</th>\n",
       "      <th>description</th>\n",
       "      <th>error</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>instructions</th>\n",
       "      <th>instructions_detailed</th>\n",
       "      <th>photo_url</th>\n",
       "      <th>preparation_time_minutes</th>\n",
       "      <th>program</th>\n",
       "      <th>program_id</th>\n",
       "      <th>serves</th>\n",
       "      <th>time_scraped</th>\n",
       "      <th>title</th>\n",
       "      <th>total_time_minutes</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>James Tanner</td>\n",
       "      <td>james_tanner</td>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[250g/8¾oz plain flour, plus extra for dusting...</td>\n",
       "      <td>[Preheat the oven to 200C/400F/Gas 6., For the...</td>\n",
       "      <td>[{'ingredient': 'plain flour', 'line': '250g/8...</td>\n",
       "      <td>None</td>\n",
       "      <td>30</td>\n",
       "      <td>Ready Steady Cook</td>\n",
       "      <td>b006vcgr</td>\n",
       "      <td>1</td>\n",
       "      <td>1499227761</td>\n",
       "      <td>Ten-minute pizza</td>\n",
       "      <td>40</td>\n",
       "      <td>http://bbc.co.uk/food/recipes/10minutepizza_87314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Mary Berry</td>\n",
       "      <td>mary_berry</td>\n",
       "      <td>0</td>\n",
       "      <td>This is my standby pasta supper as it is so de...</td>\n",
       "      <td>False</td>\n",
       "      <td>[350g/12oz penne pasta, 2 x 80g/3oz packs Parm...</td>\n",
       "      <td>[Cook the pasta in a pan of boiling salted wat...</td>\n",
       "      <td>[{'ingredient': 'pasta', 'line': '350g/12oz pe...</td>\n",
       "      <td>http://ichef.bbci.co.uk/food/ic/food_16x9_608/...</td>\n",
       "      <td>30</td>\n",
       "      <td>Mary Berry Cooks</td>\n",
       "      <td>p01s4q10</td>\n",
       "      <td>6</td>\n",
       "      <td>1499227763</td>\n",
       "      <td>15 minute pasta</td>\n",
       "      <td>30</td>\n",
       "      <td>http://bbc.co.uk/food/recipes/15_minute_pasta_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Mari Williams</td>\n",
       "      <td>mari_williams</td>\n",
       "      <td>0</td>\n",
       "      <td>Simple 3D iced biscuits inspired by Bake Off. ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[200g/7oz unsalted soft butter, 200g/7oz caste...</td>\n",
       "      <td>[To make the basic dough, line a baking tray w...</td>\n",
       "      <td>[{'ingredient': 'butter', 'line': '200g/7oz un...</td>\n",
       "      <td>http://ichef.bbci.co.uk/food/ic/food_16x9_608/...</td>\n",
       "      <td>30</td>\n",
       "      <td>The Great British Bake Off</td>\n",
       "      <td>b013pqnm</td>\n",
       "      <td>0</td>\n",
       "      <td>1499227766</td>\n",
       "      <td>3D biscuits</td>\n",
       "      <td>30</td>\n",
       "      <td>http://bbc.co.uk/food/recipes/3d_biscuits_29555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Justine Pattison</td>\n",
       "      <td>justine_pattison</td>\n",
       "      <td>0</td>\n",
       "      <td>This easy turkey crown recipe is served with s...</td>\n",
       "      <td>False</td>\n",
       "      <td>[2.5kg/5lb 8oz turkey crown (fully thawed if f...</td>\n",
       "      <td>[Preheat the oven to 220C/200C Fan/Gas 7., For...</td>\n",
       "      <td>[{'ingredient': 'turkey', 'line': '2.5kg/5lb 8...</td>\n",
       "      <td>http://ichef.bbci.co.uk/food/ic/food_16x9_608/...</td>\n",
       "      <td>30</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>1499227765</td>\n",
       "      <td>2-hour Christmas dinner</td>\n",
       "      <td>30</td>\n",
       "      <td>http://bbc.co.uk/food/recipes/2_hour_christmas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Ainsley Harriott</td>\n",
       "      <td>ainsley_harriott</td>\n",
       "      <td>0</td>\n",
       "      <td>Rib-eye is one of the most flavoursome cuts of...</td>\n",
       "      <td>False</td>\n",
       "      <td>[2.25kg/5lb rib-eye of beef, boned and rolled,...</td>\n",
       "      <td>[Place the rib-eye of beef into a large non-me...</td>\n",
       "      <td>[{'ingredient': None, 'line': '2.25kg/5lb rib-...</td>\n",
       "      <td>None</td>\n",
       "      <td>120</td>\n",
       "      <td>Great British Food Revival</td>\n",
       "      <td>b016pbs9</td>\n",
       "      <td>8</td>\n",
       "      <td>1499227769</td>\n",
       "      <td>Mustard and thyme crusted rib-eye of beef</td>\n",
       "      <td>120</td>\n",
       "      <td>http://bbc.co.uk/food/recipes/_81487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               chef           chef_id  cooking_time_minutes  \\\n",
       "0      James Tanner      james_tanner                    10   \n",
       "1        Mary Berry        mary_berry                     0   \n",
       "2     Mari Williams     mari_williams                     0   \n",
       "3  Justine Pattison  justine_pattison                     0   \n",
       "4  Ainsley Harriott  ainsley_harriott                     0   \n",
       "\n",
       "                                         description  error  \\\n",
       "0                                                     False   \n",
       "1  This is my standby pasta supper as it is so de...  False   \n",
       "2  Simple 3D iced biscuits inspired by Bake Off. ...  False   \n",
       "3  This easy turkey crown recipe is served with s...  False   \n",
       "4  Rib-eye is one of the most flavoursome cuts of...  False   \n",
       "\n",
       "                                         ingredients  \\\n",
       "0  [250g/8¾oz plain flour, plus extra for dusting...   \n",
       "1  [350g/12oz penne pasta, 2 x 80g/3oz packs Parm...   \n",
       "2  [200g/7oz unsalted soft butter, 200g/7oz caste...   \n",
       "3  [2.5kg/5lb 8oz turkey crown (fully thawed if f...   \n",
       "4  [2.25kg/5lb rib-eye of beef, boned and rolled,...   \n",
       "\n",
       "                                        instructions  \\\n",
       "0  [Preheat the oven to 200C/400F/Gas 6., For the...   \n",
       "1  [Cook the pasta in a pan of boiling salted wat...   \n",
       "2  [To make the basic dough, line a baking tray w...   \n",
       "3  [Preheat the oven to 220C/200C Fan/Gas 7., For...   \n",
       "4  [Place the rib-eye of beef into a large non-me...   \n",
       "\n",
       "                               instructions_detailed  \\\n",
       "0  [{'ingredient': 'plain flour', 'line': '250g/8...   \n",
       "1  [{'ingredient': 'pasta', 'line': '350g/12oz pe...   \n",
       "2  [{'ingredient': 'butter', 'line': '200g/7oz un...   \n",
       "3  [{'ingredient': 'turkey', 'line': '2.5kg/5lb 8...   \n",
       "4  [{'ingredient': None, 'line': '2.25kg/5lb rib-...   \n",
       "\n",
       "                                           photo_url  \\\n",
       "0                                               None   \n",
       "1  http://ichef.bbci.co.uk/food/ic/food_16x9_608/...   \n",
       "2  http://ichef.bbci.co.uk/food/ic/food_16x9_608/...   \n",
       "3  http://ichef.bbci.co.uk/food/ic/food_16x9_608/...   \n",
       "4                                               None   \n",
       "\n",
       "   preparation_time_minutes                     program program_id  serves  \\\n",
       "0                        30           Ready Steady Cook   b006vcgr       1   \n",
       "1                        30            Mary Berry Cooks   p01s4q10       6   \n",
       "2                        30  The Great British Bake Off   b013pqnm       0   \n",
       "3                        30                        None       None       6   \n",
       "4                       120  Great British Food Revival   b016pbs9       8   \n",
       "\n",
       "   time_scraped                                       title  \\\n",
       "0    1499227761                            Ten-minute pizza   \n",
       "1    1499227763                             15 minute pasta   \n",
       "2    1499227766                                3D biscuits    \n",
       "3    1499227765                     2-hour Christmas dinner   \n",
       "4    1499227769  Mustard and thyme crusted rib-eye of beef    \n",
       "\n",
       "   total_time_minutes                                                url  \n",
       "0                  40  http://bbc.co.uk/food/recipes/10minutepizza_87314  \n",
       "1                  30  http://bbc.co.uk/food/recipes/15_minute_pasta_...  \n",
       "2                  30    http://bbc.co.uk/food/recipes/3d_biscuits_29555  \n",
       "3                  30  http://bbc.co.uk/food/recipes/2_hour_christmas...  \n",
       "4                 120               http://bbc.co.uk/food/recipes/_81487  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbcdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### drop rows where selected column is NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:19.687406Z",
     "start_time": "2019-12-09T00:32:19.681038Z"
    }
   },
   "outputs": [],
   "source": [
    "#drop all the rows where photo_url = None (cell is NoneType, not string 'None') - can just use dropna\n",
    "#withphotos = bbcdata[bbcdata['photo_url'] == 'None'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:19.737756Z",
     "start_time": "2019-12-09T00:32:19.694441Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 17)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.dropna(subset=[1]) #drop rows only if column 1 has nan**\n",
    "withphotos = bbcdata.dropna(subset=['photo_url'])\n",
    "withphotos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean and vectorize instructions for 2k recipes with photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:20.959085Z",
     "start_time": "2019-12-09T00:32:19.748234Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       cook the pasta in a pan of boiling salted wa...\n",
       "2       to make the basic dough  line a baking tray ...\n",
       "3       preheat the oven to c c fan gas      for the...\n",
       "7       preheat the oven to c f gas      take a very...\n",
       "18      preheat the oven to c c fan gas      heat g ...\n",
       "Name: instructions, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cld_withphotos=withphotos['instructions'].apply(regex_nodigits)\n",
    "cld_withphotos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:20.973445Z",
     "start_time": "2019-12-09T00:32:20.962935Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltkstopwords = stopwords.words('english')\n",
    "len(nltkstopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:21.035054Z",
     "start_time": "2019-12-09T00:32:20.977712Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltkstopwords.extend(['minutes','add','remove'])\n",
    "len(nltkstopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:22.494776Z",
     "start_time": "2019-12-09T00:32:21.040251Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abd', 'able', 'absolute', 'absolutely', 'absorb', 'absorbed', 'absorbency', 'absorbent', 'absorbs', 'accommodate', 'accompanied', 'accompaniment', 'according', 'accordingly', 'accumulated', 'accurately', 'acetate', 'achieve', 'achieved', 'achieves', 'acid', 'acidity', 'ackee', 'acquire', 'across', 'act', 'action', 'active', 'actual', 'actually', 'added', 'adding', 'addition', 'additional', 'additions', 'adds', 'adhere', 'adheres', 'adjust', 'adjusting', 'aduki', 'adult', 'adults', 'advance', 'advocaat', 'aerated', 'aesthetic', 'affect', 'afraid', 'agar', 'agave', 'agents', 'agitate', 'ahead', 'aim', 'aiming', 'aioli', 'air', 'airily', 'airing', 'airtight', 'airy', 'ajar', 'al', 'alarmed', 'alaska', 'albondigas', 'alcohol', 'alcoholic', 'ale', 'alight', 'alla', 'allow', 'allowed', 'allowing', 'allows', 'allspice', 'almond', 'almonds', 'almost', 'alone', 'along', 'alongside', 'already', 'also', 'alter', 'alternate', 'alternately', 'alternating', 'alternative', 'alternatively', 'although', 'aluminium', 'aluminum', 'always', 'amalgamated', 'amaretti', 'amaretto', 'amber', 'amchur', 'among', 'amongst', 'amoul', 'amount', 'amounts', 'anchovies', 'anchovy', 'angel', 'angelica', 'anglaise', 'angle', 'angles', 'animal', 'anise', 'aniseed', 'another', 'antipasti', 'anything', 'anyway', 'apart', 'apex', 'appear', 'appearance', 'appeared', 'appearing', 'appears', 'appetizing', 'apple', 'apples', 'applicable', 'apply', 'applying', 'apprimately', 'approaching', 'appropriate', 'approx', 'approximately', 'apricot', 'apricots', 'aquavit', 'arancini', 'arbroath', 'arch', 'arched', 'area', 'areas', 'argan', 'arm', 'armagnac', 'arms', 'arnaise', 'aroma', 'aromas', 'aromatic', 'aromatics', 'around', 'arrabbiata', 'arrange', 'arrangement', 'arranging', 'arrowroot', 'arroz', 'artichoke', 'artichokes', 'asafoetida', 'ash', 'aside', 'ask', 'askew', 'asparagus', 'assemble', 'assembled', 'assembling', 'assess', 'asset', 'assist', 'assisted', 'assorted', 'assortment', 'assuming', 'atar', 'atmosphere', 'attach', 'attached', 'attaching', 'attachment', 'attention', 'attractive', 'au', 'aubergine', 'aubergines', 'authentic', 'auto', 'autumn', 'available', 'average', 'avocado', 'avocadoes', 'avocados', 'avoid', 'avoiding', 'away', 'baba', 'babas', 'baby', 'back', 'backbone', 'backs', 'backwards', 'bacon', 'bacteria', 'bad', 'bag', 'bagel', 'bagels', 'bags', 'baguette', 'baguettes', 'baharat', 'bain', 'bake', 'baked', 'bakes', 'bakestone', 'baking', 'baklava', 'balance', 'ball', 'baller', 'balloon', 'balls', 'balm', 'balsamic', 'bamboo', 'banana', 'bananas', 'bands', 'bang', 'bangers', 'bangs', 'banneton', 'banoffee', 'bantam', 'bar', 'barbados', 'barbecue', 'barbecued', 'barbecues', 'barbecuing', 'barbeque', 'barberries', 'bare', 'barely', 'bark', 'barley', 'barnacles', 'barnsley', 'bars', 'base', 'based', 'bases', 'bash', 'bashed', 'bashing', 'basic', 'basil', 'basin', 'basins', 'basis', 'basket', 'basmati', 'bass', 'baste', 'basted', 'basting', 'bat', 'batch', 'batches', 'baton', 'batons', 'batter', 'battered', 'batters', 'bauble', 'baubles', 'bavarois', 'bavette', 'bay', 'bbq', 'beacomes', 'beads', 'beak', 'beaks', 'bean', 'beans', 'beansprouts', 'beany', 'bear', 'beards', 'beat', 'beaten', 'beater', 'beaters', 'beating', 'beautiful', 'beautifully', 'bechamel', 'become', 'becomes', 'becoming', 'bed', 'bee', 'beef', 'beer', 'bees', 'beetroot', 'beetroots', 'begin', 'beginning', 'begins', 'begun', 'behind', 'beige', 'belly', 'bench', 'bend', 'beneath', 'benedict', 'beneficial', 'berries', 'berry', 'beside', 'best', 'better', 'beurre', 'beware', 'beyond', 'bhaji', 'bicarbonate', 'big', 'bigger', 'biggest', 'billowy', 'bin', 'bind', 'binds', 'bird', 'birds', 'biryani', 'biscotti', 'biscuit', 'biscuits', 'bit', 'bite', 'bites', 'bits', 'bitter', 'bitterness', 'bitters', 'black', 'blackberries', 'blackberry', 'blackcurrants', 'blacked', 'blacken', 'blackened', 'blackstrap', 'blade', 'bladed', 'blades', 'blanc', 'blanch', 'blanched', 'blanching', 'blankets', 'blast', 'blasts', 'blend', 'blended', 'blender', 'blending', 'blind', 'blini', 'blinis', 'blip', 'blister', 'blistered', 'blisters', 'blitz', 'blob', 'blobs', 'block', 'blonde', 'blood', 'bloomer', 'blossom', 'blow', 'blowtorch', 'blue', 'blueberries', 'blueberry', 'blunt', 'board', 'boarder', 'boards', 'boat', 'boats', 'bobble', 'bodies', 'body', 'boil', 'boiled', 'boiling', 'boils', 'bok', 'bold', 'bolognese', 'bombay', 'bone', 'bones', 'bonnet', 'book', 'border', 'borlotti', 'bottle', 'bottled', 'bottles', 'bottom', 'bottomed', 'bottoms', 'bought', 'bouillon', 'boulang', 'bouncy', 'bound', 'bouquet', 'bourbon', 'bow', 'bowl', 'bowlful', 'bowls', 'box', 'boxes', 'br', 'brad', 'braeburn', 'braid', 'braided', 'braiding', 'braise', 'braised', 'braising', 'bramble', 'bramley', 'bran', 'branch', 'brandy', 'brava', 'brazil', 'bread', 'breadcrumb', 'breadcrumbs', 'breaded', 'breadfruit', 'breads', 'breadstick', 'breadsticks', 'break', 'breakdown', 'breakfast', 'breaking', 'breaks', 'bream', 'breast', 'breastbone', 'breasts', 'bresaola', 'brew', 'brick', 'brief', 'briefly', 'bright', 'brilliant', 'brim', 'brine', 'bring', 'bringing', 'brings', 'brioche', 'brisk', 'brisket', 'briskly', 'british', 'broad', 'broccoli', 'broken', 'broth', 'brought', 'brown', 'browned', 'brownie', 'brownies', 'browning', 'brownish', 'browns', 'bruise', 'bruised', 'bruises', 'bruising', 'brul', 'bruschetta', 'brush', 'brushed', 'brushing', 'brussels', 'bubble', 'bubbled', 'bubbles', 'bubbling', 'bubbly', 'bucatini', 'buckwheat', 'bud', 'build', 'building', 'built', 'bulb', 'bulbs', 'bulgar', 'bulge', 'bulging', 'bulgur', 'bulked', 'bumps', 'bumpy', 'bun', 'bunch', 'bundle', 'bundt', 'bunnies', 'buns', 'bunting', 'burger', 'burgers', 'burn', 'burned', 'burning', 'burnished', 'burns', 'burnt', 'burrito', 'burritos', 'burst', 'bursts', 'burying', 'business', 'butcher', 'butter', 'butterbean', 'butterbeans', 'buttercream', 'buttered', 'butterfly', 'butterflying', 'buttermilk', 'butternut', 'butters', 'butterscotch', 'buttery', 'button', 'buttons', 'buy', 'cabbage', 'cabbages', 'cabernet', 'cacao', 'cajun', 'cake', 'cakes', 'cakey', 'calamari', 'calasparra', 'calculate', 'calculating', 'calf', 'called', 'calvados', 'camomile', 'camp', 'canap', 'candied', 'candle', 'candles', 'candy', 'cane', 'canelle', 'canned', 'cannellini', 'cannelloni', 'cannot', 'canoes', 'cans', 'cap', 'capacity', 'caper', 'caperberries', 'capers', 'caponata', 'caps', 'capturing', 'caramel', 'caramelise', 'caramelised', 'caramelises', 'caramelising', 'caramelize', 'caramelized', 'caramels', 'carameslised', 'caraway', 'carcass', 'carcasses', 'carcinogenic', 'card', 'cardamom', 'care', 'careful', 'carefully', 'carmelised', 'carne', 'carrier', 'carrot', 'carrots', 'carry', 'carrying', 'cartouche', 'carve', 'carved', 'carving', 'case', 'cases', 'cashew', 'cashews', 'casserole', 'cassia', 'cassis', 'cassoulet', 'cast', 'caster', 'catch', 'catches', 'catching', 'cats', 'caught', 'cauliflower', 'cauliflowers', 'cause', 'caused', 'caution', 'cautious', 'caviar', 'cavity', 'cavolo', 'cayenne', 'cazuela', 'celeriac', 'celery', 'cellophane', 'celtic', 'cement', 'cent', 'center', 'centimetre', 'centimetres', 'central', 'centre', 'centres', 'ceramic', 'cereal', 'certain', 'certainly', 'ceviche', 'chaat', 'chaffing', 'chair', 'chamel', 'champ', 'champagne', 'chana', 'chance', 'change', 'changed', 'changes', 'changing', 'channel', 'chanterelle', 'chantilly', 'chapatis', 'chapatti', 'chapattis', 'char', 'charcoal', 'chard', 'chargrill', 'chargrilled', 'charlotte', 'charm', 'charred', 'chars', 'che', 'cheat', 'check', 'checked', 'checker', 'checking', 'cheddar', 'cheek', 'cheeks', 'cheerful', 'cheese', 'cheesecake', 'cheesecakes', 'cheeses', 'cheesy', 'chef', 'chefs', 'chermoula', 'cherries', 'cherry', 'chervil', 'chestnut', 'chestnuts', 'chewy', 'chia', 'chick', 'chicken', 'chickens', 'chickpea', 'chickpeas', 'chicks', 'chicory', 'child', 'children', 'chilies', 'chill', 'chilled', 'chilli', 'chillies', 'chilling', 'chills', 'chimichurri', 'chimicurri', 'chimney', 'china', 'chinese', 'chip', 'chipattis', 'chipolata', 'chipolatas', 'chipotle', 'chips', 'chive', 'chives', 'chocolat', 'chocolate', 'chocolates', 'chocolatey', 'choi', 'choice', 'chokes', 'choking', 'choose', 'chop', 'chopped', 'chopping', 'chops', 'chopstick', 'chopsticks', 'chorizo', 'chosen', 'choux', 'chow', 'chowder', 'choy', 'christmas', 'chuck', 'chunk', 'chunks', 'chunky', 'churn', 'churning', 'churro', 'churros', 'chutney', 'chutneys', 'ciabatta', 'cider', 'cigar', 'cigars', 'cinnamon', 'circle', 'circles', 'circular', 'circumference', 'citric', 'citron', 'citrus', 'clair', 'clairs', 'clam', 'clamp', 'clams', 'clapshot', 'clarified', 'classic', 'claw', 'claws', 'clean', 'cleaned', 'cleaner', 'cleaning', 'cleanly', 'clear', 'clearly', 'cleaver', 'clementine', 'clever', 'cling', 'clingfilm', 'clinging', 'clings', 'clip', 'clips', 'clock', 'clockwise', 'close', 'closed', 'closely', 'closer', 'closest', 'closing', 'cloth', 'clotted', 'cloud', 'clouds', 'cloudy', 'clove', 'clover', 'cloves', 'clump', 'clumping', 'clumps', 'clumpy', 'cluster', 'clusters', 'cm', 'cmx', 'cmxcm', 'co', 'coals', 'coarse', 'coarsely', 'coarser', 'coat', 'coated', 'coating', 'coats', 'coax', 'cob', 'cobbler', 'cobs', 'cock', 'cocktail', 'cocktails', 'cocoa', 'coconut', 'cod', 'coffee', 'cognac', 'coil', 'coiling', 'coin', 'coins', 'cointreau', 'cola', 'colander', 'colcannon', 'cold', 'coleslaw', 'coley', 'collapse', 'collapsed', 'collapsing', 'collar', 'collarette', 'collars', 'collect', 'collected', 'collecting', 'colour', 'coloured', 'colouring', 'colourings', 'colours', 'columns', 'coluring', 'combination', 'combine', 'combined', 'come', 'comes', 'coming', 'comp', 'compact', 'complement', 'complete', 'completed', 'completely', 'completes', 'completing', 'components', 'compote', 'compress', 'con', 'conceal', 'concentrate', 'concentric', 'concertina', 'condensed', 'condition', 'cone', 'confident', 'confit', 'conical', 'connect', 'connected', 'conserve', 'considerable', 'considerably', 'consistency', 'consomm', 'constantly', 'construct', 'contact', 'contain', 'container', 'containers', 'containing', 'content', 'contents', 'continual', 'continually', 'continue', 'continuing', 'continuously', 'contiuously', 'contrast', 'contrasting', 'convenient', 'conventional', 'convincing', 'cook', 'cooked', 'cooker', 'cookie', 'cookies', 'cooking', 'cooks', 'cool', 'cooled', 'cooler', 'coolest', 'cooling', 'cools', 'copper', 'coq', 'coral', 'cordial', 'core', 'cored', 'corer', 'cores', 'coriander', 'corkscrew', 'corn', 'cornbread', 'corned', 'corner', 'corners', 'cornflour', 'cornichons', 'cornish', 'cornmeal', 'correct', 'correctly', 'cos', 'cotta', 'cottage', 'couche', 'could', 'coulibiac', 'coulis', 'counter', 'country', 'couple', 'courgette', 'courgettes', 'courgetti', 'couronne', 'course', 'court', 'cous', 'couscous', 'cover', 'covered', 'covering', 'covers', 'cr', 'crab', 'crabcakes', 'crabmeat', 'crack', 'cracked', 'cracker', 'crackers', 'cracking', 'crackle', 'crackled', 'crackling', 'cracks', 'craft', 'cramped', 'cranachan', 'cranachans', 'cranberries', 'cranberry', 'crannies', 'cranny', 'crazy', 'cream', 'creamed', 'creamier', 'creamily', 'creaming', 'creams', 'creamy', 'crease', 'create', 'created', 'creates', 'creating', 'creatures', 'credit', 'creme', 'creole', 'crescent', 'cress', 'crimp', 'crimped', 'crimping', 'crinkle', 'crinkled', 'crinkles', 'crisp', 'crisped', 'crisper', 'crispier', 'crispiness', 'crispness', 'crisps', 'crispy', 'criss', 'cro', 'croquette', 'croquettes', 'cross', 'crossed', 'crosses', 'crosshatch', 'crossing', 'crossways', 'crosswise', 'crostini', 'croutons', 'crowd', 'crowdie', 'crowding', 'crown', 'crumb', 'crumbed', 'crumble', 'crumbled', 'crumbling', 'crumbly', 'crumbs', 'crumpet', 'crumpets', 'crumple', 'crumpled', 'crunch', 'crunchy', 'crush', 'crushed', 'crust', 'crusted', 'crusting', 'crusts', 'crusty', 'crystallise', 'crystallised', 'crystals', 'cube', 'cubed', 'cubes', 'cubesafter', 'cucumber', 'cucumbers', 'cumberland', 'cumin', 'cup', 'cupboard', 'cupboards', 'cupcake', 'cupcakes', 'cupful', 'cupid', 'cups', 'curd', 'curdle', 'curdled', 'curdles', 'curdling', 'cure', 'cured', 'curl', 'curled', 'curling', 'curls', 'curly', 'currant', 'currants', 'curried', 'currry', 'curry', 'curve', 'curved', 'curves', 'curving', 'custard', 'custards', 'cut', 'cutlery', 'cutlet', 'cutlets', 'cuts', 'cutter', 'cutters', 'cutting', 'cycle', 'cyclist', 'cylinder', 'daal', 'dab', 'dabs', 'dacquoise', 'daily', 'dairy', 'daisies', 'daisy', 'dal', 'damaged', 'damp', 'damped', 'dampen', 'dampfnudel', 'dampfnudels', 'damsons', 'dandelion', 'dangerous', 'dangle', 'dare', 'dariole', 'dark', 'darken', 'darkened', 'darkens', 'darker', 'dash', 'dashes', 'dashi', 'date', 'dates', 'dauphinoise', 'day', 'days', 'de', 'dead', 'deal', 'debeard', 'debris', 'decant', 'decent', 'decorate', 'decorated', 'decorating', 'decoration', 'decorations', 'decorative', 'decoratively', 'decrease', 'decreasing', 'deduct', 'deep', 'deeper', 'deeply', 'defined', 'deflates', 'defrost', 'defrosted', 'deglaze', 'deglazed', 'degree', 'degrees', 'delectably', 'delicate', 'delicious', 'deliciously', 'delight', 'demerara', 'demerera', 'dense', 'dent', 'dente', 'depend', 'depending', 'depends', 'depressions', 'depth', 'described', 'description', 'deseed', 'desiccated', 'design', 'designs', 'desire', 'desired', 'dessert', 'desserts', 'dessertspoon', 'dessertspoonful', 'dessertspoonfuls', 'dessertspoons', 'details', 'devein', 'develop', 'dhal', 'diagonal', 'diagonally', 'diameter', 'diamond', 'diamonds', 'dice', 'diced', 'dick', 'die', 'died', 'difference', 'different', 'differently', 'difficult', 'diffuser', 'dig', 'digestive', 'digital', 'dijon', 'dill', 'dilute', 'diluted', 'dim', 'diminish', 'dimples', 'dinner', 'dip', 'dippable', 'dipped', 'dipping', 'dips', 'direct', 'direction', 'directions', 'directly', 'dirt', 'disappear', 'disappeared', 'disc', 'discard', 'discarded', 'discarding', 'disco', 'discolouring', 'discs', 'disguise', 'dish', 'dishes', 'dishwasher', 'disintegrate', 'disk', 'disks', 'disperse', 'displacing', 'disposable', 'dissolve', 'dissolved', 'dissolves', 'distance', 'distinctive', 'distribute', 'distributed', 'distribution', 'disturbing', 'dive', 'divide', 'dividing', 'division', 'diy', 'docker', 'dolcelatte', 'dollop', 'dollops', 'dolly', 'dome', 'domed', 'domes', 'domestic', 'dominating', 'done', 'doneness', 'door', 'doors', 'dot', 'dots', 'dotting', 'double', 'doubled', 'doubles', 'doubt', 'dough', 'doughnut', 'doughnuts', 'doughs', 'doughy', 'dove', 'dowel', 'doweling', 'dowelling', 'downward', 'downwards', 'dozen', 'drafts', 'drag', 'dragging', 'drain', 'drained', 'draining', 'drambuie', 'drape', 'drapes', 'draw', 'drawing', 'drawn', 'draws', 'dream', 'dredge', 'dress', 'dressed', 'dressing', 'dribble', 'dribbles', 'dried', 'dries', 'drink', 'drinking', 'drip', 'dripped', 'dripping', 'drips', 'drive', 'driven', 'drizzle', 'drizzled', 'drizzles', 'drizzling', 'drop', 'dropped', 'dropping', 'drops', 'drum', 'drumstick', 'drumsticks', 'dry', 'drying', 'dublin', 'duck', 'due', 'dukkah', 'dulce', 'dull', 'dump', 'dumpling', 'dumplings', 'dunk', 'duration', 'dusky', 'dust', 'dusted', 'dusting', 'dutch', 'duxelle', 'ear', 'earl', 'earlier', 'early', 'ears', 'earthenware', 'ease', 'easier', 'easiest', 'easily', 'easing', 'easter', 'easy', 'eat', 'eaten', 'eating', 'eccles', 'eclairs', 'ed', 'edamame', 'edge', 'edged', 'edges', 'edible', 'effect', 'effort', 'egg', 'eggs', 'eggwash', 'eggy', 'eight', 'eighth', 'eighths', 'either', 'el', 'elaborate', 'elapsed', 'elastic', 'elasticated', 'elbow', 'elderflower', 'elderflowers', 'electric', 'electrical', 'elegant', 'eleven', 'eliminate', 'else', 'elusive', 'emanates', 'embers', 'emboss', 'emerged', 'emmental', 'empanadas', 'emphasise', 'empty', 'emulsified', 'emulsifies', 'emulsify', 'emulsion', 'en', 'enable', 'enables', 'enamel', 'encase', 'encased', 'enchiladas', 'enclose', 'enclosed', 'enclosing', 'encourage', 'encouraging', 'end', 'ended', 'ending', 'ends', 'english', 'enjoy', 'enlarge', 'enormous', 'enough', 'ensure', 'ensures', 'ensuring', 'entire', 'entirely', 'envelope', 'equal', 'equally', 'equivalent', 'es', 'escalope', 'escalopes', 'escape', 'escapes', 'escaping', 'especially', 'espresso', 'essence', 'essential', 'evaporate', 'evaporated', 'evaporates', 'even', 'evening', 'evenly', 'eventually', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'exact', 'exactly', 'examine', 'excep', 'except', 'exception', 'excess', 'excessive', 'expand', 'expanded', 'expansion', 'expect', 'experience', 'explode', 'expose', 'exposed', 'exposing', 'exterior', 'extra', 'extract', 'extras', 'extreme', 'extremely', 'exuded', 'eye', 'eyebrows', 'eyes', 'fabric', 'fabulously', 'face', 'faces', 'facing', 'factor', 'fade', 'fagioli', 'fail', 'faintly', 'fair', 'fairly', 'fairy', 'falafel', 'falafels', 'fall', 'fallen', 'falling', 'falls', 'family', 'fan', 'fancies', 'fancy', 'fanned', 'far', 'farthest', 'fashions', 'fast', 'fasten', 'faster', 'fastest', 'fat', 'fats', 'fatty', 'favourite', 'fear', 'feather', 'feathered', 'features', 'feed', 'feeder', 'feeding', 'feel', 'feeling', 'feels', 'feet', 'feijoada', 'fence', 'fennel', 'fenugreek', 'ferment', 'fermented', 'ferociously', 'festive', 'feta', 'fettuccine', 'fgas', 'fibres', 'fibrous', 'fideu', 'fifteen', 'fifth', 'fifty', 'fig', 'figs', 'figurines', 'filing', 'fill', 'filled', 'fillet', 'fillets', 'filling', 'fillings', 'fills', 'film', 'filo', 'filter', 'final', 'finally', 'find', 'finds', 'fine', 'finely', 'finest', 'finger', 'fingered', 'fingernails', 'fingerprints', 'fingers', 'fingertip', 'fingertips', 'finish', 'finished', 'finishing', 'fir', 'firm', 'firmed', 'firmer', 'firmly', 'first', 'firstly', 'fish', 'fishcake', 'fishcakes', 'fishmonger', 'fishy', 'fit', 'fita', 'fits', 'fitted', 'fitting', 'five', 'fix', 'fixed', 'fixing', 'fixture', 'fizz', 'fizzes', 'fl', 'flageolet', 'flags', 'flake', 'flaked', 'flakes', 'flakiness', 'flaky', 'flamb', 'flame', 'flameproof', 'flames', 'flaming', 'flammable', 'flan', 'flaounes', 'flap', 'flapjack', 'flapjacks', 'flaps', 'flare', 'flash', 'flat', 'flatbread', 'flatbreads', 'flatleaf', 'flats', 'flatten', 'flattened', 'flattening', 'flattish', 'flavour', 'flavoured', 'flavouring', 'flavourings', 'flavours', 'flavoursome', 'flax', 'fleeces', 'flesh', 'fleshy', 'flexible', 'flick', 'flip', 'flipping', 'float', 'floated', 'floating', 'floats', 'flood', 'flooding', 'floor', 'floppy', 'florentine', 'florentines', 'florets', 'flour', 'floured', 'flouring', 'flours', 'floury', 'flower', 'flowering', 'flowers', 'flowing', 'floz', 'fluff', 'fluffier', 'fluffing', 'fluffly', 'fluffy', 'flute', 'fluted', 'flutes', 'flying', 'foam', 'foaming', 'foams', 'foamy', 'focaccia', 'foil', 'fold', 'folded', 'folding', 'folds', 'follow', 'followed', 'following', 'fondant', 'fondants', 'fondue', 'fontina', 'food', 'foods', 'fool', 'foot', 'football', 'force', 'forcemeat', 'fore', 'forefinger', 'forest', 'forget', 'forgiving', 'fork', 'forks', 'form', 'formed', 'forming', 'forms', 'forth', 'forty', 'forward', 'forwards', 'fougasse', 'found', 'foundation', 'four', 'fourteen', 'fourth', 'fr', 'fra', 'fragile', 'fragrance', 'fragrant', 'fraiche', 'frais', 'fraisier', 'frame', 'frames', 'frangipane', 'free', 'freehand', 'freekeh', 'freestanding', 'freezable', 'freeze', 'freezeable', 'freezer', 'freezing', 'french', 'frequently', 'fresh', 'fresher', 'freshly', 'freshness', 'friache', 'fridge', 'fried', 'friendly', 'friends', 'fries', 'frikadeller', 'frilly', 'fringe', 'fritas', 'frittata', 'fritter', 'fritters', 'fromage', 'fronds', 'front', 'frost', 'frosted', 'frosting', 'froth', 'frothing', 'frothy', 'frozen', 'fruit', 'fruits', 'fruity', 'frutti', 'fry', 'fryer', 'frying', 'fu', 'fudge', 'full', 'fully', 'fun', 'funnel', 'furiously', 'furthest', 'fusilli', 'future', 'galangal', 'galette', 'galettes', 'game', 'gammon', 'ganache', 'gap', 'gaps', 'garam', 'garden', 'garlands', 'garlic', 'garlicky', 'garni', 'garnish', 'garnished', 'garnishes', 'garnishing', 'garnsih', 'gas', 'gask', 'gateau', 'gather', 'gathered', 'gathering', 'gathers', 'gazpacho', 'gel', 'gelantine', 'gelatine', 'gem', 'gemolata', 'gems', 'generous', 'generously', 'genoise', 'gentle', 'gentlest', 'gently', 'get', 'gets', 'getting', 'gg', 'ghee', 'gherkin', 'gherkins', 'ghoogras', 'ghosts', 'giblets', 'gift', 'gifts', 'gilded', 'gin', 'ginger', 'gingerbread', 'gingernut', 'gingernuts', 'gingery', 'give', 'given', 'gives', 'giving', 'gizzard', 'gl', 'glac', 'glass', 'glasses', 'glaze', 'glazed', 'glazes', 'glazing', 'glide', 'glistening', 'glitter', 'glittery', 'glorious', 'glory', 'glossy', 'glove', 'gloves', 'glowing', 'glucose', 'glue', 'glug', 'gluten', 'glutens', 'glycerine', 'gnocchi', 'go', 'goat', 'goats', 'goes', 'going', 'goji', 'gold', 'golden', 'golf', 'gone', 'good', 'gooey', 'gooeyness', 'goose', 'gooseberries', 'gooseberry', 'gore', 'goreng', 'gorgeous', 'gorgeously', 'gorgonzola', 'got', 'goug', 'goujons', 'goulash', 'grab', 'gradually', 'grain', 'grains', 'grainy', 'gram', 'grams', 'grand', 'granita', 'granite', 'granola', 'granulated', 'granules', 'grapefruit', 'grapefruits', 'grapes', 'grass', 'grate', 'grated', 'grater', 'graters', 'gratin', 'gratinated', 'grating', 'gratings', 'gravad', 'gravel', 'gravy', 'grease', 'greased', 'greaseproof', 'greasing', 'greasy', 'great', 'greatly', 'greek', 'green', 'greenish', 'greens', 'gremolata', 'grenadine', 'grey', 'gribiche', 'grid', 'griddle', 'griddled', 'grill', 'grilled', 'grilling', 'grind', 'grinder', 'grinding', 'grinds', 'grip', 'gristle', 'grit', 'grittiness', 'gritty', 'grooves', 'ground', 'grounded', 'groundnut', 'grown', 'gruy', 'gruyere', 'guacamole', 'guanciale', 'guard', 'guernsey', 'guests', 'guide', 'guides', 'gum', 'gumbo', 'haddock', 'haggis', 'hair', 'hairy', 'hake', 'half', 'halfway', 'halloumi', 'halloween', 'halt', 'halve', 'halved', 'halves', 'halwa', 'ham', 'hammer', 'hamper', 'hand', 'handful', 'handfuls', 'handheld', 'handle', 'handled', 'handles', 'handling', 'hands', 'handy', 'hang', 'hanging', 'hanout', 'happen', 'happening', 'happens', 'happy', 'hard', 'harden', 'hardened', 'hardening', 'harder', 'haricot', 'harina', 'harissa', 'hash', 'hat', 'hatch', 'hatching', 'hats', 'hazard', 'hazelnut', 'hazelnuts', 'hazy', 'head', 'heading', 'heads', 'heal', 'heaped', 'heaping', 'heaps', 'hear', 'heart', 'hearts', 'heat', 'heated', 'heating', 'heatproof', 'heats', 'heavily', 'heavy', 'heel', 'height', 'held', 'help', 'helped', 'helpful', 'helping', 'helpings', 'helps', 'hemp', 'hen', 'henna', 'hens', 'herb', 'herbal', 'herbed', 'herbs', 'herby', 'herring', 'hexagon', 'hey', 'high', 'highball', 'higher', 'highest', 'highly', 'hillocks', 'hinder', 'hint', 'hiss', 'hit', 'hits', 'hitting', 'hive', 'hob', 'hock', 'hoisin', 'hold', 'holding', 'holds', 'hole', 'holes', 'holey', 'hollandaise', 'hollow', 'hollowed', 'hollows', 'holly', 'hollywood', 'hologram', 'home', 'homemade', 'homogenise', 'honey', 'honeycomb', 'honeycombe', 'honeyed', 'hook', 'hoop', 'hoops', 'hopeless', 'horizonatally', 'horizontal', 'horizontally', 'horn', 'horns', 'horseradish', 'hot', 'hotplate', 'hotpot', 'hotter', 'hottest', 'hour', 'hours', 'house', 'however', 'hr', 'hrs', 'huge', 'hull', 'hulled', 'humanly', 'humidity', 'hummus', 'hundreds', 'hunza', 'hurry', 'husk', 'husks', 'iberico', 'ice', 'iced', 'iche', 'icicles', 'icing', 'icings', 'idea', 'ideal', 'ideally', 'ideas', 'identified', 'ie', 'ignore', 'ilm', 'image', 'imagination', 'immediatedly', 'immediately', 'imperative', 'important', 'importantly', 'impression', 'impressive', 'improve', 'impurities', 'inactive', 'inc', 'inch', 'inches', 'incision', 'incisions', 'include', 'including', 'incorporate', 'incorporated', 'incorporating', 'increase', 'increased', 'increasing', 'indent', 'indentation', 'indentations', 'index', 'indian', 'indicates', 'indicating', 'individual', 'individually', 'inflate', 'inform', 'informal', 'information', 'infrequently', 'infuse', 'infused', 'ingredient', 'ingredients', 'initial', 'initially', 'ink', 'inner', 'insert', 'inserted', 'inserting', 'inside', 'insides', 'insist', 'instant', 'instead', 'instruction', 'instructions', 'intact', 'intend', 'intended', 'intense', 'intensify', 'interior', 'interleave', 'internal', 'internet', 'interspersing', 'intervals', 'intricate', 'intricately', 'introduce', 'invert', 'inverted', 'invisible', 'inwards', 'inwide', 'inxin', 'inzimino', 'irish', 'iron', 'irregular', 'ish', 'island', 'isremoved', 'italian', 'ivory', 'jack', 'jackets', 'jade', 'jaffa', 'jaggery', 'jalapeno', 'jalfrezi', 'jam', 'jammy', 'japanese', 'jar', 'jars', 'jasmine', 'jellied', 'jellies', 'jellified', 'jelly', 'jerk', 'jerusalem', 'jigsaw', 'job', 'joconde', 'joffol', 'join', 'joined', 'joining', 'joins', 'joint', 'joints', 'juces', 'jug', 'jugs', 'juice', 'juices', 'juicier', 'juicy', 'julienne', 'julienned', 'jumbo', 'jump', 'jumping', 'juniper', 'jus', 'kachori', 'kaffir', 'kale', 'karahi', 'katsu', 'kebab', 'kebabs', 'kedgeree', 'keep', 'keeping', 'keeps', 'kept', 'kernel', 'kernels', 'ketchup', 'ketjap', 'kettle', 'key', 'kg', 'khao', 'kick', 'kid', 'kidney', 'kidneys', 'kids', 'kiev', 'kill', 'kilo', 'kimchi', 'kind', 'kinds', 'king', 'kirsch', 'kisses', 'kitchen', 'kiwi', 'klippfisk', 'knead', 'kneadable', 'kneaded', 'kneading', 'knickerbockerglory', 'knife', 'knives', 'knob', 'knobs', 'knock', 'knocked', 'knocking', 'knot', 'knots', 'knotted', 'know', 'known', 'knuckle', 'knuckles', 'kofta', 'kohlrabi', 'kransekake', 'kransekakes', 'krispies', 'label', 'labneh', 'lace', 'laces', 'lacey', 'lacquered', 'ladle', 'ladleful', 'ladlefuls', 'ladles', 'ladybird', 'lager', 'lamb', 'lambs', 'lanes', 'langoustines', 'lard', 'lardons', 'large', 'larger', 'largest', 'largish', 'lasagne', 'lashings', 'last', 'lastly', 'later', 'latte', 'lattice', 'lavender', 'lavosh', 'lax', 'lay', 'layer', 'layered', 'layering', 'layers', 'laying', 'lb', 'leaf', 'leafy', 'leak', 'leakage', 'leaks', 'lean', 'leaning', 'least', 'leat', 'leather', 'leathery', 'leave', 'leavers', 'leaves', 'leaving', 'leche', 'leek', 'leeks', 'left', 'leftover', 'leftovers', 'leg', 'legless', 'legs', 'leicester', 'lemon', 'lemonade', 'lemongrass', 'lemons', 'length', 'lengths', 'lengthways', 'lengthwise', 'lentil', 'lentils', 'less', 'lessen', 'let', 'letter', 'letting', 'lettuce', 'lettuces', 'level', 'levelling', 'levels', 'lever', 'liberally', 'lices', 'lid', 'lidded', 'lids', 'lie', 'life', 'lift', 'lifted', 'lifting', 'light', 'lighten', 'lighter', 'lightly', 'lightness', 'lights', 'like', 'liked', 'likely', 'liking', 'lilac', 'limbs', 'lime', 'limes', 'limoncello', 'lin', 'line', 'lined', 'linen', 'liner', 'lines', 'linguine', 'lining', 'linseed', 'linseeds', 'lip', 'lipped', 'liqueur', 'liquid', 'liquidise', 'liquidiser', 'liquidizer', 'liquids', 'liquor', 'liquorice', 'list', 'listed', 'lit', 'literally', 'litre', 'litres', 'little', 'lively', 'liver', 'livers', 'lml', 'loads', 'loaf', 'loaves', 'lobster', 'log', 'logs', 'loin', 'lollies', 'lollipop', 'lollipops', 'lolly', 'long', 'longer', 'longest', 'look', 'looking', 'looks', 'loop', 'looping', 'loops', 'loose', 'loosely', 'loosen', 'loosened', 'loosens', 'looser', 'lose', 'loses', 'losing', 'lost', 'lot', 'lots', 'lovage', 'love', 'lovely', 'low', 'lower', 'lowered', 'lowering', 'lowest', 'lowish', 'lubricate', 'lubrication', 'lukewarm', 'lump', 'lumps', 'lumpy', 'lunch', 'lunchbox', 'lunchboxes', 'lunchtime', 'lurk', 'luscious', 'luster', 'lustre', 'luxury', 'lying', 'macadamia', 'macadamias', 'macaron', 'macaroni', 'macaroon', 'macaroons', 'mace', 'macerate', 'macerated', 'machine', 'mackerel', 'mad', 'made', 'madeira', 'madeleine', 'madeleines', 'madras', 'magazine', 'magic', 'mahlepi', 'mahogany', 'main', 'maintain', 'major', 'majoram', 'make', 'maker', 'makes', 'making', 'malleable', 'mallet', 'malt', 'malted', 'man', 'manage', 'manageable', 'mandolin', 'mandoline', 'mange', 'mangetout', 'mango', 'mangoes', 'manie', 'manis', 'manually', 'manufacturer', 'manufacturers', 'many', 'maple', 'marble', 'marbled', 'marbling', 'margarine', 'margin', 'marie', 'marinade', 'marinate', 'marinated', 'marjoram', 'mark', 'marked', 'market', 'marking', 'marks', 'marmalade', 'marnier', 'marquise', 'marrow', 'marrrow', 'marry', 'mars', 'marsala', 'marshmallow', 'marshmallows', 'martini', 'marzipan', 'masa', 'masala', 'mascarpone', 'mash', 'mashed', 'masher', 'mashing', 'masking', 'mass', 'massa', 'massage', 'massaging', 'massaman', 'massive', 'massively', 'mastic', 'mat', 'match', 'matches', 'matchstick', 'material', 'materials', 'mats', 'matt', 'matter', 'matures', 'matzo', 'mauve', 'maximum', 'may', 'mayo', 'mayonnaise', 'meal', 'mean', 'means', 'meantime', 'meanwhile', 'measure', 'measured', 'measurement', 'measures', 'measuring', 'meat', 'meatball', 'meatballs', 'meatloaf', 'meats', 'meaty', 'medallions', 'medals', 'medium', 'medley', 'meet', 'meets', 'mein', 'melon', 'melt', 'melted', 'melting', 'meltingly', 'melts', 'membrane', 'membranes', 'membrillo', 'men', 'merge', 'mergeuz', 'meringue', 'meringues', 'mesh', 'meshed', 'mess', 'messy', 'metal', 'metallic', 'method', 'metrexcm', 'mexican', 'micro', 'microwavable', 'microwave', 'microwaveable', 'middle', 'middles', 'midpoint', 'midway', 'might', 'mild', 'milk', 'milkshake', 'milky', 'mill', 'milled', 'millefeuille', 'millimetres', 'min', 'mince', 'minceand', 'minced', 'mincemeat', 'mincer', 'mincing', 'mind', 'mine', 'mineral', 'mini', 'minimal', 'minimise', 'minimum', 'mins', 'mint', 'minted', 'mintues', 'minus', 'minuscule', 'minute', 'mirin', 'mirror', 'miso', 'misshaping', 'missing', 'mister', 'mix', 'mixed', 'mixer', 'mixes', 'mixing', 'mixtuer', 'mixture', 'mixtures', 'mixure', 'mizuna', 'ml', 'mm', 'mocha', 'moderate', 'moderately', 'moist', 'moisten', 'moistened', 'moistening', 'moisture', 'moka', 'molasses', 'molds', 'moment', 'moments', 'money', 'monkfish', 'month', 'months', 'moon', 'mop', 'morel', 'morels', 'morning', 'mortar', 'mostly', 'motion', 'motor', 'mould', 'moulding', 'moulds', 'mound', 'mounds', 'moussaka', 'mousse', 'mousses', 'moussey', 'mouth', 'move', 'movement', 'movements', 'moving', 'mozzarella', 'much', 'mud', 'muddle', 'muffin', 'muffins', 'mug', 'mugs', 'mulch', 'mulled', 'mullet', 'mummies', 'muscavado', 'muscovado', 'mush', 'mushroom', 'mushrooms', 'mushy', 'muslin', 'mussel', 'mussels', 'must', 'mustaches', 'mustard', 'mustards', 'mustardy', 'mutton', 'naan', 'naans', 'nachos', 'naked', 'napkin', 'napkins', 'narrow', 'narrower', 'narrowest', 'nasi', 'natural', 'navy', 'nb', 'nduja', 'near', 'nearer', 'nearest', 'nearly', 'neat', 'neaten', 'neatly', 'necessary', 'neck', 'need', 'needed', 'needle', 'needles', 'needs', 'negra', 'nero', 'nest', 'nestle', 'nestling', 'nests', 'net', 'neutralising', 'never', 'new', 'newspaper', 'next', 'nibs', 'nice', 'nicely', 'nicest', 'nigella', 'night', 'nine', 'nog', 'noise', 'noisette', 'non', 'nonstick', 'noodle', 'noodles', 'nook', 'nooks', 'normal', 'normally', 'nose', 'noses', 'notch', 'note', 'nothing', 'notice', 'nougat', 'nozzle', 'nozzles', 'nudge', 'nudges', 'nugget', 'nuggets', 'number', 'nut', 'nutmeg', 'nutritious', 'nuts', 'nutty', 'oat', 'oatcakes', 'oatmeal', 'oats', 'oblong', 'obsession', 'obtain', 'obvious', 'occasion', 'occasional', 'occasionally', 'occassionally', 'octopus', 'odd', 'oelek', 'offcuts', 'offers', 'officiously', 'offset', 'often', 'oil', 'oiled', 'oils', 'oily', 'ok', 'okay', 'okra', 'old', 'olive', 'olives', 'omelette', 'omelettes', 'one', 'ones', 'onion', 'onions', 'online', 'onto', 'ooze', 'oozes', 'oozing', 'oozy', 'opaque', 'open', 'opened', 'opener', 'opening', 'opera', 'operated', 'opposite', 'option', 'optional', 'ora', 'orange', 'oranges', 'order', 'ordinary', 'ordination', 'oregano', 'oreo', 'original', 'orzo', 'others', 'otherwise', 'outdoors', 'outer', 'outline', 'outside', 'outsides', 'outward', 'outwards', 'oval', 'ovals', 'oveb', 'oven', 'ovenproof', 'ovens', 'overbeat', 'overcomes', 'overcook', 'overcooking', 'overcrowd', 'overdone', 'overfill', 'overhanding', 'overhang', 'overhanging', 'overheat', 'overlap', 'overlapping', 'overlaps', 'overload', 'overloading', 'overly', 'overmix', 'overnight', 'overpower', 'overpowering', 'overripe', 'overwhip', 'overwork', 'overworking', 'ox', 'oxen', 'oxtail', 'oyster', 'oz', 'pace', 'pack', 'package', 'packaging', 'packed', 'packet', 'packing', 'pad', 'paddle', 'paella', 'pain', 'paint', 'paintbrush', 'pair', 'pairs', 'pak', 'pakora', 'pakoras', 'palate', 'palatte', 'palce', 'pale', 'paleness', 'paler', 'palette', 'pallet', 'palm', 'palmiers', 'palms', 'pan', 'pancake', 'pancakes', 'pancetta', 'panch', 'paneer', 'panels', 'panettone', 'panhandle', 'panini', 'panko', 'panna', 'pans', 'paper', 'papers', 'papery', 'pappardelle', 'papper', 'paprika', 'paprikas', 'par', 'parallel', 'paratha', 'parathas', 'parboil', 'parboiled', 'parcel', 'parcels', 'parchment', 'pare', 'parfait', 'paring', 'parkin', 'parma', 'parmesan', 'parmigiana', 'parnsips', 'parsley', 'parsnip', 'parsnips', 'part', 'partially', 'particular', 'particularly', 'partly', 'partridge', 'parts', 'party', 'pass', 'passata', 'passed', 'passing', 'passion', 'passionfruit', 'pasta', 'paste', 'pastel', 'pasteurised', 'pastie', 'pasties', 'pastis', 'pastrami', 'pastries', 'pastry', 'pasty', 'pat', 'pata', 'patatas', 'patch', 'patched', 'patches', 'patching', 'pate', 'patient', 'patissi', 'patissiere', 'pattern', 'patterned', 'patterns', 'pattie', 'patties', 'patting', 'patty', 'paul', 'pause', 'pav', 'pavlova', 'pay', 'pe', 'pea', 'peach', 'peaches', 'peak', 'peaked', 'peaks', 'peanut', 'peanuts', 'pear', 'pearl', 'pearlescent', 'pearly', 'pears', 'peas', 'peashoots', 'pecan', 'pecans', 'pecorino', 'pecornio', 'peek', 'peel', 'peeled', 'peeler', 'peelers', 'peeling', 'peelings', 'peels', 'peg', 'pegs', 'pen', 'pence', 'pencil', 'penetrate', 'penne', 'people', 'peper', 'peppadew', 'pepper', 'peppercorns', 'peppermill', 'peppermint', 'peppers', 'per', 'percent', 'percorino', 'perfect', 'perfectly', 'perforated', 'performed', 'perfumed', 'perhaps', 'peri', 'perimeter', 'period', 'periodically', 'permeate', 'persillade', 'persistent', 'person', 'pes', 'pestle', 'pesto', 'petal', 'petals', 'petits', 'pheasant', 'pheasants', 'phoran', 'picada', 'pick', 'picked', 'picking', 'pickle', 'pickled', 'pickles', 'pickling', 'picnic', 'picnics', 'pictured', 'pie', 'piece', 'pieces', 'pierce', 'pierced', 'piercing', 'pierogi', 'pies', 'pigeon', 'pigs', 'pilaf', 'pilaff', 'pile', 'piles', 'piling', 'pillar', 'pillars', 'pillowcase', 'pillowy', 'piment', 'pimenton', 'pimms', 'pin', 'pinch', 'pinched', 'pinches', 'pinching', 'pine', 'pineapple', 'pineapples', 'pinenuts', 'ping', 'pink', 'pinkness', 'pins', 'pint', 'pints', 'pinwheel', 'pinwheels', 'pip', 'pipe', 'pipeable', 'piped', 'piping', 'pips', 'piquant', 'piquillo', 'piri', 'pistachio', 'pistachios', 'pistou', 'pita', 'pith', 'pitta', 'pittas', 'pitted', 'pizza', 'pizzas', 'place', 'placed', 'places', 'placing', 'plaice', 'plain', 'plait', 'plaited', 'plaits', 'plan', 'plantains', 'plasters', 'plastic', 'plate', 'plates', 'platter', 'play', 'pleasant', 'pleasantly', 'please', 'pleat', 'pleated', 'pleats', 'plenty', 'pliable', 'pliant', 'pliers', 'plop', 'plug', 'plum', 'plump', 'plums', 'plunge', 'plunging', 'plus', 'poach', 'poached', 'poaches', 'poaching', 'pocked', 'pocket', 'pockets', 'pod', 'podded', 'pods', 'point', 'pointed', 'pointing', 'pointless', 'points', 'pointy', 'pois', 'poke', 'poked', 'poking', 'polenta', 'pollack', 'pollo', 'pollock', 'poly', 'polystyrene', 'polythene', 'pomegranate', 'pomegranates', 'pomelo', 'pond', 'pong', 'pontefract', 'pool', 'pooled', 'pop', 'popcorn', 'poppadoms', 'popped', 'popping', 'poppy', 'pops', 'porcini', 'pork', 'porridge', 'port', 'portion', 'portioned', 'portions', 'portobello', 'position', 'positioned', 'positioning', 'posset', 'possible', 'pot', 'potato', 'potatoes', 'pots', 'pouch', 'poultry', 'pound', 'pounding', 'pour', 'pourable', 'poured', 'pouring', 'povitica', 'powder', 'powdered', 'powders', 'powdery', 'power', 'powered', 'powerful', 'praline', 'prawn', 'prawns', 'pre', 'preapred', 'precise', 'precooked', 'precooking', 'precut', 'prefer', 'preferably', 'preference', 'preferred', 'preheat', 'preheated', 'preheating', 'preparation', 'prepare', 'prepared', 'preparing', 'present', 'presentable', 'preserve', 'preserved', 'preserving', 'press', 'pressed', 'pressing', 'pressure', 'presto', 'prettiest', 'pretty', 'pretzel', 'pretzels', 'prevent', 'prevents', 'previous', 'previously', 'prick', 'pricked', 'printing', 'prints', 'prise', 'prising', 'probably', 'probe', 'problem', 'process', 'processing', 'processor', 'processors', 'prod', 'prodded', 'produce', 'produces', 'professional', 'profiterole', 'profiteroles', 'progress', 'progresses', 'pronto', 'proof', 'prop', 'propelling', 'proper', 'properly', 'props', 'prosciuto', 'prosciutto', 'proscuitto', 'prosecco', 'protect', 'protecting', 'protection', 'protects', 'protrude', 'protruding', 'proudly', 'prove', 'proved', 'proven', 'provence', 'provide', 'provided', 'providing', 'proving', 'provolone', 'prs', 'prune', 'prunes', 'psyllium', 'pt', 'pu', 'pudding', 'puddings', 'puff', 'puffed', 'puffs', 'puffy', 'pugliese', 'pull', 'pullao', 'pulled', 'pulling', 'pulls', 'pulp', 'pulpy', 'pulse', 'pulsing', 'pumpernickel', 'pumpkin', 'punch', 'pungent', 'pupils', 'puppy', 'pur', 'pure', 'puree', 'purple', 'purposes', 'push', 'pushed', 'pushes', 'pushing', 'put', 'puttanesca', 'putting', 'puy', 'puzzle', 'pyramid', 'quail', 'quails', 'quality', 'quantities', 'quantity', 'quark', 'quarter', 'quartered', 'quarters', 'quenelle', 'quenelles', 'quesadilla', 'quesadillas', 'quiche', 'quick', 'quicker', 'quickest', 'quickly', 'quill', 'quince', 'quinces', 'quinoa', 'quite', 'rabbit', 'rack', 'racket', 'rackets', 'racks', 'radish', 'radishes', 'rag', 'ragged', 'rago', 'ragu', 'raise', 'raised', 'raises', 'raisin', 'raising', 'raisins', 'raita', 'ramekin', 'ramekins', 'random', 'rapeseed', 'rapid', 'rapidly', 'rare', 'rarebit', 'ras', 'rascals', 'rasher', 'rashers', 'raspberries', 'raspberry', 'ratafia', 'ratafias', 'ratatouille', 'rate', 'rather', 'ratio', 'ravioli', 'raviolo', 'raw', 'razor', 'rds', 'reach', 'reached', 'reaches', 'reaching', 'reacting', 'reactive', 'read', 'reads', 'ready', 'real', 'realistic', 'really', 'rear', 'reason', 'reasonably', 'reblochon', 'recently', 'recipe', 'recommends', 'rectangle', 'rectangles', 'rectangular', 'red', 'redcurrant', 'redcurrants', 'redistribute', 'reduce', 'reduced', 'reduces', 'reducing', 'reduction', 'refer', 'refresh', 'refreshing', 'refridgerate', 'refrigerate', 'refrigerated', 'refrigerator', 'regular', 'regularly', 'reheat', 'reheated', 'reheats', 'rehydrate', 'rehydrated', 'reindeer', 'reknead', 'relative', 'relatively', 'relax', 'release', 'released', 'releases', 'releasing', 'relevant', 'religieuse', 'relish', 'rellish', 'reluctant', 'remain', 'remainder', 'remaining', 'remains', 'remember', 'remembering', 'remnants', 'remoulade', 'removed', 'removes', 'removing', 'rendang', 'render', 'rendered', 'renders', 'repeat', 'repeatedly', 'repeating', 'repeats', 'replace', 'replenishing', 'represent', 'require', 'required', 'reroll', 'rerolling', 'res', 'rescue', 'resealable', 'resemble', 'resembles', 'resembling', 'reserve', 'reserved', 'reserving', 'residual', 'residue', 'resist', 'resistance', 'rest', 'rested', 'resting', 'restrain', 'result', 'resulting', 'results', 'retain', 'retained', 'retaining', 'retains', 'return', 'returned', 'returning', 'returns', 'reusable', 'reused', 'reusing', 'reveal', 'revealing', 'reverse', 'reward', 'rg', 'rhubarb', 'rib', 'ribbon', 'ribbons', 'ribs', 'rice', 'ricer', 'rich', 'richer', 'richly', 'richness', 'ricotta', 'rid', 'ridged', 'ridges', 'riesling', 'rigatoni', 'right', 'rigid', 'rigorous', 'rim', 'rind', 'ring', 'ringlets', 'rings', 'rinse', 'rinsed', 'rinsing', 'rioja', 'rip', 'ripe', 'ripeness', 'ripped', 'ripple', 'rise', 'risen', 'rises', 'rising', 'risotto', 'road', 'roast', 'roasted', 'roasting', 'robin', 'robust', 'rock', 'rockefeller', 'rocket', 'rocky', 'rod', 'rods', 'rogue', 'roll', 'rollable', 'rolled', 'rollers', 'rolling', 'rolls', 'roly', 'romana', 'romano', 'roof', 'rooftops', 'room', 'roomy', 'root', 'roots', 'rope', 'ropes', 'roquefort', 'rosace', 'rose', 'rosemary', 'roses', 'rosette', 'rosettes', 'rosewater', 'rosti', 'rostis', 'rotary', 'rotate', 'rotating', 'roti', 'rotis', 'rotollo', 'rotolo', 'rough', 'roughen', 'roughened', 'roughly', 'rouille', 'roulade', 'round', 'rounded', 'rounds', 'roux', 'rover', 'row', 'rows', 'royal', 'royale', 'rub', 'rubbed', 'rubber', 'rubbing', 'ruby', 'rugby', 'ruin', 'rule', 'ruler', 'rum', 'rumoured', 'run', 'runner', 'runnier', 'running', 'runny', 'runs', 'rush', 'russian', 'rustic', 'rye', 'sabl', 'sable', 'sables', 'sac', 'sacher', 'sachets', 'sack', 'saddle', 'safe', 'safely', 'safer', 'saffron', 'sag', 'sage', 'sail', 'sake', 'salad', 'salads', 'salady', 'salami', 'salmon', 'salmoriglio', 'salsa', 'salt', 'salted', 'saltfish', 'saltiness', 'salting', 'salty', 'sambal', 'samosa', 'samosas', 'samphire', 'sand', 'sandpaper', 'sandwich', 'sandwiched', 'sandwiches', 'sandwiching', 'sandy', 'santa', 'sardine', 'sardines', 'satiny', 'satisfied', 'satsuma', 'satsumas', 'sauce', 'sauceboats', 'saucepan', 'saucepans', 'saucer', 'sauces', 'saucy', 'sauerkraut', 'sausage', 'sausagemeat', 'sausages', 'saut', 'saute', 'sauvignon', 'savarin', 'save', 'saving', 'savoiardi', 'savoury', 'savoy', 'saw', 'say', 'sbord', 'scald', 'scalded', 'scalding', 'scale', 'scales', 'scallop', 'scallops', 'scalpel', 'scant', 'scatter', 'scattered', 'scattering', 'schedule', 'schnitzel', 'scissors', 'scone', 'scones', 'scoop', 'scooped', 'scooping', 'scoops', 'scorch', 'scorched', 'scorching', 'score', 'scored', 'scores', 'scoring', 'scotch', 'scourer', 'scouse', 'scraggy', 'scramble', 'scrambled', 'scrape', 'scraped', 'scraper', 'scraping', 'scraps', 'scratchings', 'screen', 'screw', 'screwdriver', 'screwpine', 'scroll', 'scrub', 'scrubbing', 'scruffy', 'scrunch', 'scrunched', 'scrupulously', 'scuff', 'sculpt', 'scum', 'sea', 'seabass', 'seafood', 'seal', 'sealable', 'sealed', 'sealing', 'seam', 'seame', 'sear', 'seared', 'searing', 'seashell', 'season', 'seasonal', 'seasoned', 'seasoning', 'seated', 'seaweed', 'second', 'seconds', 'section', 'sections', 'secure', 'secured', 'securely', 'securing', 'sediment', 'see', 'seed', 'seeded', 'seedless', 'seeds', 'seem', 'seems', 'seen', 'seep', 'seeped', 'seeping', 'segment', 'segmented', 'segments', 'seive', 'seize', 'self', 'semi', 'semicircle', 'semicircles', 'semicircular', 'semolina', 'separate', 'separated', 'separately', 'separates', 'separating', 'seperate', 'sequence', 'serrano', 'serrated', 'serve', 'served', 'serving', 'servings', 'sesame', 'sessions', 'set', 'sets', 'setting', 'settle', 'settled', 'seven', 'several', 'shade', 'shaggy', 'shake', 'shaken', 'shaker', 'shakes', 'shaking', 'shallot', 'shallots', 'shallow', 'shank', 'shanks', 'shaoxing', 'shape', 'shaped', 'shapes', 'shaping', 'shard', 'shards', 'sharp', 'sharpen', 'sharply', 'sharpness', 'shatkora', 'shatter', 'shave', 'shaved', 'shavings', 'shears', 'sheen', 'sheep', 'sheet', 'sheets', 'shelf', 'shell', 'shellfish', 'shells', 'shelves', 'shepherd', 'sherbet', 'sherry', 'shiitake', 'shimmer', 'shimmering', 'shimmers', 'shin', 'shine', 'shining', 'shins', 'shiny', 'shisho', 'shoot', 'shoots', 'shop', 'short', 'shortbread', 'shortbreads', 'shortcrust', 'shortcut', 'shortening', 'shorter', 'shortest', 'shortly', 'shot', 'shoulder', 'show', 'shower', 'showing', 'shown', 'shoyu', 'shred', 'shredded', 'shredding', 'shreds', 'shrikhand', 'shrimp', 'shrimps', 'shrink', 'shrinking', 'shrunk', 'shut', 'sichuan', 'side', 'sided', 'sides', 'sideways', 'sieve', 'sieved', 'sift', 'sifted', 'sifting', 'sign', 'signature', 'significantly', 'silhouette', 'silicon', 'silicone', 'silken', 'silky', 'silver', 'similar', 'similarly', 'simmer', 'simmered', 'simmering', 'simon', 'simple', 'simpler', 'simply', 'since', 'sinew', 'sinewy', 'singe', 'single', 'sink', 'sirloin', 'sit', 'sits', 'sitting', 'six', 'sixth', 'size', 'sized', 'sizes', 'sizzle', 'sizzles', 'sizzling', 'skeletons', 'skewer', 'skewered', 'skewers', 'skim', 'skimmed', 'skimming', 'skimp', 'skin', 'skinless', 'skinned', 'skins', 'skip', 'sky', 'slabs', 'slacken', 'slackens', 'slams', 'slash', 'slashes', 'slashing', 'slather', 'slathered', 'slaw', 'sleeves', 'slender', 'slice', 'sliced', 'slicer', 'slices', 'slicing', 'slick', 'slide', 'slides', 'sliding', 'slight', 'slightly', 'slim', 'slip', 'slit', 'slits', 'sliver', 'slivers', 'sloe', 'slope', 'sloping', 'sloppy', 'slot', 'slots', 'slotted', 'slow', 'slowly', 'slug', 'slush', 'slushy', 'sm', 'small', 'smaller', 'smallest', 'smallish', 'smash', 'smear', 'smearing', 'smell', 'smelling', 'smells', 'smoke', 'smoked', 'smokie', 'smoking', 'smoky', 'smooth', 'smoother', 'smoothie', 'smoothing', 'smoothish', 'smoothly', 'smothered', 'snack', 'snacking', 'snap', 'snaps', 'sneaky', 'snip', 'snipping', 'snow', 'snowflake', 'snowflakes', 'snows', 'snug', 'snugly', 'soak', 'soaked', 'soaking', 'soaks', 'soapy', 'soba', 'socca', 'soda', 'soft', 'soften', 'softened', 'softening', 'softens', 'softer', 'softly', 'softness', 'soggy', 'soi', 'soil', 'soldiers', 'sole', 'solid', 'solidified', 'solidify', 'solidifying', 'solids', 'solution', 'someone', 'something', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorbet', 'sorrel', 'sort', 'souffl', 'souffle', 'sound', 'sounds', 'soup', 'soupy', 'sour', 'source', 'sourdough', 'soured', 'sourer', 'souvlakia', 'soy', 'soya', 'space', 'spaced', 'spacer', 'spacers', 'spaces', 'spacing', 'spaghetti', 'spanakopita', 'spanish', 'spare', 'spareribs', 'sparingly', 'sparkling', 'spatchcocked', 'spatter', 'spatula', 'spatulas', 'spatzle', 'spear', 'spears', 'special', 'speck', 'speckled', 'specks', 'spectacle', 'speed', 'speeding', 'spelt', 'spherical', 'spice', 'spiced', 'spices', 'spicy', 'spike', 'spikes', 'spill', 'spilling', 'spills', 'spinach', 'spinner', 'spinning', 'spiral', 'spiraling', 'spiralled', 'spirals', 'spirit', 'spit', 'spitting', 'splash', 'splashes', 'splatter', 'splattering', 'split', 'splitting', 'splutter', 'spoil', 'spoilage', 'sponge', 'sponges', 'sponginess', 'spongy', 'spooky', 'spoon', 'spooned', 'spoonful', 'spoonfuls', 'spooning', 'spoons', 'spotlessly', 'spots', 'spotted', 'sprat', 'sprats', 'spray', 'sprayed', 'spraying', 'spread', 'spreadable', 'spreading', 'spreads', 'sprig', 'sprigs', 'spring', 'springclip', 'springform', 'springfrom', 'springing', 'springs', 'springy', 'sprinked', 'sprinkle', 'sprinkled', 'sprinkles', 'sprinkling', 'sprout', 'sprouting', 'sprouts', 'spun', 'spurts', 'square', 'squares', 'squash', 'squashed', 'squashing', 'squashy', 'squeak', 'squeeze', 'squeezed', 'squeezing', 'squeezy', 'squelch', 'squid', 'squidge', 'squidgy', 'squidy', 'squiggles', 'squirt', 'squirting', 'squish', 'squished', 'stab', 'stabilise', 'stack', 'stacked', 'stacking', 'stacks', 'stage', 'stages', 'staggered', 'staining', 'stainless', 'stale', 'stalk', 'stalks', 'stamens', 'stamp', 'stand', 'standard', 'standing', 'stands', 'stanley', 'star', 'starch', 'starches', 'starchy', 'stars', 'start', 'started', 'starter', 'starting', 'starts', 'stated', 'stay', 'stays', 'steadily', 'steady', 'steak', 'steaks', 'steam', 'steamed', 'steamer', 'steamers', 'steaming', 'steams', 'steel', 'steep', 'steeped', 'stem', 'stemmed', 'stems', 'stencil', 'stenciled', 'step', 'steps', 'sterile', 'sterilise', 'sterilised', 'sterilized', 'stevia', 'stew', 'stewed', 'stewing', 'stews', 'stick', 'stickier', 'stickiness', 'sticking', 'sticks', 'sticky', 'stiff', 'stiffen', 'stiffened', 'stiffer', 'still', 'stilton', 'stir', 'stirred', 'stirring', 'stirs', 'stock', 'stockpot', 'stocks', 'stomach', 'stone', 'stones', 'stool', 'stop', 'stopped', 'stopper', 'stoppers', 'stopping', 'stops', 'storage', 'store', 'stored', 'storing', 'stout', 'stove', 'straight', 'straightaway', 'straighten', 'straightway', 'strain', 'strained', 'straining', 'strand', 'strands', 'straw', 'strawberries', 'strawberry', 'straws', 'stray', 'streaks', 'streaky', 'stream', 'strength', 'strengthen', 'stretch', 'stretched', 'stretches', 'stretching', 'stretchy', 'strewn', 'string', 'strings', 'stringy', 'strip', 'striped', 'stripes', 'stripped', 'strips', 'strirring', 'stroganoff', 'strokes', 'stromboli', 'strong', 'stronger', 'strongly', 'strudel', 'strudels', 'struggle', 'struggling', 'stuck', 'stud', 'studded', 'stuff', 'stuffed', 'stuffing', 'sturdy', 'style', 'submerged', 'submerging', 'subsided', 'subsides', 'substitute', 'substituting', 'subtlety', 'successful', 'succulent', 'sucked', 'suddenly', 'suet', 'sufficient', 'sufficiently', 'sugar', 'sugared', 'sugars', 'sugary', 'suggestion', 'suit', 'suitable', 'suitably', 'sultanas', 'sum', 'sumac', 'summer', 'sun', 'sundae', 'sunday', 'sundried', 'sunflower', 'sunk', 'sunlight', 'super', 'superb', 'supervise', 'supervised', 'support', 'supports', 'supposed', 'sure', 'surface', 'surfaces', 'surplus', 'surprise', 'surround', 'surrounded', 'surrounding', 'sushi', 'suspend', 'suspended', 'suspending', 'swap', 'swapping', 'sweat', 'sweated', 'swede', 'swedish', 'sweep', 'sweet', 'sweetcorn', 'sweeten', 'sweetened', 'sweetener', 'sweeter', 'sweetness', 'sweets', 'swell', 'swells', 'swift', 'swiftly', 'swimming', 'swirl', 'swirled', 'swirling', 'swirls', 'swirly', 'swiss', 'switch', 'swollen', 'syllabub', 'syringe', 'syrup', 'syrups', 'syrupy', 'szechuan', 'tabasco', 'tabbouleh', 'table', 'tablespoon', 'tablespoonful', 'tablespoonfuls', 'tablespoons', 'tablspoons', 'tack', 'tacked', 'tacky', 'taco', 'tag', 'tagine', 'tagliatelle', 'tags', 'tahini', 'tail', 'tails', 'taint', 'take', 'taken', 'takes', 'taking', 'taleggio', 'tall', 'tamari', 'tamarind', 'tandoori', 'tang', 'tangerine', 'tangerines', 'tangle', 'tangled', 'tangy', 'tap', 'tape', 'tapenade', 'tapered', 'tapering', 'tapioca', 'tapped', 'tapping', 'tarka', 'tarpaulin', 'tarragon', 'tarst', 'tart', 'tartar', 'tartare', 'tarte', 'tartiflette', 'tartlet', 'tartlets', 'tarts', 'taste', 'tastes', 'tasting', 'tatin', 'taught', 'taut', 'tbsp', 'te', 'tea', 'teabag', 'teabags', 'teacake', 'teacakes', 'teacloth', 'teacups', 'tear', 'teardrop', 'tearing', 'tears', 'tease', 'teasing', 'teaspoon', 'teaspoonful', 'teaspoonfuls', 'teaspoons', 'teaspoonsful', 'teau', 'technique', 'tedious', 'tell', 'temper', 'temperature', 'temperatures', 'tempered', 'template', 'templates', 'temptation', 'tempted', 'temptingly', 'ten', 'tend', 'tender', 'tenderer', 'tenderise', 'tenderize', 'tenderloin', 'tenderloins', 'tenderness', 'tending', 'tends', 'tennis', 'tension', 'tentacles', 'tepid', 'tequila', 'teriyaki', 'terracotta', 'terrine', 'test', 'tested', 'tester', 'testing', 'texture', 'textured', 'textures', 'thai', 'thanks', 'thaw', 'thawed', 'therapeutic', 'therefore', 'thermometer', 'thick', 'thicken', 'thickened', 'thickening', 'thickens', 'thicker', 'thickest', 'thickish', 'thickly', 'thickness', 'thicktest', 'thigh', 'thighs', 'thin', 'thing', 'things', 'think', 'thinly', 'thinner', 'thinnest', 'thinning', 'thins', 'third', 'thirds', 'thirty', 'thorough', 'thoroughly', 'though', 'thousands', 'thr', 'thread', 'threading', 'threads', 'three', 'throughly', 'throughout', 'throw', 'thumb', 'thumbnail', 'thyme', 'tidy', 'tie', 'tied', 'tier', 'tiers', 'tight', 'tighten', 'tightly', 'tightness', 'tikka', 'tile', 'till', 'tilt', 'tilting', 'time', 'timer', 'timers', 'times', 'timing', 'timings', 'tin', 'tines', 'tinged', 'tinned', 'tins', 'tiny', 'tip', 'tipped', 'tipping', 'tips', 'tiramisu', 'tired', 'tissi', 'tissue', 'toad', 'toast', 'toasted', 'toaster', 'toasting', 'toasts', 'toasty', 'toc', 'toddy', 'toffee', 'tofu', 'together', 'tomato', 'tomatoes', 'tombstones', 'ton', 'tone', 'tongs', 'tongue', 'tonic', 'tons', 'tools', 'tooth', 'toothpick', 'toothpicks', 'top', 'topped', 'topping', 'toppings', 'toppling', 'tops', 'torch', 'torn', 'torta', 'tortellini', 'tortilla', 'tortillas', 'toss', 'tossed', 'tossing', 'total', 'totally', 'touch', 'touched', 'touching', 'tough', 'toughen', 'tout', 'toward', 'towards', 'towel', 'towels', 'tower', 'towers', 'towl', 'trace', 'traces', 'trademark', 'traditional', 'trail', 'trails', 'tranfer', 'transfer', 'transferring', 'translucent', 'transparent', 'transport', 'transporting', 'trap', 'trapped', 'trapping', 'tray', 'traybake', 'trays', 'treacle', 'treat', 'trebled', 'tree', 'trees', 'trench', 'triangle', 'triangles', 'triangular', 'trick', 'trickle', 'trickling', 'tricky', 'trifle', 'trim', 'trimmed', 'trimming', 'trimmings', 'triple', 'tripled', 'trivet', 'tropical', 'trouble', 'trout', 'true', 'truffle', 'truffles', 'trunk', 'trust', 'try', 'trying', 'tsp', 'tub', 'tube', 'tubes', 'tuck', 'tucked', 'tucking', 'tug', 'tuile', 'tuiles', 'tuille', 'tumble', 'tumblers', 'tumeric', 'tuna', 'tupperware', 'tur', 'tureen', 'turkey', 'turkish', 'turmeric', 'turn', 'turned', 'turning', 'turnip', 'turnips', 'turnovers', 'turns', 'tutti', 'tweezers', 'twelve', 'twenty', 'twice', 'twinkle', 'twist', 'twisted', 'twisting', 'twists', 'two', 'twofold', 'tying', 'type', 'types', 'tzatziki', 'udon', 'ultimate', 'ultimately', 'ultra', 'un', 'unattended', 'uncooked', 'uncover', 'uncovered', 'uncurl', 'uncut', 'undecorated', 'undercooked', 'undercooking', 'underdone', 'underneath', 'underside', 'undersides', 'undisturbed', 'undo', 'undone', 'uneaten', 'uneven', 'unevenly', 'unfold', 'unfrozen', 'unglazed', 'ungreased', 'uniform', 'union', 'unless', 'unlined', 'unmanageably', 'unmelted', 'unmixed', 'unmould', 'unopened', 'unpeeled', 'unravelling', 'unroll', 'unsalted', 'unscrunch', 'unsmoked', 'unspring', 'unsure', 'untidily', 'untie', 'untruss', 'unwanted', 'unwelcome', 'unwrap', 'upend', 'upon', 'upper', 'uppermost', 'upright', 'upside', 'upturn', 'upturned', 'upward', 'upwards', 'urge', 'usable', 'use', 'used', 'useful', 'using', 'usual', 'usually', 'vaguest', 'vanilla', 'vanish', 'variation', 'variations', 'variety', 'various', 'vary', 'varying', 'veal', 'veg', 'vegan', 'vegetable', 'vegetables', 'vegetarian', 'vegetarians', 'veggie', 'veggies', 'vein', 'veins', 'velodrome', 'velvety', 'venison', 'vent', 'vents', 'venue', 'verde', 'vermicelli', 'vermouth', 'versa', 'version', 'vertical', 'vertically', 'vibrant', 'vice', 'victoria', 'video', 'vierge', 'vigorous', 'vigorously', 'vin', 'vinaigrette', 'vindaloo', 'vine', 'vinegar', 'vinegary', 'vingar', 'violet', 'violets', 'virgin', 'virtually', 'visible', 'visibly', 'visual', 'visually', 'vivid', 'vocer', 'vodka', 'volume', 'vortex', 'wad', 'wafer', 'waffle', 'waffles', 'wait', 'waiting', 'wall', 'wallpaper', 'walnut', 'walnuts', 'wamed', 'want', 'warm', 'warmed', 'warming', 'warn', 'wasabi', 'wash', 'washed', 'washing', 'washy', 'watch', 'watching', 'wate', 'water', 'watercress', 'wateriness', 'watermelon', 'watery', 'wave', 'wavy', 'wax', 'waxed', 'waxy', 'way', 'ways', 'wear', 'wearing', 'weave', 'wedge', 'wedges', 'wee', 'week', 'weekly', 'weeks', 'weigh', 'weighing', 'weight', 'weights', 'well', 'wellington', 'wellingtons', 'wells', 'welly', 'welsh', 'wensleydale', 'wet', 'wetted', 'wetting', 'whack', 'whatever', 'wheat', 'wheatgerm', 'wheel', 'whenever', 'whether', 'whichever', 'whilst', 'whip', 'whipped', 'whipping', 'whips', 'whirlpool', 'whisk', 'whisked', 'whiskers', 'whisking', 'whisks', 'whisky', 'white', 'whitecurrants', 'whites', 'whiz', 'whizz', 'whizzing', 'whole', 'wholegrain', 'wholemeal', 'whose', 'wide', 'widely', 'widened', 'wider', 'widest', 'width', 'widthways', 'wild', 'wilt', 'wilted', 'wilts', 'windows', 'wine', 'wing', 'wings', 'winter', 'wipe', 'wire', 'wish', 'wishbone', 'wishy', 'witha', 'within', 'without', 'witth', 'wobble', 'wobbles', 'wobbly', 'wodge', 'wok', 'wonderful', 'wonders', 'wonton', 'wontons', 'wood', 'wooden', 'woody', 'worcestershire', 'word', 'words', 'work', 'worked', 'working', 'works', 'worktop', 'worry', 'worth', 'would', 'woven', 'wrap', 'wrapped', 'wrapper', 'wrappers', 'wrapping', 'wraps', 'wreath', 'wring', 'wrinkle', 'wrinkles', 'wrists', 'writing', 'wth', 'xanthan', 'xanthum', 'xcm', 'xcs', 'xd', 'xin', 'xml', 'xre', 'xs', 'xt', 'xx', 'xxcm', 'xxin', 'yaki', 'yakitori', 'year', 'yeast', 'yeasty', 'yellow', 'yet', 'yield', 'yielded', 'yoghurt', 'yogurt', 'yolk', 'yolks', 'yorkshire', 'yout', 'yung', 'yuzu', 'za', 'zag', 'zags', 'zest', 'zested', 'zester', 'zests', 'zesty', 'zig', 'zigzag', 'zucchini']\n"
     ]
    }
   ],
   "source": [
    "corpus = cld_withphotos\n",
    "tf_vectorizer = TfidfVectorizer(stop_words=nltkstopwords) #using extended stopwords\n",
    "tokens_tfidf = tf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(tf_vectorizer.get_feature_names()) #add stemmer too?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:25.691774Z",
     "start_time": "2019-12-09T00:32:22.502483Z"
    }
   },
   "outputs": [],
   "source": [
    "#more cleaning! stemming?\n",
    "#TFIDF\n",
    "nmf_10 = NMF(10) \n",
    "withphotos_tf_nmf_10 = nmf_10.fit_transform(tokens_tfidf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **NMF topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:25.720017Z",
     "start_time": "2019-12-09T00:32:25.693987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "cook, heat, simmer, stirring, bring, boil, stock, stir, water, saucepan, rice, onion, sauce, pan, pasta, reduce, tender, lid, garlic, season\n",
      "\n",
      "Topic  1\n",
      "chocolate, sugar, whisk, mixture, cream, bowl, egg, set, cool, baking, oven, butter, pour, melted, meringue, place, heat, milk, vanilla, water\n",
      "\n",
      "Topic  2\n",
      "pastry, pie, filling, roll, tart, case, egg, brush, oven, edges, tin, cm, beaten, baking, fridge, bake, cut, mixture, line, surface\n",
      "\n",
      "Topic  3\n",
      "dough, flour, surface, baking, knead, roll, floured, work, bowl, place, lightly, yeast, leave, tray, onto, oven, mix, together, size, shape\n",
      "\n",
      "Topic  4\n",
      "chicken, marinade, breasts, cooked, pieces, thighs, breast, juices, marinate, place, ingredients, clear, cover, stock, skin, sauce, pan, barbecue, rice, run\n",
      "\n",
      "Topic  5\n",
      "pepper, salad, olive, place, dressing, season, salt, bowl, lemon, oil, mix, black, drizzle, freshly, juice, fish, ground, well, ingredients, taste\n",
      "\n",
      "Topic  6\n",
      "cake, tin, icing, beat, cakes, cool, sugar, cm, skewer, oven, tins, top, grease, mixture, butter, rack, inserted, line, comes, spread\n",
      "\n",
      "Topic  7\n",
      "wok, noodles, sauce, stir, fry, soy, rice, spring, lime, chilli, coriander, oil, fish, paste, ginger, onions, heat, sesame, minute, prawns\n",
      "\n",
      "Topic  8\n",
      "potatoes, lamb, oven, dish, beef, meat, casserole, roasting, gas, roast, preheat, place, potato, brown, vegetables, tin, mash, tender, pork, hours\n",
      "\n",
      "Topic  9\n",
      "pan, heat, fry, frying, oil, cook, medium, brown, golden, hot, side, mushrooms, set, batter, crisp, cooked, aside, salt, high, kitchen\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_10, tf_vectorizer.get_feature_names(), 20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics look good!\n",
    "\n",
    "tfidf+nmf with extended stopwords (minutes, add, remove)\n",
    "\n",
    "* Topic 0 roast - potatoes, lamb, beef, pork\n",
    "* Topic 1 dessert - chocolate, meringue\n",
    "* Topic 2 dessert - pastry, pie, tart\n",
    "* Topic 3 bread (dough, yeast, knead, roll, shape - no sugar)\n",
    "* Topic 4 chicken\n",
    "* Topic 5 salad\n",
    "* Topic 6 cake\n",
    "* Topic 7 Asian - curry, rice, stir-fry\n",
    "* Topic 8 pasta\n",
    "* Topic 9 pan-fry (european?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (Previously, tfidf+ nmf, 10 topics without extended stopwords )\n",
    "    * Topic 0 rice stir fry\n",
    "    * Topic 1 dessert - chocolate, meringue\n",
    "    * Topic 2 dessert - pastry, pie, tart\n",
    "    * Topic 3 dough, yeast (bread?)\n",
    "    * Topic 4 chicken\n",
    "    * Topic 5 salad\n",
    "    * Topic 6 cake\n",
    "    * Topic 7 Asian, noodles (wok, fry, soy, seasame) \n",
    "    * Topic 8 pasta, spaghetti\n",
    "    * Topic 9 potatoes, lamb roast, beef, pork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:25.727016Z",
     "start_time": "2019-12-09T00:32:25.722477Z"
    }
   },
   "outputs": [],
   "source": [
    "#restarted kernel, order of topics changed? one topic content changed also, the rest were the sam\n",
    "\n",
    "#next? more cleaning? - topics look good, except for stir and stirring in topic 0, tin and tins in Topic 6, not much need for stemmer\n",
    "    #can still add stemmer to improve it\n",
    "    #also, can add minute/minutes/hours to stopwords\n",
    "    \n",
    "#cluster on topics? in order to assign new image to cluster - can we expect the clusters to match the topics?\n",
    "\n",
    "#Clustering - if use Kmeans, need to convert and approx cosine similarity\n",
    "    #then predict which cluster the image belongs to; \n",
    "    # also need to predict/assign cluster to each image in dataset (could be slow)\n",
    "    # Kmeans assigns one doc to one cluster only, \"hard\"\n",
    "    # \"Fuzzy Kmeans\" gives probability (documentation minimal, run on cmd line?)\n",
    "#should be able to do this with topics also?\n",
    "    # topic modelling, each document has topic weights, can assign to max-weighted topic; LDA?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get doc-topic matrix \n",
    "to use NMF topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:25.741941Z",
     "start_time": "2019-12-09T00:32:25.729765Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1                                          15 minute pasta\n",
       "2                                             3D biscuits \n",
       "3                                  2-hour Christmas dinner\n",
       "7        A classic sponge cake (with passion fruit fill...\n",
       "18               Albanian baked lamb with rice (Tavë kosi)\n",
       "                               ...                        \n",
       "10574                                   Yorkshire puddings\n",
       "10576                           Yorkshire ‘tapas’ puddings\n",
       "10582                                             Yule log\n",
       "10585                             Za’atar cod with relish \n",
       "10589                                Zesty tofu cheesecake\n",
       "Name: title, Length: 2225, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "withphotos.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Set index to original index - *Not recipe ID!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:25.750512Z",
     "start_time": "2019-12-09T00:32:25.744078Z"
    }
   },
   "outputs": [],
   "source": [
    "#index = withphotos.title  #instead of changing index from ID to title, better to use ID, or keep both?\n",
    "index = withphotos.index\n",
    "columns = ['topic ' + str(num) for num in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:32:25.801646Z",
     "start_time": "2019-12-09T00:32:25.752909Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic 0</th>\n",
       "      <th>topic 1</th>\n",
       "      <th>topic 2</th>\n",
       "      <th>topic 3</th>\n",
       "      <th>topic 4</th>\n",
       "      <th>topic 5</th>\n",
       "      <th>topic 6</th>\n",
       "      <th>topic 7</th>\n",
       "      <th>topic 8</th>\n",
       "      <th>topic 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.03491</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.05341</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.15217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03898</td>\n",
       "      <td>0.00568</td>\n",
       "      <td>0.06684</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.04154</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02900</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.05478</td>\n",
       "      <td>0.00824</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02322</td>\n",
       "      <td>0.00574</td>\n",
       "      <td>0.03262</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.17753</td>\n",
       "      <td>0.01710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.06505</td>\n",
       "      <td>0.00689</td>\n",
       "      <td>0.01557</td>\n",
       "      <td>0.00073</td>\n",
       "      <td>0.03536</td>\n",
       "      <td>0.09565</td>\n",
       "      <td>0.00641</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.06629</td>\n",
       "      <td>0.00546</td>\n",
       "      <td>0.00220</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00519</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.16475</td>\n",
       "      <td>0.06193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10574</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.05277</td>\n",
       "      <td>0.00224</td>\n",
       "      <td>0.02373</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00771</td>\n",
       "      <td>0.00681</td>\n",
       "      <td>0.00064</td>\n",
       "      <td>0.08445</td>\n",
       "      <td>0.01913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10576</td>\n",
       "      <td>0.03424</td>\n",
       "      <td>0.05877</td>\n",
       "      <td>0.01129</td>\n",
       "      <td>0.01175</td>\n",
       "      <td>0.05156</td>\n",
       "      <td>0.01469</td>\n",
       "      <td>0.01892</td>\n",
       "      <td>0.01663</td>\n",
       "      <td>0.05066</td>\n",
       "      <td>0.04952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10582</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.05960</td>\n",
       "      <td>0.01715</td>\n",
       "      <td>0.02681</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.15664</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10585</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00647</td>\n",
       "      <td>0.01219</td>\n",
       "      <td>0.08390</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01125</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.06765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10589</td>\n",
       "      <td>0.00274</td>\n",
       "      <td>0.04956</td>\n",
       "      <td>0.00213</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01208</td>\n",
       "      <td>0.03089</td>\n",
       "      <td>0.02671</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic 0  topic 1  topic 2  topic 3  topic 4  topic 5  topic 6  topic 7  \\\n",
       "1      0.03491  0.00000  0.00000  0.00000  0.00000  0.05341  0.00000  0.00000   \n",
       "2      0.00000  0.03898  0.00568  0.06684  0.00000  0.00000  0.04154  0.00000   \n",
       "3      0.05478  0.00824  0.00000  0.02322  0.00574  0.03262  0.00000  0.00000   \n",
       "7      0.00000  0.06505  0.00689  0.01557  0.00073  0.03536  0.09565  0.00641   \n",
       "18     0.06629  0.00546  0.00220  0.00000  0.00000  0.00519  0.00000  0.00000   \n",
       "...        ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "10574  0.00000  0.05277  0.00224  0.02373  0.00000  0.00771  0.00681  0.00064   \n",
       "10576  0.03424  0.05877  0.01129  0.01175  0.05156  0.01469  0.01892  0.01663   \n",
       "10582  0.00000  0.05960  0.01715  0.02681  0.00000  0.00000  0.15664  0.00000   \n",
       "10585  0.00000  0.00000  0.00000  0.00647  0.01219  0.08390  0.00000  0.01125   \n",
       "10589  0.00274  0.04956  0.00213  0.00000  0.00000  0.01208  0.03089  0.02671   \n",
       "\n",
       "       topic 8  topic 9  \n",
       "1      0.00000  0.15217  \n",
       "2      0.02900  0.00000  \n",
       "3      0.17753  0.01710  \n",
       "7      0.00000  0.00000  \n",
       "18     0.16475  0.06193  \n",
       "...        ...      ...  \n",
       "10574  0.08445  0.01913  \n",
       "10576  0.05066  0.04952  \n",
       "10582  0.00000  0.00000  \n",
       "10585  0.00000  0.06765  \n",
       "10589  0.00000  0.00000  \n",
       "\n",
       "[2225 rows x 10 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#withphotos_tf_nmf_10 = nmf_10.fit_transform(tokens_tfidf) \n",
    "doc_topic = pd.DataFrame(withphotos_tf_nmf_10.round(5),index,columns)\n",
    "doc_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Assign document to topic with highest weight\n",
    "*alternative is clustering in topic space, documents are points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:51:12.435038Z",
     "start_time": "2019-12-09T00:51:12.408445Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic 0</th>\n",
       "      <th>topic 1</th>\n",
       "      <th>topic 2</th>\n",
       "      <th>topic 3</th>\n",
       "      <th>topic 4</th>\n",
       "      <th>topic 5</th>\n",
       "      <th>topic 6</th>\n",
       "      <th>topic 7</th>\n",
       "      <th>topic 8</th>\n",
       "      <th>topic 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.03491</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.05341</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.15217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03898</td>\n",
       "      <td>0.00568</td>\n",
       "      <td>0.06684</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.04154</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02900</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.05478</td>\n",
       "      <td>0.00824</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02322</td>\n",
       "      <td>0.00574</td>\n",
       "      <td>0.03262</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.17753</td>\n",
       "      <td>0.01710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.06505</td>\n",
       "      <td>0.00689</td>\n",
       "      <td>0.01557</td>\n",
       "      <td>0.00073</td>\n",
       "      <td>0.03536</td>\n",
       "      <td>0.09565</td>\n",
       "      <td>0.00641</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.06629</td>\n",
       "      <td>0.00546</td>\n",
       "      <td>0.00220</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00519</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.16475</td>\n",
       "      <td>0.06193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic 0  topic 1  topic 2  topic 3  topic 4  topic 5  topic 6  topic 7  \\\n",
       "1   0.03491  0.00000  0.00000  0.00000  0.00000  0.05341  0.00000  0.00000   \n",
       "2   0.00000  0.03898  0.00568  0.06684  0.00000  0.00000  0.04154  0.00000   \n",
       "3   0.05478  0.00824  0.00000  0.02322  0.00574  0.03262  0.00000  0.00000   \n",
       "7   0.00000  0.06505  0.00689  0.01557  0.00073  0.03536  0.09565  0.00641   \n",
       "18  0.06629  0.00546  0.00220  0.00000  0.00000  0.00519  0.00000  0.00000   \n",
       "\n",
       "    topic 8  topic 9  \n",
       "1   0.00000  0.15217  \n",
       "2   0.02900  0.00000  \n",
       "3   0.17753  0.01710  \n",
       "7   0.00000  0.00000  \n",
       "18  0.16475  0.06193  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#numpy.argmax -- Returns the indices of the maximum values along an axis. Default index is into the flattened array (axis=None, works on entire array)\n",
    "#returns first occurrence if max occurs multiple times\n",
    "#*may need to reshape?\n",
    "\n",
    "#test on small df first\n",
    "test = doc_topic.head()\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:51:14.260561Z",
     "start_time": "2019-12-09T00:51:14.251921Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 3, 8, 6, 8])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.array(test), axis = 1) #works! #.reshape(5,10) not necessary in this case\n",
    "#topics changed after changing index from title to ID?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:51:16.224485Z",
     "start_time": "2019-12-09T00:51:16.217206Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T21:27:37.474785Z",
     "start_time": "2019-12-06T21:27:37.460163Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "7\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "for row in test.transpose():\n",
    "    print(row) #index is ID (prev index =title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:51:18.588761Z",
     "start_time": "2019-12-09T00:51:18.580592Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic 0    0.03491\n",
       "topic 1    0.00000\n",
       "topic 2    0.00000\n",
       "topic 3    0.00000\n",
       "topic 4    0.00000\n",
       "topic 5    0.05341\n",
       "topic 6    0.00000\n",
       "topic 7    0.00000\n",
       "topic 8    0.00000\n",
       "topic 9    0.15217\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.transpose().iloc[:,0] #prints first row**\n",
    "#test.transpose().iloc[0] prints the 0th column! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:52:31.157574Z",
     "start_time": "2019-12-09T00:52:31.145884Z"
    }
   },
   "outputs": [],
   "source": [
    "def assign_topic(df):\n",
    "    '''For each row in df, finds the topic with highest weight, \n",
    "    assign that topic label to the row.\n",
    "    Returns a dataframe''' #df easiest to access, will group recipes and images by labels later\n",
    "    label_list = []\n",
    "    for i in range(len(df)): #easier way to do this?\n",
    "        topic_label = np.argmax(np.array(df.transpose().iloc[:,i]), axis = 0)\n",
    "        label_list.append(topic_label)\n",
    "    return pd.DataFrame(label_list, index = index, columns = ['topics'])#columns needs to be a list\n",
    "    #better if make df outside of function?\n",
    "    #np.argmax(np.array(test), axis = 1)\n",
    "#edit ftn before pickling*\n",
    "\n",
    "#index = withphotos.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:52:34.317563Z",
     "start_time": "2019-12-09T00:52:32.544631Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10574</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10576</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10582</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10585</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10589</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       topics\n",
       "1           9\n",
       "2           3\n",
       "3           8\n",
       "7           6\n",
       "18          8\n",
       "...       ...\n",
       "10574       8\n",
       "10576       1\n",
       "10582       6\n",
       "10585       5\n",
       "10589       1\n",
       "\n",
       "[2225 rows x 1 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#assign_topic(test)\n",
    "labelled = assign_topic(doc_topic)\n",
    "labelled  #is 1 coln df enough? use other data structure? save as sparse matrix instead?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:53:23.986798Z",
     "start_time": "2019-12-09T00:53:23.971513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10574</td>\n",
       "      <td>8</td>\n",
       "      <td>10574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10576</td>\n",
       "      <td>1</td>\n",
       "      <td>10576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10582</td>\n",
       "      <td>6</td>\n",
       "      <td>10582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10585</td>\n",
       "      <td>5</td>\n",
       "      <td>10585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10589</td>\n",
       "      <td>1</td>\n",
       "      <td>10589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       topics    idx\n",
       "1           9      1\n",
       "2           3      2\n",
       "3           8      3\n",
       "7           6      7\n",
       "18          8     18\n",
       "...       ...    ...\n",
       "10574       8  10574\n",
       "10576       1  10576\n",
       "10582       6  10582\n",
       "10585       5  10585\n",
       "10589       1  10589\n",
       "\n",
       "[2225 rows x 2 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled['idx'] = labelled.index  #*not necessary? this id is not the unique recipe ID*!\n",
    "labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T21:27:40.132147Z",
     "start_time": "2019-12-06T21:27:40.125719Z"
    }
   },
   "outputs": [],
   "source": [
    "#if use this data structure, need to loop through 10 times to save each topic label as a list? to make list of lists?\n",
    "#instead, just look up topic label from df? (could be slow on app? but dataset not big)\n",
    "\n",
    "#pseudocode: (more complicated than I expected?)\n",
    "\n",
    "    #give each topic label a summary (2.3.3, 2-word max?), provide those to the user on app (drop down list? or type in box with drop-down)\n",
    "    #write ftn that takes that word as input, and returns a list of all recipe titles of that label\n",
    "    \n",
    "    #another ftn: *combine labelled sublist with image data - match each recipe with its image \n",
    "        #(*what's the fastest way? image title has title with underscore and recipe ID - should keep ID from original data (not a column)?)\n",
    "        #returns a list of images (titles?), use those to find features\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Label names of 10 topics:**\n",
    "\n",
    "```\n",
    "* Topic 0 roast - potatoes, lamb, beef, pork\n",
    "* Topic 1 dessert - chocolate, meringue\n",
    "* Topic 2 dessert - pastry, pie, tart\n",
    "* Topic 3 bread \n",
    "* Topic 4 chicken\n",
    "* Topic 5 salad\n",
    "* Topic 6 cake\n",
    "* Topic 7 Asian - curry, rice, stir-fry\n",
    "* Topic 8 pasta\n",
    "* Topic 9 pan fry\n",
    "```\n",
    "(combine the 2 dessert topics?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:52:42.369037Z",
     "start_time": "2019-12-09T00:52:42.359629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.indexes.numeric.Int64Index"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_coln = withphotos.index\n",
    "type(idx_coln) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find documents of the selected topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:53:29.394169Z",
     "start_time": "2019-12-09T00:53:29.386374Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_topic_num(name):\n",
    "    '''For a given label name (out of 10 choices), returns the topic number'''\n",
    "    label_names_dict = {'roast':0,\n",
    "                   'dessert (chocolate)':1, \n",
    "                   'dessert (pastry, pie)':2, \n",
    "                   'bread':3,\n",
    "                   'chicken':4,\n",
    "                   'salad':5,\n",
    "                   'cake':6,\n",
    "                   'Asian':7,\n",
    "                   'pasta':8,\n",
    "                   'pan fry':9} #*change to floats for combined?\n",
    "    # dictionary --keys=names, values=topic number; match label names with topic number in doc_topic and labelled df\n",
    "    if name in label_names_dict: #check user input is key in dict\n",
    "        topic_num = label_names_dict[name]#get matching values(topic number) \n",
    "        return topic_num #=6 for cake, works\n",
    "\n",
    "def get_recipes_by_topic(num, data):\n",
    "    '''Takes in the topic number (0 to 9),\n",
    "    returns a list of IDs of all recipe titles assigned to that topic\n",
    "    (those that have the label as the topic with maximum weight from NMF topic modelling)'''\n",
    "    docs_same_label = []\n",
    "    #result = [f(x) for x in df['col']] better and faster than iter ftns\n",
    "    #result = [f(x, y) for x, y in zip(df['col1'], df['col2'])]\n",
    "    for topicn, idn in zip(data['topics'], data['idx']):\n",
    "        if topicn == num:\n",
    "            docs_same_label.append(idn)\n",
    "    return docs_same_label\n",
    "#better to cluster than assigning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:53:00.479672Z",
     "start_time": "2019-12-09T00:53:00.474030Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_topic_num('roast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:53:01.895316Z",
     "start_time": "2019-12-09T00:53:01.888867Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topics    6\n",
       "Name: 7, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check classic sponge cake recipe (topic 6, cake)\n",
    "labelled.iloc[3] #name is index, can't use loc on it as a regular column?\n",
    "#iloc[0], expect to get 8 for pasta?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:53:34.807596Z",
     "start_time": "2019-12-09T00:53:34.799470Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cakes = get_recipes_by_topic(float(6), labelled)\n",
    "#a list of all recipe ids that are cakes\n",
    "len(cakes) #restart kernel, length increased?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:53:43.241108Z",
     "start_time": "2019-12-09T00:53:43.234820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2097"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#^^^debug\n",
    "cakes[40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Match docs with selected topic to their images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean image titles to extract IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T21:27:40.367303Z",
     "start_time": "2019-12-06T21:27:40.362458Z"
    }
   },
   "outputs": [],
   "source": [
    "#!problem: ID in image title is not the same as index in recipe? recipe id max at 10589 (10589 text recipes)\n",
    "#but images have a much larger ID (dep on how many are scraped from website?)\n",
    "#how to match recipes to image?\n",
    "\n",
    "#in Cosine_Similarity code, using index in bbc_list (order of images in directory) \n",
    "    #In first nb-Feature_Extractor code, also following that order - but not matching because recipes with no images don't exist in features.csv\n",
    "    #image index in FtExtr 0-2225, but how to get recipe withphotos text index (1,2,3,7,18) to each image? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* clean images and use 4-5 digit number as recipe ID\n",
    "* add column for URL to text df\n",
    "* add extract the same recipe ID number as ID column to doc_topic and labelled dfs\n",
    "* *add recipe ID column to FeatureExtractor code, extract features again (2k, <1hr locally?)*\n",
    "* join id-topic text df with id-features image df\n",
    "* can then filter recipe and images by topic!\n",
    "\n",
    "*allrecipes set has ID as image title, can probably use same code to match those features with recipe text - is it worth it?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:53:56.769224Z",
     "start_time": "2019-12-09T00:53:56.635907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2225"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbcimg_list = !ls '/Users/xinrucheng/Documents/Metis_bootcamp/week_9/project5data/2017_140k/recipe_photos/bbc_photos/pages-photos/'\n",
    "len(bbcimg_list) #doesn't work if use !ls img_path variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:53:57.992000Z",
     "start_time": "2019-12-09T00:53:57.982787Z"
    }
   },
   "outputs": [],
   "source": [
    "#example image title: _chicken_chasseur_with_19163_16x9.jpg\n",
    "def clean_img_title(imglist):\n",
    "    '''use regex to clean image titles to get unique recipe ID,\n",
    "    imglist is a list of image title strings\n",
    "    Returns a dataframe where the column ID is integer IDs, \n",
    "    and the column title is the original image title''' \n",
    "    import re\n",
    "    ID_list = []\n",
    "    title_list = []\n",
    "    for filename in imglist:\n",
    "        title_list.append(filename)\n",
    "        #x = re.split(\"_\", filename)\n",
    "        ids = re.findall(r\"_(\\d+)_\", str(filename))\n",
    "        #print(ids)\n",
    "        ID_list.append(ids)\n",
    "        \n",
    "    imgID_df = pd.DataFrame(ID_list, columns=['ID', 'title']) #1 col vs 2 col error* made extra column of None\n",
    "    imgID_df['title'] = title_list\n",
    "\n",
    "    return imgID_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:53:59.753897Z",
     "start_time": "2019-12-09T00:53:59.720201Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testpath = '/Users/xinrucheng/Documents/Metis_bootcamp/week_9/metis_passion_project/data/raw/testimages/bbctest'\n",
    "\n",
    "test_list = !ls '/Users/xinrucheng/Documents/Metis_bootcamp/week_9/metis_passion_project/data/raw/testimages/bbctest'\n",
    "len(test_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:54:01.264498Z",
     "start_time": "2019-12-09T00:54:01.258675Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'baileysandchocolatec_72293_16x9.jpg'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T21:27:41.066706Z",
     "start_time": "2019-12-06T21:27:41.061591Z"
    }
   },
   "outputs": [],
   "source": [
    "#clean_img_title(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T21:27:41.074054Z",
     "start_time": "2019-12-06T21:27:41.069401Z"
    }
   },
   "outputs": [],
   "source": [
    "#regex notes\n",
    "#re.findall returns a list of matches (Return an empty list if no match was found), better than re.search (returns a match object)\n",
    "#regex - () for group it captures and returns! can still keep str structure outside of group to help it search\n",
    "#https://regex101.com/#python  (regex expr above test str box, colour-coded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:54:04.134419Z",
     "start_time": "2019-12-09T00:54:04.128025Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'15_minute_pasta_33407_16x9.jpg'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbcimg_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:54:06.577408Z",
     "start_time": "2019-12-09T00:54:06.548369Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>33407</td>\n",
       "      <td>15_minute_pasta_33407_16x9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>79341</td>\n",
       "      <td>2_hour_christmas_dinner_79341_16x9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>29555</td>\n",
       "      <td>3d_biscuits_29555_16x9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>19163</td>\n",
       "      <td>_chicken_chasseur_with_19163_16x9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>49934</td>\n",
       "      <td>_schichttorte_49934_16x9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>93848</td>\n",
       "      <td>yorkshirepudding_93848_16x9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2221</td>\n",
       "      <td>86010</td>\n",
       "      <td>yorkshirepuddings_86010_16x9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2222</td>\n",
       "      <td>15656</td>\n",
       "      <td>yule_log_15656_16x9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2223</td>\n",
       "      <td>98478</td>\n",
       "      <td>zaatar_cod_with_relish_98478_16x9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2224</td>\n",
       "      <td>84103</td>\n",
       "      <td>zesty_tofu_cheesecake_84103_16x9.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                   title\n",
       "0     33407          15_minute_pasta_33407_16x9.jpg\n",
       "1     79341  2_hour_christmas_dinner_79341_16x9.jpg\n",
       "2     29555              3d_biscuits_29555_16x9.jpg\n",
       "3     19163   _chicken_chasseur_with_19163_16x9.jpg\n",
       "4     49934            _schichttorte_49934_16x9.jpg\n",
       "...     ...                                     ...\n",
       "2220  93848         yorkshirepudding_93848_16x9.jpg\n",
       "2221  86010        yorkshirepuddings_86010_16x9.jpg\n",
       "2222  15656                 yule_log_15656_16x9.jpg\n",
       "2223  98478   zaatar_cod_with_relish_98478_16x9.jpg\n",
       "2224  84103    zesty_tofu_cheesecake_84103_16x9.jpg\n",
       "\n",
       "[2225 rows x 2 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_ids = clean_img_title(bbcimg_list) #worked for smaller test folder?, not sure where extra column comes from - bbc_list and test_list not the same dimensions\n",
    "img_ids #column 0 is id, drop col 1 by index but not by name?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get ID from URL column in text df\n",
    "extract the same recipe ID number as ID column in 'doc_topic' df and 'labelled' df (2.3.4.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:54:10.377591Z",
     "start_time": "2019-12-09T00:54:10.305420Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chef</th>\n",
       "      <th>chef_id</th>\n",
       "      <th>cooking_time_minutes</th>\n",
       "      <th>description</th>\n",
       "      <th>error</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>instructions</th>\n",
       "      <th>instructions_detailed</th>\n",
       "      <th>photo_url</th>\n",
       "      <th>preparation_time_minutes</th>\n",
       "      <th>program</th>\n",
       "      <th>program_id</th>\n",
       "      <th>serves</th>\n",
       "      <th>time_scraped</th>\n",
       "      <th>title</th>\n",
       "      <th>total_time_minutes</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Mary Berry</td>\n",
       "      <td>mary_berry</td>\n",
       "      <td>0</td>\n",
       "      <td>This is my standby pasta supper as it is so de...</td>\n",
       "      <td>False</td>\n",
       "      <td>[350g/12oz penne pasta, 2 x 80g/3oz packs Parm...</td>\n",
       "      <td>[Cook the pasta in a pan of boiling salted wat...</td>\n",
       "      <td>[{'ingredient': 'pasta', 'line': '350g/12oz pe...</td>\n",
       "      <td>http://ichef.bbci.co.uk/food/ic/food_16x9_608/...</td>\n",
       "      <td>30</td>\n",
       "      <td>Mary Berry Cooks</td>\n",
       "      <td>p01s4q10</td>\n",
       "      <td>6</td>\n",
       "      <td>1499227763</td>\n",
       "      <td>15 minute pasta</td>\n",
       "      <td>30</td>\n",
       "      <td>http://bbc.co.uk/food/recipes/15_minute_pasta_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Mari Williams</td>\n",
       "      <td>mari_williams</td>\n",
       "      <td>0</td>\n",
       "      <td>Simple 3D iced biscuits inspired by Bake Off. ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[200g/7oz unsalted soft butter, 200g/7oz caste...</td>\n",
       "      <td>[To make the basic dough, line a baking tray w...</td>\n",
       "      <td>[{'ingredient': 'butter', 'line': '200g/7oz un...</td>\n",
       "      <td>http://ichef.bbci.co.uk/food/ic/food_16x9_608/...</td>\n",
       "      <td>30</td>\n",
       "      <td>The Great British Bake Off</td>\n",
       "      <td>b013pqnm</td>\n",
       "      <td>0</td>\n",
       "      <td>1499227766</td>\n",
       "      <td>3D biscuits</td>\n",
       "      <td>30</td>\n",
       "      <td>http://bbc.co.uk/food/recipes/3d_biscuits_29555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Justine Pattison</td>\n",
       "      <td>justine_pattison</td>\n",
       "      <td>0</td>\n",
       "      <td>This easy turkey crown recipe is served with s...</td>\n",
       "      <td>False</td>\n",
       "      <td>[2.5kg/5lb 8oz turkey crown (fully thawed if f...</td>\n",
       "      <td>[Preheat the oven to 220C/200C Fan/Gas 7., For...</td>\n",
       "      <td>[{'ingredient': 'turkey', 'line': '2.5kg/5lb 8...</td>\n",
       "      <td>http://ichef.bbci.co.uk/food/ic/food_16x9_608/...</td>\n",
       "      <td>30</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>1499227765</td>\n",
       "      <td>2-hour Christmas dinner</td>\n",
       "      <td>30</td>\n",
       "      <td>http://bbc.co.uk/food/recipes/2_hour_christmas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Delia Smith</td>\n",
       "      <td>delia_smith</td>\n",
       "      <td>30</td>\n",
       "      <td>Delia shows you how to bake a perfect sponge c...</td>\n",
       "      <td>False</td>\n",
       "      <td>[175g/6oz self-raising flour, 1 rounded tsp ba...</td>\n",
       "      <td>[Preheat the oven to 170C/325F/Gas 3., Take a ...</td>\n",
       "      <td>[{'ingredient': 'self-raising flour', 'line': ...</td>\n",
       "      <td>http://ichef.bbci.co.uk/food/ic/food_16x9_608/...</td>\n",
       "      <td>30</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td>1499227775</td>\n",
       "      <td>A classic sponge cake (with passion fruit fill...</td>\n",
       "      <td>60</td>\n",
       "      <td>http://bbc.co.uk/food/recipes/aclassicspongeca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Rick Stein</td>\n",
       "      <td>rick_stein</td>\n",
       "      <td>0</td>\n",
       "      <td>Tavë kosi is a national dish in Albania, but I...</td>\n",
       "      <td>False</td>\n",
       "      <td>[70g/2½oz butter, 1 tbsp olive oil, 1.2kg/2lb ...</td>\n",
       "      <td>[Preheat the oven to 180C/160C Fan/Gas 4., Hea...</td>\n",
       "      <td>[{'ingredient': 'butter', 'line': '70g/2½oz bu...</td>\n",
       "      <td>http://ichef.bbci.co.uk/food/ic/food_16x9_608/...</td>\n",
       "      <td>30</td>\n",
       "      <td>Rick Stein: From Venice to Istanbul</td>\n",
       "      <td>b0667qf6</td>\n",
       "      <td>8</td>\n",
       "      <td>1499227798</td>\n",
       "      <td>Albanian baked lamb with rice (Tavë kosi)</td>\n",
       "      <td>30</td>\n",
       "      <td>http://bbc.co.uk/food/recipes/albanian_baked_l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                chef           chef_id  cooking_time_minutes  \\\n",
       "1         Mary Berry        mary_berry                     0   \n",
       "2      Mari Williams     mari_williams                     0   \n",
       "3   Justine Pattison  justine_pattison                     0   \n",
       "7        Delia Smith       delia_smith                    30   \n",
       "18        Rick Stein        rick_stein                     0   \n",
       "\n",
       "                                          description  error  \\\n",
       "1   This is my standby pasta supper as it is so de...  False   \n",
       "2   Simple 3D iced biscuits inspired by Bake Off. ...  False   \n",
       "3   This easy turkey crown recipe is served with s...  False   \n",
       "7   Delia shows you how to bake a perfect sponge c...  False   \n",
       "18  Tavë kosi is a national dish in Albania, but I...  False   \n",
       "\n",
       "                                          ingredients  \\\n",
       "1   [350g/12oz penne pasta, 2 x 80g/3oz packs Parm...   \n",
       "2   [200g/7oz unsalted soft butter, 200g/7oz caste...   \n",
       "3   [2.5kg/5lb 8oz turkey crown (fully thawed if f...   \n",
       "7   [175g/6oz self-raising flour, 1 rounded tsp ba...   \n",
       "18  [70g/2½oz butter, 1 tbsp olive oil, 1.2kg/2lb ...   \n",
       "\n",
       "                                         instructions  \\\n",
       "1   [Cook the pasta in a pan of boiling salted wat...   \n",
       "2   [To make the basic dough, line a baking tray w...   \n",
       "3   [Preheat the oven to 220C/200C Fan/Gas 7., For...   \n",
       "7   [Preheat the oven to 170C/325F/Gas 3., Take a ...   \n",
       "18  [Preheat the oven to 180C/160C Fan/Gas 4., Hea...   \n",
       "\n",
       "                                instructions_detailed  \\\n",
       "1   [{'ingredient': 'pasta', 'line': '350g/12oz pe...   \n",
       "2   [{'ingredient': 'butter', 'line': '200g/7oz un...   \n",
       "3   [{'ingredient': 'turkey', 'line': '2.5kg/5lb 8...   \n",
       "7   [{'ingredient': 'self-raising flour', 'line': ...   \n",
       "18  [{'ingredient': 'butter', 'line': '70g/2½oz bu...   \n",
       "\n",
       "                                            photo_url  \\\n",
       "1   http://ichef.bbci.co.uk/food/ic/food_16x9_608/...   \n",
       "2   http://ichef.bbci.co.uk/food/ic/food_16x9_608/...   \n",
       "3   http://ichef.bbci.co.uk/food/ic/food_16x9_608/...   \n",
       "7   http://ichef.bbci.co.uk/food/ic/food_16x9_608/...   \n",
       "18  http://ichef.bbci.co.uk/food/ic/food_16x9_608/...   \n",
       "\n",
       "    preparation_time_minutes                              program program_id  \\\n",
       "1                         30                     Mary Berry Cooks   p01s4q10   \n",
       "2                         30           The Great British Bake Off   b013pqnm   \n",
       "3                         30                                 None       None   \n",
       "7                         30                                 None       None   \n",
       "18                        30  Rick Stein: From Venice to Istanbul   b0667qf6   \n",
       "\n",
       "    serves  time_scraped                                              title  \\\n",
       "1        6    1499227763                                    15 minute pasta   \n",
       "2        0    1499227766                                       3D biscuits    \n",
       "3        6    1499227765                            2-hour Christmas dinner   \n",
       "7        8    1499227775  A classic sponge cake (with passion fruit fill...   \n",
       "18       8    1499227798          Albanian baked lamb with rice (Tavë kosi)   \n",
       "\n",
       "    total_time_minutes                                                url  \n",
       "1                   30  http://bbc.co.uk/food/recipes/15_minute_pasta_...  \n",
       "2                   30    http://bbc.co.uk/food/recipes/3d_biscuits_29555  \n",
       "3                   30  http://bbc.co.uk/food/recipes/2_hour_christmas...  \n",
       "7                   60  http://bbc.co.uk/food/recipes/aclassicspongeca...  \n",
       "18                  30  http://bbc.co.uk/food/recipes/albanian_baked_l...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "withphotos.head() #df with recipes that don't have photos dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:54:12.867105Z",
     "start_time": "2019-12-09T00:54:12.830037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2225 entries, 1 to 10589\n",
      "Data columns (total 17 columns):\n",
      "chef                        2225 non-null object\n",
      "chef_id                     2225 non-null object\n",
      "cooking_time_minutes        2225 non-null int64\n",
      "description                 2225 non-null object\n",
      "error                       2225 non-null bool\n",
      "ingredients                 2225 non-null object\n",
      "instructions                2225 non-null object\n",
      "instructions_detailed       2225 non-null object\n",
      "photo_url                   2225 non-null object\n",
      "preparation_time_minutes    2225 non-null int64\n",
      "program                     1699 non-null object\n",
      "program_id                  1699 non-null object\n",
      "serves                      2225 non-null int64\n",
      "time_scraped                2225 non-null int64\n",
      "title                       2225 non-null object\n",
      "total_time_minutes          2225 non-null int64\n",
      "url                         2225 non-null object\n",
      "dtypes: bool(1), int64(5), object(11)\n",
      "memory usage: 297.7+ KB\n"
     ]
    }
   ],
   "source": [
    "withphotos.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:54:16.346481Z",
     "start_time": "2019-12-09T00:54:16.338309Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        http://bbc.co.uk/food/recipes/15_minute_pasta_...\n",
       "2          http://bbc.co.uk/food/recipes/3d_biscuits_29555\n",
       "3        http://bbc.co.uk/food/recipes/2_hour_christmas...\n",
       "7        http://bbc.co.uk/food/recipes/aclassicspongeca...\n",
       "18       http://bbc.co.uk/food/recipes/albanian_baked_l...\n",
       "                               ...                        \n",
       "10574    http://bbc.co.uk/food/recipes/yorkshirepudding...\n",
       "10576    http://bbc.co.uk/food/recipes/yorkshire_tapas_...\n",
       "10582         http://bbc.co.uk/food/recipes/yule_log_15656\n",
       "10585    http://bbc.co.uk/food/recipes/zaatar_cod_with_...\n",
       "10589    http://bbc.co.uk/food/recipes/zesty_tofu_chees...\n",
       "Name: url, Length: 2225, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_col = withphotos['url']\n",
    "url_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:54:18.444776Z",
     "start_time": "2019-12-09T00:54:18.438664Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(url_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:54:20.248321Z",
     "start_time": "2019-12-09T00:54:20.241726Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(url_col.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:54:44.108336Z",
     "start_time": "2019-12-09T00:54:44.102158Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://bbc.co.uk/food/recipes/toms_turkey_roll_with_44115'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#^^^debug\n",
    "url_col.iloc[2097]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:54:49.064676Z",
     "start_time": "2019-12-09T00:54:49.057777Z"
    }
   },
   "outputs": [],
   "source": [
    "#use regex to clean url:\n",
    "#example url  'http://bbc.co.uk/food/recipes/3d_biscuits_29555'\n",
    "    # test regex: \\W\\w+_(\\d+) looks into last segment (incl '/' and finds 29555)\n",
    "\n",
    "def get_id_from_url(col):\n",
    "    '''use regex to clean urls to get unique recipe ID for text recipes\n",
    "    imglist is a list of image title strings\n",
    "    Returns a dataframe where one column is integer IDs\n",
    "    the other is the corresponding recipe title''' #may not need\n",
    "    ID_list = []\n",
    "    recipe_list = [] \n",
    "    for url in col:\n",
    "        ids = re.findall(r\"\\W\\w+_(\\d+)\", str(url))\n",
    "        #print(ids)\n",
    "        ID_list.append(ids)\n",
    "        recipes = re.findall(r\"\\W(\\w+)_\\d+\", str(url))\n",
    "        recipe_list.append(recipes)\n",
    "        \n",
    "    imgID_df = pd.DataFrame(ID_list, columns=['id']) #, title_list,, 'title'])\n",
    "    imgID_df['short_title'] = recipe_list\n",
    "\n",
    "    return imgID_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:54:51.791154Z",
     "start_time": "2019-12-09T00:54:51.730865Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>short_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>33407</td>\n",
       "      <td>[15_minute_pasta]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>29555</td>\n",
       "      <td>[3d_biscuits]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>79341</td>\n",
       "      <td>[2_hour_christmas_dinner]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9406</td>\n",
       "      <td>[aclassicspongecakewi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>92485</td>\n",
       "      <td>[albanian_baked_lamb_with]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>86010</td>\n",
       "      <td>[yorkshirepuddings]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2221</td>\n",
       "      <td>93245</td>\n",
       "      <td>[yorkshire_tapas_puddings]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2222</td>\n",
       "      <td>15656</td>\n",
       "      <td>[yule_log]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2223</td>\n",
       "      <td>98478</td>\n",
       "      <td>[zaatar_cod_with_relish]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2224</td>\n",
       "      <td>84103</td>\n",
       "      <td>[zesty_tofu_cheesecake]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                 short_title\n",
       "0     33407           [15_minute_pasta]\n",
       "1     29555               [3d_biscuits]\n",
       "2     79341   [2_hour_christmas_dinner]\n",
       "3      9406      [aclassicspongecakewi]\n",
       "4     92485  [albanian_baked_lamb_with]\n",
       "...     ...                         ...\n",
       "2220  86010         [yorkshirepuddings]\n",
       "2221  93245  [yorkshire_tapas_puddings]\n",
       "2222  15656                  [yule_log]\n",
       "2223  98478    [zaatar_cod_with_relish]\n",
       "2224  84103     [zesty_tofu_cheesecake]\n",
       "\n",
       "[2225 rows x 2 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_title_df = get_id_from_url(url_col)\n",
    "id_title_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "Dec 6 debugging corrupted data after merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:54:56.099281Z",
     "start_time": "2019-12-09T00:54:56.090902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             object\n",
       "short_title    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_title_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:54:57.743560Z",
     "start_time": "2019-12-09T00:54:57.732884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: short_title, dtype: object)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ID = 494 and 1495\n",
    "\n",
    "id_title_df.loc[id_title_df['id']=='494','short_title'] #Pandas indexing!blog?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:55:52.052374Z",
     "start_time": "2019-12-09T00:55:52.043984Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                           79152\n",
       "short_title    [chocolate_roulade]\n",
       "Name: 494, dtype: object"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#^^^debug\n",
    "id_title_df.loc[494] #by index, not by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:55:13.249336Z",
     "start_time": "2019-12-09T00:55:13.231167Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short_title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>33407</td>\n",
       "      <td>[15_minute_pasta]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29555</td>\n",
       "      <td>[3d_biscuits]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79341</td>\n",
       "      <td>[2_hour_christmas_dinner]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9406</td>\n",
       "      <td>[aclassicspongecakewi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92485</td>\n",
       "      <td>[albanian_baked_lamb_with]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86010</td>\n",
       "      <td>[yorkshirepuddings]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93245</td>\n",
       "      <td>[yorkshire_tapas_puddings]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15656</td>\n",
       "      <td>[yule_log]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98478</td>\n",
       "      <td>[zaatar_cod_with_relish]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84103</td>\n",
       "      <td>[zesty_tofu_cheesecake]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      short_title\n",
       "id                               \n",
       "33407           [15_minute_pasta]\n",
       "29555               [3d_biscuits]\n",
       "79341   [2_hour_christmas_dinner]\n",
       "9406       [aclassicspongecakewi]\n",
       "92485  [albanian_baked_lamb_with]\n",
       "...                           ...\n",
       "86010         [yorkshirepuddings]\n",
       "93245  [yorkshire_tapas_puddings]\n",
       "15656                  [yule_log]\n",
       "98478    [zaatar_cod_with_relish]\n",
       "84103     [zesty_tofu_cheesecake]\n",
       "\n",
       "[2225 rows x 1 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reset index to unique ID to locate corrupted row by ID - need to change dtype before assignment??\n",
    "id_title_df_newID = id_title_df.set_index(['id']) \n",
    "id_title_df_newID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:56:21.843616Z",
     "start_time": "2019-12-09T00:56:21.813503Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot do label indexing on <class 'pandas.core.indexes.base.Index'> with these indexers [494] of <class 'int'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-2783d4d99a3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mid_title_df_newID\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m494\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/anaconda/envs/metis/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1424\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/anaconda/envs/metis/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0;31m# fall thru to straight lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/anaconda/envs/metis/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_key\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1724\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1725\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1727\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/anaconda/envs/metis/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_scalar_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;31m# a scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert_slice_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/anaconda/envs/metis/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_convert_scalar_indexer\u001b[0;34m(self, key, kind)\u001b[0m\n\u001b[1;32m   3136\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"loc\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3137\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3138\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalid_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/anaconda/envs/metis/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_invalid_indexer\u001b[0;34m(self, form, key)\u001b[0m\n\u001b[1;32m   3338\u001b[0m             \u001b[0;34m\"cannot do {form} indexing on {klass} with these \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3339\u001b[0m             \"indexers [{key}] of {kind}\".format(\n\u001b[0;32m-> 3340\u001b[0;31m                 \u001b[0mform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3341\u001b[0m             )\n\u001b[1;32m   3342\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot do label indexing on <class 'pandas.core.indexes.base.Index'> with these indexers [494] of <class 'int'>"
     ]
    }
   ],
   "source": [
    "#id_title_df_newID.loc[494]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:56:35.631300Z",
     "start_time": "2019-12-09T00:56:35.624446Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "short_title    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_title_df_newID.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:56:37.088489Z",
     "start_time": "2019-12-09T00:56:37.081820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([33407, 29555, 79341,  9406, 92485, 75464, 12416,  3274, 21317,\n",
       "            86765,\n",
       "            ...\n",
       "            91487, 74830, 20002, 69240, 93848, 86010, 93245, 15656, 98478,\n",
       "            84103],\n",
       "           dtype='int64', name='id', length=2225)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_title_df_newID.index.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:56:39.232771Z",
     "start_time": "2019-12-09T00:56:39.225880Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "short_title    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_title_df_newID.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:58:25.031126Z",
     "start_time": "2019-12-09T00:58:25.017235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                           79152\n",
       "short_title    [chocolate_roulade]\n",
       "Name: 494, dtype: object"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "id_title_df.iloc[494] #still by index, not by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:58:26.606529Z",
     "start_time": "2019-12-09T00:58:26.599340Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                           79152\n",
       "short_title    [chocolate_roulade]\n",
       "Name: 494, dtype: object"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_title_df.loc[494] \n",
    "#^^^debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add IDs to doc_topic and labelled topic df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:49:27.857571Z",
     "start_time": "2019-12-06T02:49:27.838241Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic 0</th>\n",
       "      <th>topic 1</th>\n",
       "      <th>topic 2</th>\n",
       "      <th>topic 3</th>\n",
       "      <th>topic 4</th>\n",
       "      <th>topic 5</th>\n",
       "      <th>topic 6</th>\n",
       "      <th>topic 7</th>\n",
       "      <th>topic 8</th>\n",
       "      <th>topic 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00372</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.24902</td>\n",
       "      <td>0.12035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.01707</td>\n",
       "      <td>0.03746</td>\n",
       "      <td>0.00612</td>\n",
       "      <td>0.06794</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.04312</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.13109</td>\n",
       "      <td>0.01019</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02241</td>\n",
       "      <td>0.00662</td>\n",
       "      <td>0.04167</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02045</td>\n",
       "      <td>0.02462</td>\n",
       "      <td>0.02640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.06281</td>\n",
       "      <td>0.00652</td>\n",
       "      <td>0.01441</td>\n",
       "      <td>0.00045</td>\n",
       "      <td>0.04389</td>\n",
       "      <td>0.09654</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.11935</td>\n",
       "      <td>0.00865</td>\n",
       "      <td>0.00292</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00399</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.07147</td>\n",
       "      <td>0.00792</td>\n",
       "      <td>0.05999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic 0  topic 1  topic 2  topic 3  topic 4  topic 5  topic 6  topic 7  \\\n",
       "1   0.00000  0.00000  0.00000  0.00000  0.00000  0.00372  0.00000  0.00000   \n",
       "2   0.01707  0.03746  0.00612  0.06794  0.00000  0.00000  0.04312  0.00000   \n",
       "3   0.13109  0.01019  0.00000  0.02241  0.00662  0.04167  0.00000  0.02045   \n",
       "7   0.00000  0.06281  0.00652  0.01441  0.00045  0.04389  0.09654  0.00000   \n",
       "18  0.11935  0.00865  0.00292  0.00000  0.00000  0.00399  0.00000  0.07147   \n",
       "\n",
       "    topic 8  topic 9  \n",
       "1   0.24902  0.12035  \n",
       "2   0.00000  0.00000  \n",
       "3   0.02462  0.02640  \n",
       "7   0.00000  0.00000  \n",
       "18  0.00792  0.05999  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from section 2.3.4.1: doc_topic = pd.DataFrame(withphotos_tf_nmf_10.round(5),index,columns)\n",
    "#index = withphotos.index\n",
    "#columns = ['topic ' + str(num) for num in range(10)]\n",
    "doc_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:49:27.890661Z",
     "start_time": "2019-12-06T02:49:27.860207Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic 0</th>\n",
       "      <th>topic 1</th>\n",
       "      <th>topic 2</th>\n",
       "      <th>topic 3</th>\n",
       "      <th>topic 4</th>\n",
       "      <th>topic 5</th>\n",
       "      <th>topic 6</th>\n",
       "      <th>topic 7</th>\n",
       "      <th>topic 8</th>\n",
       "      <th>topic 9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>33407</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00372</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.24902</td>\n",
       "      <td>0.12035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29555</td>\n",
       "      <td>0.01707</td>\n",
       "      <td>0.03746</td>\n",
       "      <td>0.00612</td>\n",
       "      <td>0.06794</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.04312</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79341</td>\n",
       "      <td>0.13109</td>\n",
       "      <td>0.01019</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02241</td>\n",
       "      <td>0.00662</td>\n",
       "      <td>0.04167</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02045</td>\n",
       "      <td>0.02462</td>\n",
       "      <td>0.02640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9406</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.06281</td>\n",
       "      <td>0.00652</td>\n",
       "      <td>0.01441</td>\n",
       "      <td>0.00045</td>\n",
       "      <td>0.04389</td>\n",
       "      <td>0.09654</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92485</td>\n",
       "      <td>0.11935</td>\n",
       "      <td>0.00865</td>\n",
       "      <td>0.00292</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00399</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.07147</td>\n",
       "      <td>0.00792</td>\n",
       "      <td>0.05999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic 0  topic 1  topic 2  topic 3  topic 4  topic 5  topic 6  topic 7  \\\n",
       "id                                                                              \n",
       "33407  0.00000  0.00000  0.00000  0.00000  0.00000  0.00372  0.00000  0.00000   \n",
       "29555  0.01707  0.03746  0.00612  0.06794  0.00000  0.00000  0.04312  0.00000   \n",
       "79341  0.13109  0.01019  0.00000  0.02241  0.00662  0.04167  0.00000  0.02045   \n",
       "9406   0.00000  0.06281  0.00652  0.01441  0.00045  0.04389  0.09654  0.00000   \n",
       "92485  0.11935  0.00865  0.00292  0.00000  0.00000  0.00399  0.00000  0.07147   \n",
       "\n",
       "       topic 8  topic 9  \n",
       "id                       \n",
       "33407  0.24902  0.12035  \n",
       "29555  0.00000  0.00000  \n",
       "79341  0.02462  0.02640  \n",
       "9406   0.00000  0.00000  \n",
       "92485  0.00792  0.05999  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_ids = id_title_df['id']\n",
    "\n",
    "doc_id_topic = pd.DataFrame(withphotos_tf_nmf_10.round(5), index=index_ids, columns=columns)\n",
    "doc_id_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:49:27.905398Z",
     "start_time": "2019-12-06T02:49:27.893327Z"
    }
   },
   "outputs": [],
   "source": [
    "def assign_topic_new(df):\n",
    "    '''For each row in df, finds the topic with highest weight, \n",
    "    assign that topic label to the row.\n",
    "    Returns a dataframe''' #df easiest to access, will group recipes and images by labels later\n",
    "    label_list = []\n",
    "    for i in range(len(df)): #easier way to do this?\n",
    "        topic_label = np.argmax(np.array(df.transpose().iloc[:,i]), axis = 0)\n",
    "        label_list.append(topic_label)\n",
    "    return pd.DataFrame(label_list, index = index_ids, columns = ['topics'])#columns needs to be a list\n",
    "    #better if make df outside of function?\n",
    "    #np.argmax(np.array(test), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:49:29.034515Z",
     "start_time": "2019-12-06T02:49:27.908320Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>33407</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29555</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79341</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9406</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86010</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93245</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15656</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98478</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84103</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       topics\n",
       "id           \n",
       "33407       8\n",
       "29555       3\n",
       "79341       0\n",
       "9406        6\n",
       "92485       0\n",
       "...       ...\n",
       "86010       1\n",
       "93245       1\n",
       "15656       6\n",
       "98478       5\n",
       "84103       1\n",
       "\n",
       "[2225 rows x 1 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_topic_labelled = assign_topic_new(doc_id_topic)\n",
    "id_topic_labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:49:29.042679Z",
     "start_time": "2019-12-06T02:49:29.037088Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(id_topic_labelled) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:49:29.060339Z",
     "start_time": "2019-12-06T02:49:29.045178Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>33407</td>\n",
       "      <td>8</td>\n",
       "      <td>33407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29555</td>\n",
       "      <td>3</td>\n",
       "      <td>29555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79341</td>\n",
       "      <td>0</td>\n",
       "      <td>79341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9406</td>\n",
       "      <td>6</td>\n",
       "      <td>9406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92485</td>\n",
       "      <td>0</td>\n",
       "      <td>92485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86010</td>\n",
       "      <td>1</td>\n",
       "      <td>86010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93245</td>\n",
       "      <td>1</td>\n",
       "      <td>93245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15656</td>\n",
       "      <td>6</td>\n",
       "      <td>15656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98478</td>\n",
       "      <td>5</td>\n",
       "      <td>98478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84103</td>\n",
       "      <td>1</td>\n",
       "      <td>84103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       topics     ID\n",
       "id                  \n",
       "33407       8  33407\n",
       "29555       3  29555\n",
       "79341       0  79341\n",
       "9406        6   9406\n",
       "92485       0  92485\n",
       "...       ...    ...\n",
       "86010       1  86010\n",
       "93245       1  93245\n",
       "15656       6  15656\n",
       "98478       5  98478\n",
       "84103       1  84103\n",
       "\n",
       "[2225 rows x 2 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dec 5 - add additional ID column for joining \n",
    "id_topic_labelled['ID']=id_topic_labelled.index\n",
    "id_topic_labelled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add recipe IDs to features.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:49:35.090162Z",
     "start_time": "2019-12-06T02:49:29.063043Z"
    }
   },
   "outputs": [],
   "source": [
    "#update Feature_Extractor-Copy1 code: Feature_Extractor_IDs\n",
    "bbc_ft_id = pd.read_csv('/Users/xinrucheng/Documents/Metis_bootcamp/week_9/metis_passion_project/data/processed/bbc_ft_id.csv')\n",
    "#Dec 4 10pm updated csv with ID as index\n",
    "\n",
    "#csv with image features, IDs are assumed to be in the same order as images before feature extraction** \n",
    "#(if incorrect, need to extract features again)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:49:35.135790Z",
     "start_time": "2019-12-06T02:49:35.093275Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>4086</th>\n",
       "      <th>4087</th>\n",
       "      <th>4088</th>\n",
       "      <th>4089</th>\n",
       "      <th>4090</th>\n",
       "      <th>4091</th>\n",
       "      <th>4092</th>\n",
       "      <th>4093</th>\n",
       "      <th>4094</th>\n",
       "      <th>4095</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>33407</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.509541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.018812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.679488</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>79341</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017876</td>\n",
       "      <td>2.738089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.499179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.004055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.274214</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>29555</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.795153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.155841</td>\n",
       "      <td>0.043871</td>\n",
       "      <td>2.865308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.179471</td>\n",
       "      <td>0.549198</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>19163</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.468124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.004137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.093263</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>49934</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.590657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.397941</td>\n",
       "      <td>1.178361</td>\n",
       "      <td>0.598724</td>\n",
       "      <td>2.223511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4098 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  Unnamed: 0         0         1         2         3         4  \\\n",
       "0  33407           0  0.000000  3.509541  0.000000  0.000000  0.000000   \n",
       "1  79341           1  0.017876  2.738089  0.000000  0.000000  2.499179   \n",
       "2  29555           2  0.000000  0.000000  0.000000  1.795153  0.000000   \n",
       "3  19163           3  0.000000  0.468124  0.000000  0.000000  0.000000   \n",
       "4  49934           4  0.000000  0.000000  2.590657  0.000000  0.000000   \n",
       "\n",
       "          5         6         7  ...      4086      4087  4088  4089  4090  \\\n",
       "0  0.000000  0.000000  0.000000  ...  0.000000  0.000000   0.0   0.0   0.0   \n",
       "1  0.000000  0.000000  0.000000  ...  0.525963  0.000000   0.0   0.0   0.0   \n",
       "2  1.155841  0.043871  2.865308  ...  0.000000  0.000000   0.0   0.0   0.0   \n",
       "3  0.000000  0.000000  0.000000  ...  0.000000  0.055124   0.0   0.0   0.0   \n",
       "4  0.000000  0.000000  0.000000  ...  0.000000  0.000000   0.0   0.0   0.0   \n",
       "\n",
       "       4091      4092      4093      4094      4095  \n",
       "0  0.000000  1.018812  0.000000  2.679488  0.000000  \n",
       "1  3.004055  0.000000  0.000000  4.274214  0.000000  \n",
       "2  0.232908  0.000000  1.179471  0.549198  0.000000  \n",
       "3  2.004137  0.000000  0.000000  4.093263  0.000000  \n",
       "4  0.000000  3.397941  1.178361  0.598724  2.223511  \n",
       "\n",
       "[5 rows x 4098 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_ft_id.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Join id_topic (text) and id_features (img) on IDs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:49:35.141877Z",
     "start_time": "2019-12-06T02:49:35.138076Z"
    }
   },
   "outputs": [],
   "source": [
    "#now both have ID as index, why can't they join?*\n",
    "#alternative is to add id column for both?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:49:35.151788Z",
     "start_time": "2019-12-06T02:49:35.144458Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['33407', '29555', '79341', '9406', '92485', '75464', '12416', '03274',\n",
       "       '21317', '86765',\n",
       "       ...\n",
       "       '91487', '74830', '20002', '69240', '93848', '86010', '93245', '15656',\n",
       "       '98478', '84103'],\n",
       "      dtype='object', name='id', length=2225)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_topic_labelled.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:49:35.170095Z",
     "start_time": "2019-12-06T02:49:35.154385Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=2225, step=1)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_ft_id.index #not displaying index as above, index or object type not right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:49:35.191703Z",
     "start_time": "2019-12-06T02:49:35.175931Z"
    }
   },
   "outputs": [],
   "source": [
    "#bbc_ft_id._set_index('ID') #set index on more time, order slightly different - do they need to be swapped when joining on ID?\n",
    "#bbc_ft_id.index #should delete ID column by default? still there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:49:35.306102Z",
     "start_time": "2019-12-06T02:49:35.202065Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>4086</th>\n",
       "      <th>4087</th>\n",
       "      <th>4088</th>\n",
       "      <th>4089</th>\n",
       "      <th>4090</th>\n",
       "      <th>4091</th>\n",
       "      <th>4092</th>\n",
       "      <th>4093</th>\n",
       "      <th>4094</th>\n",
       "      <th>4095</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>33407</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.509541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.018812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.679488</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>79341</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017876</td>\n",
       "      <td>2.738089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.499179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.004055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.274214</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>29555</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.795153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.155841</td>\n",
       "      <td>0.043871</td>\n",
       "      <td>2.865308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.179471</td>\n",
       "      <td>0.549198</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>19163</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.468124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.004137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.093263</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>49934</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.590657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.397941</td>\n",
       "      <td>1.178361</td>\n",
       "      <td>0.598724</td>\n",
       "      <td>2.223511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4098 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  Unnamed: 0         0         1         2         3         4  \\\n",
       "0  33407           0  0.000000  3.509541  0.000000  0.000000  0.000000   \n",
       "1  79341           1  0.017876  2.738089  0.000000  0.000000  2.499179   \n",
       "2  29555           2  0.000000  0.000000  0.000000  1.795153  0.000000   \n",
       "3  19163           3  0.000000  0.468124  0.000000  0.000000  0.000000   \n",
       "4  49934           4  0.000000  0.000000  2.590657  0.000000  0.000000   \n",
       "\n",
       "          5         6         7  ...      4086      4087  4088  4089  4090  \\\n",
       "0  0.000000  0.000000  0.000000  ...  0.000000  0.000000   0.0   0.0   0.0   \n",
       "1  0.000000  0.000000  0.000000  ...  0.525963  0.000000   0.0   0.0   0.0   \n",
       "2  1.155841  0.043871  2.865308  ...  0.000000  0.000000   0.0   0.0   0.0   \n",
       "3  0.000000  0.000000  0.000000  ...  0.000000  0.055124   0.0   0.0   0.0   \n",
       "4  0.000000  0.000000  0.000000  ...  0.000000  0.000000   0.0   0.0   0.0   \n",
       "\n",
       "       4091      4092      4093      4094      4095  \n",
       "0  0.000000  1.018812  0.000000  2.679488  0.000000  \n",
       "1  3.004055  0.000000  0.000000  4.274214  0.000000  \n",
       "2  0.232908  0.000000  1.179471  0.549198  0.000000  \n",
       "3  2.004137  0.000000  0.000000  4.093263  0.000000  \n",
       "4  0.000000  3.397941  1.178361  0.598724  2.223511  \n",
       "\n",
       "[5 rows x 4098 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_ft_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:49:35.562332Z",
     "start_time": "2019-12-06T02:49:35.310833Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>idx</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>4086</th>\n",
       "      <th>4087</th>\n",
       "      <th>4088</th>\n",
       "      <th>4089</th>\n",
       "      <th>4090</th>\n",
       "      <th>4091</th>\n",
       "      <th>4092</th>\n",
       "      <th>4093</th>\n",
       "      <th>4094</th>\n",
       "      <th>4095</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>33407</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.509541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.018812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.679488</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>79341</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017876</td>\n",
       "      <td>2.738089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.499179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.004055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.274214</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>29555</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.795153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.155841</td>\n",
       "      <td>0.043871</td>\n",
       "      <td>2.865308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.179471</td>\n",
       "      <td>0.549198</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>19163</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.468124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.004137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.093263</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>49934</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.590657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.397941</td>\n",
       "      <td>1.178361</td>\n",
       "      <td>0.598724</td>\n",
       "      <td>2.223511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>93848</td>\n",
       "      <td>2220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.831429</td>\n",
       "      <td>0.658973</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.089992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.701145</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2221</td>\n",
       "      <td>86010</td>\n",
       "      <td>2221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.034029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.085165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2222</td>\n",
       "      <td>15656</td>\n",
       "      <td>2222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.596903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.511248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2223</td>\n",
       "      <td>98478</td>\n",
       "      <td>2223</td>\n",
       "      <td>1.356347</td>\n",
       "      <td>1.915114</td>\n",
       "      <td>0.181121</td>\n",
       "      <td>1.021998</td>\n",
       "      <td>0.682499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.045778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2224</td>\n",
       "      <td>84103</td>\n",
       "      <td>2224</td>\n",
       "      <td>0.399728</td>\n",
       "      <td>0.824599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.132298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.680424</td>\n",
       "      <td>4.284847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 4098 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID   idx         0         1         2         3         4         5  \\\n",
       "0     33407     0  0.000000  3.509541  0.000000  0.000000  0.000000  0.000000   \n",
       "1     79341     1  0.017876  2.738089  0.000000  0.000000  2.499179  0.000000   \n",
       "2     29555     2  0.000000  0.000000  0.000000  1.795153  0.000000  1.155841   \n",
       "3     19163     3  0.000000  0.468124  0.000000  0.000000  0.000000  0.000000   \n",
       "4     49934     4  0.000000  0.000000  2.590657  0.000000  0.000000  0.000000   \n",
       "...     ...   ...       ...       ...       ...       ...       ...       ...   \n",
       "2220  93848  2220  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2221  86010  2221  0.000000  0.000000  0.080336  0.000000  0.000000  0.000000   \n",
       "2222  15656  2222  0.000000  2.596903  0.000000  0.000000  0.000000  0.000000   \n",
       "2223  98478  2223  1.356347  1.915114  0.181121  1.021998  0.682499  0.000000   \n",
       "2224  84103  2224  0.399728  0.824599  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "             6         7  ...      4086      4087  4088  4089  4090      4091  \\\n",
       "0     0.000000  0.000000  ...  0.000000  0.000000   0.0   0.0   0.0  0.000000   \n",
       "1     0.000000  0.000000  ...  0.525963  0.000000   0.0   0.0   0.0  3.004055   \n",
       "2     0.043871  2.865308  ...  0.000000  0.000000   0.0   0.0   0.0  0.232908   \n",
       "3     0.000000  0.000000  ...  0.000000  0.055124   0.0   0.0   0.0  2.004137   \n",
       "4     0.000000  0.000000  ...  0.000000  0.000000   0.0   0.0   0.0  0.000000   \n",
       "...        ...       ...  ...       ...       ...   ...   ...   ...       ...   \n",
       "2220  0.000000  0.000000  ...  0.831429  0.658973   0.0   0.0   0.0  2.089992   \n",
       "2221  2.034029  0.000000  ...  0.000000  2.085165   0.0   0.0   0.0  0.000000   \n",
       "2222  0.000000  0.000000  ...  1.511248  0.000000   0.0   0.0   0.0  0.000000   \n",
       "2223  0.000000  1.045778  ...  0.000000  0.000000   0.0   0.0   0.0  0.000000   \n",
       "2224  0.000000  0.000000  ...  0.000000  0.000000   0.0   0.0   0.0  0.000000   \n",
       "\n",
       "          4092      4093      4094      4095  \n",
       "0     1.018812  0.000000  2.679488  0.000000  \n",
       "1     0.000000  0.000000  4.274214  0.000000  \n",
       "2     0.000000  1.179471  0.549198  0.000000  \n",
       "3     0.000000  0.000000  4.093263  0.000000  \n",
       "4     3.397941  1.178361  0.598724  2.223511  \n",
       "...        ...       ...       ...       ...  \n",
       "2220  0.000000  0.000000  5.701145  0.000000  \n",
       "2221  0.000000  0.000000  0.000000  0.000000  \n",
       "2222  0.000000  0.010338  0.000000  0.000000  \n",
       "2223  0.000000  0.000000  0.000000  0.000000  \n",
       "2224  5.132298  0.000000  3.680424  4.284847  \n",
       "\n",
       "[2225 rows x 4098 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_ft_id.rename(columns={'Unnamed: 0':'idx'}, inplace=True)#rename column\n",
    "bbc_ft_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:49:35.579159Z",
     "start_time": "2019-12-06T02:49:35.567153Z"
    }
   },
   "outputs": [],
   "source": [
    "#bbc_ft_id.drop('idx') #can't drop column after renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:49:35.683819Z",
     "start_time": "2019-12-06T02:49:35.586245Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>idx</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>4086</th>\n",
       "      <th>4087</th>\n",
       "      <th>4088</th>\n",
       "      <th>4089</th>\n",
       "      <th>4090</th>\n",
       "      <th>4091</th>\n",
       "      <th>4092</th>\n",
       "      <th>4093</th>\n",
       "      <th>4094</th>\n",
       "      <th>4095</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>33407</td>\n",
       "      <td>33407</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.509541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.018812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.679488</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79341</td>\n",
       "      <td>79341</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017876</td>\n",
       "      <td>2.738089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.499179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.004055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.274214</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29555</td>\n",
       "      <td>29555</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.795153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.155841</td>\n",
       "      <td>0.043871</td>\n",
       "      <td>2.865308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.179471</td>\n",
       "      <td>0.549198</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19163</td>\n",
       "      <td>19163</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.468124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.004137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.093263</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49934</td>\n",
       "      <td>49934</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.590657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.397941</td>\n",
       "      <td>1.178361</td>\n",
       "      <td>0.598724</td>\n",
       "      <td>2.223511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93848</td>\n",
       "      <td>93848</td>\n",
       "      <td>2220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.831429</td>\n",
       "      <td>0.658973</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.089992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.701145</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86010</td>\n",
       "      <td>86010</td>\n",
       "      <td>2221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.034029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.085165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15656</td>\n",
       "      <td>15656</td>\n",
       "      <td>2222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.596903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.511248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98478</td>\n",
       "      <td>98478</td>\n",
       "      <td>2223</td>\n",
       "      <td>1.356347</td>\n",
       "      <td>1.915114</td>\n",
       "      <td>0.181121</td>\n",
       "      <td>1.021998</td>\n",
       "      <td>0.682499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.045778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84103</td>\n",
       "      <td>84103</td>\n",
       "      <td>2224</td>\n",
       "      <td>0.399728</td>\n",
       "      <td>0.824599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.132298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.680424</td>\n",
       "      <td>4.284847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 4098 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID   idx         0         1         2         3         4  \\\n",
       "ID                                                                     \n",
       "33407  33407     0  0.000000  3.509541  0.000000  0.000000  0.000000   \n",
       "79341  79341     1  0.017876  2.738089  0.000000  0.000000  2.499179   \n",
       "29555  29555     2  0.000000  0.000000  0.000000  1.795153  0.000000   \n",
       "19163  19163     3  0.000000  0.468124  0.000000  0.000000  0.000000   \n",
       "49934  49934     4  0.000000  0.000000  2.590657  0.000000  0.000000   \n",
       "...      ...   ...       ...       ...       ...       ...       ...   \n",
       "93848  93848  2220  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "86010  86010  2221  0.000000  0.000000  0.080336  0.000000  0.000000   \n",
       "15656  15656  2222  0.000000  2.596903  0.000000  0.000000  0.000000   \n",
       "98478  98478  2223  1.356347  1.915114  0.181121  1.021998  0.682499   \n",
       "84103  84103  2224  0.399728  0.824599  0.000000  0.000000  0.000000   \n",
       "\n",
       "              5         6         7  ...      4086      4087  4088  4089  \\\n",
       "ID                                   ...                                   \n",
       "33407  0.000000  0.000000  0.000000  ...  0.000000  0.000000   0.0   0.0   \n",
       "79341  0.000000  0.000000  0.000000  ...  0.525963  0.000000   0.0   0.0   \n",
       "29555  1.155841  0.043871  2.865308  ...  0.000000  0.000000   0.0   0.0   \n",
       "19163  0.000000  0.000000  0.000000  ...  0.000000  0.055124   0.0   0.0   \n",
       "49934  0.000000  0.000000  0.000000  ...  0.000000  0.000000   0.0   0.0   \n",
       "...         ...       ...       ...  ...       ...       ...   ...   ...   \n",
       "93848  0.000000  0.000000  0.000000  ...  0.831429  0.658973   0.0   0.0   \n",
       "86010  0.000000  2.034029  0.000000  ...  0.000000  2.085165   0.0   0.0   \n",
       "15656  0.000000  0.000000  0.000000  ...  1.511248  0.000000   0.0   0.0   \n",
       "98478  0.000000  0.000000  1.045778  ...  0.000000  0.000000   0.0   0.0   \n",
       "84103  0.000000  0.000000  0.000000  ...  0.000000  0.000000   0.0   0.0   \n",
       "\n",
       "       4090      4091      4092      4093      4094      4095  \n",
       "ID                                                             \n",
       "33407   0.0  0.000000  1.018812  0.000000  2.679488  0.000000  \n",
       "79341   0.0  3.004055  0.000000  0.000000  4.274214  0.000000  \n",
       "29555   0.0  0.232908  0.000000  1.179471  0.549198  0.000000  \n",
       "19163   0.0  2.004137  0.000000  0.000000  4.093263  0.000000  \n",
       "49934   0.0  0.000000  3.397941  1.178361  0.598724  2.223511  \n",
       "...     ...       ...       ...       ...       ...       ...  \n",
       "93848   0.0  2.089992  0.000000  0.000000  5.701145  0.000000  \n",
       "86010   0.0  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "15656   0.0  0.000000  0.000000  0.010338  0.000000  0.000000  \n",
       "98478   0.0  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "84103   0.0  0.000000  5.132298  0.000000  3.680424  4.284847  \n",
       "\n",
       "[2225 rows x 4098 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_ft_id.set_index(['ID'], drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:49:35.689980Z",
     "start_time": "2019-12-06T02:49:35.686493Z"
    }
   },
   "outputs": [],
   "source": [
    "#pd.merge(id_topic_labelled, bbc_ft_id, on='ID', how='right')\n",
    "#ValueError: You are trying to merge on object and int64 columns. If you wish to proceed you should use pd.concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:49:35.697066Z",
     "start_time": "2019-12-06T02:49:35.693166Z"
    }
   },
   "outputs": [],
   "source": [
    "#bbc_ft_id.rename(columns={'ID':'id'}, inplace=True)#rename column\n",
    "#bbc_ft_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:49:35.707959Z",
     "start_time": "2019-12-06T02:49:35.699776Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topics     int64\n",
       "ID        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_topic_labelled.dtypes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:49:35.718830Z",
     "start_time": "2019-12-06T02:49:35.710772Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID        int64\n",
       "idx       int64\n",
       "0       float64\n",
       "1       float64\n",
       "2       float64\n",
       "         ...   \n",
       "4091    float64\n",
       "4092    float64\n",
       "4093    float64\n",
       "4094    float64\n",
       "4095    float64\n",
       "Length: 4098, dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_ft_id.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:49:35.725443Z",
     "start_time": "2019-12-06T02:49:35.722071Z"
    }
   },
   "outputs": [],
   "source": [
    "#id_topic_int = id_topic_labelled.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:49:35.731913Z",
     "start_time": "2019-12-06T02:49:35.727986Z"
    }
   },
   "outputs": [],
   "source": [
    "#id_topic_int.join(bbc_ft_id, on='ID', how='left', lsuffix='_left', rsuffix='_right') #Join columns with other DataFrame either on index or on a key column.\n",
    "#join index-on-index by default, ordering? sort? by default=True\n",
    "#why still NaN? should now share index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:49:35.738147Z",
     "start_time": "2019-12-06T02:49:35.734912Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pd.merge(id_topic_labelled.astype(float),bbc_ft_id.astype(float),left_index=True, right_index=True,how='outer') #how='inner' by default\n",
    "#MergeError: No common columns to perform merge on. Merge options: left_on=None, right_on=None, left_index=False, right_index=False\n",
    "#astype didn't help? stuck\n",
    "#when set index = True, how=inner, no overlap (get empty df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Problematic line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:49:36.294880Z",
     "start_time": "2019-12-06T02:49:35.741456Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics</th>\n",
       "      <th>ID</th>\n",
       "      <th>idx</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>4086</th>\n",
       "      <th>4087</th>\n",
       "      <th>4088</th>\n",
       "      <th>4089</th>\n",
       "      <th>4090</th>\n",
       "      <th>4091</th>\n",
       "      <th>4092</th>\n",
       "      <th>4093</th>\n",
       "      <th>4094</th>\n",
       "      <th>4095</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>33407</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.509541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.018812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.679488</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>29555</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.795153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.155841</td>\n",
       "      <td>0.043871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.179471</td>\n",
       "      <td>0.549198</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>79341</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017876</td>\n",
       "      <td>2.738089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.499179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.004055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.274214</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>9406</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.492087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239919</td>\n",
       "      <td>3.340010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.14826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.192654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.489918</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>92485</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.017035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.544566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.973426</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2265</td>\n",
       "      <td>1</td>\n",
       "      <td>86010</td>\n",
       "      <td>2221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.034029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.085165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2266</td>\n",
       "      <td>1</td>\n",
       "      <td>93245</td>\n",
       "      <td>2219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.411434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.053507</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.605391</td>\n",
       "      <td>...</td>\n",
       "      <td>2.162922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.092675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.310158</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2267</td>\n",
       "      <td>6</td>\n",
       "      <td>15656</td>\n",
       "      <td>2222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.596903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.511248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2268</td>\n",
       "      <td>5</td>\n",
       "      <td>98478</td>\n",
       "      <td>2223</td>\n",
       "      <td>1.356347</td>\n",
       "      <td>1.915114</td>\n",
       "      <td>0.181121</td>\n",
       "      <td>1.021998</td>\n",
       "      <td>0.682499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2269</td>\n",
       "      <td>1</td>\n",
       "      <td>84103</td>\n",
       "      <td>2224</td>\n",
       "      <td>0.399728</td>\n",
       "      <td>0.824599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.132298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.680424</td>\n",
       "      <td>4.284847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2270 rows × 4099 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      topics     ID   idx         0         1         2         3         4  \\\n",
       "0          8  33407     0  0.000000  3.509541  0.000000  0.000000  0.000000   \n",
       "1          3  29555     2  0.000000  0.000000  0.000000  1.795153  0.000000   \n",
       "2          0  79341     1  0.017876  2.738089  0.000000  0.000000  2.499179   \n",
       "3          6   9406     5  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4          0  92485     6  0.000000  1.017035  0.000000  0.000000  0.510078   \n",
       "...      ...    ...   ...       ...       ...       ...       ...       ...   \n",
       "2265       1  86010  2221  0.000000  0.000000  0.080336  0.000000  0.000000   \n",
       "2266       1  93245  2219  0.000000  4.411434  0.000000  0.000000  1.053507   \n",
       "2267       6  15656  2222  0.000000  2.596903  0.000000  0.000000  0.000000   \n",
       "2268       5  98478  2223  1.356347  1.915114  0.181121  1.021998  0.682499   \n",
       "2269       1  84103  2224  0.399728  0.824599  0.000000  0.000000  0.000000   \n",
       "\n",
       "             5         6  ...      4086      4087  4088     4089  4090  \\\n",
       "0     0.000000  0.000000  ...  0.000000  0.000000   0.0  0.00000   0.0   \n",
       "1     1.155841  0.043871  ...  0.000000  0.000000   0.0  0.00000   0.0   \n",
       "2     0.000000  0.000000  ...  0.525963  0.000000   0.0  0.00000   0.0   \n",
       "3     0.000000  2.492087  ...  0.239919  3.340010   0.0  2.14826   0.0   \n",
       "4     0.000000  0.000000  ...  0.000000  0.000000   0.0  0.00000   0.0   \n",
       "...        ...       ...  ...       ...       ...   ...      ...   ...   \n",
       "2265  0.000000  2.034029  ...  0.000000  2.085165   0.0  0.00000   0.0   \n",
       "2266  0.000000  0.605391  ...  2.162922  0.000000   0.0  0.00000   0.0   \n",
       "2267  0.000000  0.000000  ...  1.511248  0.000000   0.0  0.00000   0.0   \n",
       "2268  0.000000  0.000000  ...  0.000000  0.000000   0.0  0.00000   0.0   \n",
       "2269  0.000000  0.000000  ...  0.000000  0.000000   0.0  0.00000   0.0   \n",
       "\n",
       "          4091      4092      4093      4094      4095  \n",
       "0     0.000000  1.018812  0.000000  2.679488  0.000000  \n",
       "1     0.232908  0.000000  1.179471  0.549198  0.000000  \n",
       "2     3.004055  0.000000  0.000000  4.274214  0.000000  \n",
       "3     0.000000  2.192654  0.000000  0.489918  0.000000  \n",
       "4     2.544566  0.000000  0.000000  4.973426  0.000000  \n",
       "...        ...       ...       ...       ...       ...  \n",
       "2265  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2266  2.092675  0.000000  0.000000  1.310158  0.000000  \n",
       "2267  0.000000  0.000000  0.010338  0.000000  0.000000  \n",
       "2268  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2269  0.000000  5.132298  0.000000  3.680424  4.284847  \n",
       "\n",
       "[2270 rows x 4099 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this line with merging caused corrupted rows! **get 2270 when should have 2225\n",
    "#*debug by trying to join 2 smaller dfs (5rows each) to see what's happening\n",
    "combined = pd.merge(id_topic_labelled.astype(int), bbc_ft_id, on='ID', how='inner') #expect inner join to give fewer rows?\n",
    "combined\n",
    "\n",
    "#start by separating as type and merge*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:49:36.353290Z",
     "start_time": "2019-12-06T02:49:36.303415Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>33407</td>\n",
       "      <td>8</td>\n",
       "      <td>33407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29555</td>\n",
       "      <td>3</td>\n",
       "      <td>29555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79341</td>\n",
       "      <td>0</td>\n",
       "      <td>79341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9406</td>\n",
       "      <td>6</td>\n",
       "      <td>9406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92485</td>\n",
       "      <td>0</td>\n",
       "      <td>92485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86010</td>\n",
       "      <td>1</td>\n",
       "      <td>86010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93245</td>\n",
       "      <td>1</td>\n",
       "      <td>93245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15656</td>\n",
       "      <td>6</td>\n",
       "      <td>15656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98478</td>\n",
       "      <td>5</td>\n",
       "      <td>98478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84103</td>\n",
       "      <td>1</td>\n",
       "      <td>84103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       topics     ID\n",
       "id                  \n",
       "33407       8  33407\n",
       "29555       3  29555\n",
       "79341       0  79341\n",
       "9406        6   9406\n",
       "92485       0  92485\n",
       "...       ...    ...\n",
       "86010       1  86010\n",
       "93245       1  93245\n",
       "15656       6  15656\n",
       "98478       5  98478\n",
       "84103       1  84103\n",
       "\n",
       "[2225 rows x 2 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_topic_labelled['ID'] = id_topic_labelled['ID'].astype(int)\n",
    "id_topic_labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:49:36.623490Z",
     "start_time": "2019-12-06T02:49:36.356005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics</th>\n",
       "      <th>ID</th>\n",
       "      <th>idx</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>4086</th>\n",
       "      <th>4087</th>\n",
       "      <th>4088</th>\n",
       "      <th>4089</th>\n",
       "      <th>4090</th>\n",
       "      <th>4091</th>\n",
       "      <th>4092</th>\n",
       "      <th>4093</th>\n",
       "      <th>4094</th>\n",
       "      <th>4095</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>33407</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.509541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.018812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.679488</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>29555</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.795153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.155841</td>\n",
       "      <td>0.043871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.179471</td>\n",
       "      <td>0.549198</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>79341</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017876</td>\n",
       "      <td>2.738089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.499179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.004055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.274214</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>9406</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.492087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239919</td>\n",
       "      <td>3.340010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.14826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.192654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.489918</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>92485</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.017035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.544566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.973426</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2265</td>\n",
       "      <td>1</td>\n",
       "      <td>86010</td>\n",
       "      <td>2221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.034029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.085165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2266</td>\n",
       "      <td>1</td>\n",
       "      <td>93245</td>\n",
       "      <td>2219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.411434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.053507</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.605391</td>\n",
       "      <td>...</td>\n",
       "      <td>2.162922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.092675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.310158</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2267</td>\n",
       "      <td>6</td>\n",
       "      <td>15656</td>\n",
       "      <td>2222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.596903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.511248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2268</td>\n",
       "      <td>5</td>\n",
       "      <td>98478</td>\n",
       "      <td>2223</td>\n",
       "      <td>1.356347</td>\n",
       "      <td>1.915114</td>\n",
       "      <td>0.181121</td>\n",
       "      <td>1.021998</td>\n",
       "      <td>0.682499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2269</td>\n",
       "      <td>1</td>\n",
       "      <td>84103</td>\n",
       "      <td>2224</td>\n",
       "      <td>0.399728</td>\n",
       "      <td>0.824599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.132298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.680424</td>\n",
       "      <td>4.284847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2270 rows × 4099 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      topics     ID   idx         0         1         2         3         4  \\\n",
       "0          8  33407     0  0.000000  3.509541  0.000000  0.000000  0.000000   \n",
       "1          3  29555     2  0.000000  0.000000  0.000000  1.795153  0.000000   \n",
       "2          0  79341     1  0.017876  2.738089  0.000000  0.000000  2.499179   \n",
       "3          6   9406     5  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4          0  92485     6  0.000000  1.017035  0.000000  0.000000  0.510078   \n",
       "...      ...    ...   ...       ...       ...       ...       ...       ...   \n",
       "2265       1  86010  2221  0.000000  0.000000  0.080336  0.000000  0.000000   \n",
       "2266       1  93245  2219  0.000000  4.411434  0.000000  0.000000  1.053507   \n",
       "2267       6  15656  2222  0.000000  2.596903  0.000000  0.000000  0.000000   \n",
       "2268       5  98478  2223  1.356347  1.915114  0.181121  1.021998  0.682499   \n",
       "2269       1  84103  2224  0.399728  0.824599  0.000000  0.000000  0.000000   \n",
       "\n",
       "             5         6  ...      4086      4087  4088     4089  4090  \\\n",
       "0     0.000000  0.000000  ...  0.000000  0.000000   0.0  0.00000   0.0   \n",
       "1     1.155841  0.043871  ...  0.000000  0.000000   0.0  0.00000   0.0   \n",
       "2     0.000000  0.000000  ...  0.525963  0.000000   0.0  0.00000   0.0   \n",
       "3     0.000000  2.492087  ...  0.239919  3.340010   0.0  2.14826   0.0   \n",
       "4     0.000000  0.000000  ...  0.000000  0.000000   0.0  0.00000   0.0   \n",
       "...        ...       ...  ...       ...       ...   ...      ...   ...   \n",
       "2265  0.000000  2.034029  ...  0.000000  2.085165   0.0  0.00000   0.0   \n",
       "2266  0.000000  0.605391  ...  2.162922  0.000000   0.0  0.00000   0.0   \n",
       "2267  0.000000  0.000000  ...  1.511248  0.000000   0.0  0.00000   0.0   \n",
       "2268  0.000000  0.000000  ...  0.000000  0.000000   0.0  0.00000   0.0   \n",
       "2269  0.000000  0.000000  ...  0.000000  0.000000   0.0  0.00000   0.0   \n",
       "\n",
       "          4091      4092      4093      4094      4095  \n",
       "0     0.000000  1.018812  0.000000  2.679488  0.000000  \n",
       "1     0.232908  0.000000  1.179471  0.549198  0.000000  \n",
       "2     3.004055  0.000000  0.000000  4.274214  0.000000  \n",
       "3     0.000000  2.192654  0.000000  0.489918  0.000000  \n",
       "4     2.544566  0.000000  0.000000  4.973426  0.000000  \n",
       "...        ...       ...       ...       ...       ...  \n",
       "2265  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2266  2.092675  0.000000  0.000000  1.310158  0.000000  \n",
       "2267  0.000000  0.000000  0.010338  0.000000  0.000000  \n",
       "2268  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2269  0.000000  5.132298  0.000000  3.680424  4.284847  \n",
       "\n",
       "[2270 rows x 4099 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_merged = pd.merge(id_topic_labelled, bbc_ft_id, on='ID', how='inner') #expect inner join to give fewer rows?\n",
    "debug_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:54:47.037700Z",
     "start_time": "2019-12-06T02:54:46.837782Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topics      int64\n",
       "ID          int64\n",
       "idx         int64\n",
       "0         float64\n",
       "1         float64\n",
       "           ...   \n",
       "4091      float64\n",
       "4092      float64\n",
       "4093      float64\n",
       "4094      float64\n",
       "4095      float64\n",
       "Length: 4099, dtype: object"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbg=pd.merge(id_topic_labelled, bbc_ft_id, on='ID', how='inner',left_index=False, right_index=False) \n",
    "dbg.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T02:55:10.360346Z",
     "start_time": "2019-12-06T02:55:10.354252Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2270, 4099)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If `on` is None and not merging on indexes then this defaults to the intersection of the columns in both DataFrames.\n",
    "#only intersecting column should be ID?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^debug\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T23:36:24.722375Z",
     "start_time": "2019-12-05T23:36:24.719027Z"
    }
   },
   "outputs": [],
   "source": [
    "#convert topics and id back to int?\n",
    "#combined['topic_int'] = combined.topics.astype(int)\n",
    "#combined['id'] = combined.id.astype(int)\n",
    "#combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T23:36:26.651641Z",
     "start_time": "2019-12-05T23:36:26.438901Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics</th>\n",
       "      <th>ID</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>4086</th>\n",
       "      <th>4087</th>\n",
       "      <th>4088</th>\n",
       "      <th>4089</th>\n",
       "      <th>4090</th>\n",
       "      <th>4091</th>\n",
       "      <th>4092</th>\n",
       "      <th>4093</th>\n",
       "      <th>4094</th>\n",
       "      <th>4095</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33407.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.509541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.018812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.679488</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29555.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.795153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.155841</td>\n",
       "      <td>0.043871</td>\n",
       "      <td>2.865308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.179471</td>\n",
       "      <td>0.549198</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>79341.0</td>\n",
       "      <td>0.017876</td>\n",
       "      <td>2.738089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.499179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.004055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.274214</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9406.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.492087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239919</td>\n",
       "      <td>3.340010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.14826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.192654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.489918</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>92485.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.017035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.544566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.973426</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2265</td>\n",
       "      <td>7.0</td>\n",
       "      <td>86010.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.034029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.085165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2266</td>\n",
       "      <td>1.0</td>\n",
       "      <td>93245.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.411434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.053507</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.605391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.162922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.092675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.310158</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2267</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15656.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.596903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.511248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2268</td>\n",
       "      <td>5.0</td>\n",
       "      <td>98478.0</td>\n",
       "      <td>1.356347</td>\n",
       "      <td>1.915114</td>\n",
       "      <td>0.181121</td>\n",
       "      <td>1.021998</td>\n",
       "      <td>0.682499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.045778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2269</td>\n",
       "      <td>1.0</td>\n",
       "      <td>84103.0</td>\n",
       "      <td>0.399728</td>\n",
       "      <td>0.824599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.132298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.680424</td>\n",
       "      <td>4.284847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2270 rows × 4098 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      topics       ID         0         1         2         3         4  \\\n",
       "0        0.0  33407.0  0.000000  3.509541  0.000000  0.000000  0.000000   \n",
       "1        3.0  29555.0  0.000000  0.000000  0.000000  1.795153  0.000000   \n",
       "2        7.0  79341.0  0.017876  2.738089  0.000000  0.000000  2.499179   \n",
       "3        6.0   9406.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4        7.0  92485.0  0.000000  1.017035  0.000000  0.000000  0.510078   \n",
       "...      ...      ...       ...       ...       ...       ...       ...   \n",
       "2265     7.0  86010.0  0.000000  0.000000  0.080336  0.000000  0.000000   \n",
       "2266     1.0  93245.0  0.000000  4.411434  0.000000  0.000000  1.053507   \n",
       "2267     6.0  15656.0  0.000000  2.596903  0.000000  0.000000  0.000000   \n",
       "2268     5.0  98478.0  1.356347  1.915114  0.181121  1.021998  0.682499   \n",
       "2269     1.0  84103.0  0.399728  0.824599  0.000000  0.000000  0.000000   \n",
       "\n",
       "             5         6         7  ...      4086      4087  4088     4089  \\\n",
       "0     0.000000  0.000000  0.000000  ...  0.000000  0.000000   0.0  0.00000   \n",
       "1     1.155841  0.043871  2.865308  ...  0.000000  0.000000   0.0  0.00000   \n",
       "2     0.000000  0.000000  0.000000  ...  0.525963  0.000000   0.0  0.00000   \n",
       "3     0.000000  2.492087  0.000000  ...  0.239919  3.340010   0.0  2.14826   \n",
       "4     0.000000  0.000000  0.000000  ...  0.000000  0.000000   0.0  0.00000   \n",
       "...        ...       ...       ...  ...       ...       ...   ...      ...   \n",
       "2265  0.000000  2.034029  0.000000  ...  0.000000  2.085165   0.0  0.00000   \n",
       "2266  0.000000  0.605391  0.000000  ...  2.162922  0.000000   0.0  0.00000   \n",
       "2267  0.000000  0.000000  0.000000  ...  1.511248  0.000000   0.0  0.00000   \n",
       "2268  0.000000  0.000000  1.045778  ...  0.000000  0.000000   0.0  0.00000   \n",
       "2269  0.000000  0.000000  0.000000  ...  0.000000  0.000000   0.0  0.00000   \n",
       "\n",
       "      4090      4091      4092      4093      4094      4095  \n",
       "0      0.0  0.000000  1.018812  0.000000  2.679488  0.000000  \n",
       "1      0.0  0.232908  0.000000  1.179471  0.549198  0.000000  \n",
       "2      0.0  3.004055  0.000000  0.000000  4.274214  0.000000  \n",
       "3      0.0  0.000000  2.192654  0.000000  0.489918  0.000000  \n",
       "4      0.0  2.544566  0.000000  0.000000  4.973426  0.000000  \n",
       "...    ...       ...       ...       ...       ...       ...  \n",
       "2265   0.0  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2266   0.0  2.092675  0.000000  0.000000  1.310158  0.000000  \n",
       "2267   0.0  0.000000  0.000000  0.010338  0.000000  0.000000  \n",
       "2268   0.0  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2269   0.0  0.000000  5.132298  0.000000  3.680424  4.284847  \n",
       "\n",
       "[2270 rows x 4098 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.drop(['idx'],axis=1) #why is idx 6? thought that recipe had no image*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T23:21:24.802570Z",
     "start_time": "2019-12-05T23:20:59.878Z"
    }
   },
   "outputs": [],
   "source": [
    "#keep as float, usable?*\n",
    "#how to index by ID? reset index of combined again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T23:21:24.804744Z",
     "start_time": "2019-12-05T23:20:59.893Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pd.merge(df_a, df_b, on='subject_id', how='left') #if no match to a in b, output null in that cell\n",
    "#pd.merge(bbc_ft_id, id_topic_labelled, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T23:21:24.806896Z",
     "start_time": "2019-12-05T23:20:59.904Z"
    }
   },
   "outputs": [],
   "source": [
    "#pd.concat([id_topic_labelled, bbc_ft_id], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T23:21:24.809382Z",
     "start_time": "2019-12-05T23:20:59.976Z"
    }
   },
   "outputs": [],
   "source": [
    "#id_topic_labelled.join(bbc_ft_id, how='inner')#join using indexes\n",
    "#default how = left, join using topic frame's index (default)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check matched IDs make sense for a few random recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T23:36:52.519105Z",
     "start_time": "2019-12-05T23:36:52.474516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topics        1.000000\n",
       "ID        84103.000000\n",
       "idx        2224.000000\n",
       "0             0.399728\n",
       "1             0.824599\n",
       "              ...     \n",
       "4091          0.000000\n",
       "4092          5.132298\n",
       "4093          0.000000\n",
       "4094          3.680424\n",
       "4095          4.284847\n",
       "Name: 2269, Length: 4099, dtype: float64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 2.3.5.1, 10 topics\n",
    "\n",
    "\n",
    "```\n",
    "* Topic 0 roast - potatoes, lamb, beef, pork\n",
    "* Topic 1 dessert - chocolate, meringue\n",
    "* Topic 2 dessert - pastry, pie, tart\n",
    "* Topic 3 bread \n",
    "* Topic 4 chicken\n",
    "* Topic 5 salad\n",
    "* Topic 6 cake\n",
    "* Topic 7 Asian - curry, rice, stir-fry\n",
    "* Topic 8 pasta\n",
    "* Topic 9 pan fry\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recipe ID 84103 is zesty_tofu_cheesecake_84103_16x9.jpg\n",
    "\n",
    "    - should be dessert or cake (1,2,6)\n",
    "    \n",
    " Assigned to topic 1, ok?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T23:36:58.823628Z",
     "start_time": "2019-12-05T23:36:58.802610Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topics        7.000000\n",
       "ID        92485.000000\n",
       "idx           6.000000\n",
       "0             0.000000\n",
       "1             1.017035\n",
       "              ...     \n",
       "4091          2.544566\n",
       "4092          0.000000\n",
       "4093          0.000000\n",
       "4094          4.973426\n",
       "4095          0.000000\n",
       "Name: 4, Length: 4099, dtype: float64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.iloc[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Albanian baked lamb classified as topic 8 which is pasta, not great, but could be due to not having enough topics\n",
    "(pasta could be baked?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare match results from 2 dataframes with just ID and title:\n",
    "\n",
    "img_ids  and id_title_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T23:37:51.496335Z",
     "start_time": "2019-12-05T23:37:51.439144Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>33407</td>\n",
       "      <td>15_minute_pasta_33407_16x9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>79341</td>\n",
       "      <td>2_hour_christmas_dinner_79341_16x9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>29555</td>\n",
       "      <td>3d_biscuits_29555_16x9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>19163</td>\n",
       "      <td>_chicken_chasseur_with_19163_16x9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>49934</td>\n",
       "      <td>_schichttorte_49934_16x9.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                   title\n",
       "0  33407          15_minute_pasta_33407_16x9.jpg\n",
       "1  79341  2_hour_christmas_dinner_79341_16x9.jpg\n",
       "2  29555              3d_biscuits_29555_16x9.jpg\n",
       "3  19163   _chicken_chasseur_with_19163_16x9.jpg\n",
       "4  49934            _schichttorte_49934_16x9.jpg"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#automate check?\n",
    "img_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T23:37:53.507798Z",
     "start_time": "2019-12-05T23:37:53.475155Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>short_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>33407</td>\n",
       "      <td>[15_minute_pasta]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>29555</td>\n",
       "      <td>[3d_biscuits]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>79341</td>\n",
       "      <td>[2_hour_christmas_dinner]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9406</td>\n",
       "      <td>[aclassicspongecakewi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>92485</td>\n",
       "      <td>[albanian_baked_lamb_with]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                 short_title\n",
       "0  33407           [15_minute_pasta]\n",
       "1  29555               [3d_biscuits]\n",
       "2  79341   [2_hour_christmas_dinner]\n",
       "3   9406      [aclassicspongecakewi]\n",
       "4  92485  [albanian_baked_lamb_with]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_title_df.head() #from 2.3.7.2, ids and titles extracted from text url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T20:19:29.497749Z",
     "start_time": "2019-12-05T20:19:29.493073Z"
    }
   },
   "source": [
    "IDs 33407 and 29555 seem to match, good\n",
    "\n",
    "Also searched in image directory and 92485 is indeed albanian_baked_lamb, consistent with id_title_df from url\n",
    "\n",
    "Can automate this check for larger dataset (later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T23:38:02.184036Z",
     "start_time": "2019-12-05T23:38:02.145879Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topics        6.000000\n",
       "ID        15656.000000\n",
       "idx        2222.000000\n",
       "0             0.000000\n",
       "1             2.596903\n",
       "              ...     \n",
       "4091          0.000000\n",
       "4092          0.000000\n",
       "4093          0.010338\n",
       "4094          0.000000\n",
       "4095          0.000000\n",
       "Name: 2267, Length: 4099, dtype: float64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.iloc[2267]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T23:38:05.316337Z",
     "start_time": "2019-12-05T23:38:05.300643Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                         15656\n",
       "title    yule_log_15656_16x9.jpg\n",
       "Name: 2222, dtype: object"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#are indices idx consistent? idx from image df, can use to check  (if write separate ftn)\n",
    "img_ids.iloc[2222]  #same ID, good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yule log is a cake, classified as topic 6, good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get images from selected topic category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T23:39:59.878300Z",
     "start_time": "2019-12-05T23:39:59.862800Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_recipes_by_topic_ID(num, data):\n",
    "    '''Takes in the topic number (0 to 9),\n",
    "    returns a list of IDs of all recipe titles assigned to that topic\n",
    "    (those that have the label as the topic with maximum weight from NMF topic modelling)'''\n",
    "    docs_same_label = []\n",
    "    #result = [f(x) for x in df['col']] better and faster than iter ftns\n",
    "    #result = [f(x, y) for x, y in zip(df['col1'], df['col2'])]\n",
    "    for topicn, idn in zip(data['topics'], data['ID']): #change id to ID \n",
    "        if topicn == num:\n",
    "            docs_same_label.append(idn)\n",
    "    return docs_same_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T00:30:02.166703Z",
     "start_time": "2019-12-06T00:30:02.143260Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cakes_list = get_recipes_by_topic(6.0, combined)\n",
    "#a list of all recipes IDs that are cakes\n",
    "len(cakes) #still 150 here, where did 150->153 happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T23:40:13.509534Z",
     "start_time": "2019-12-05T23:40:13.494732Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9406.0"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cakes_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T23:40:21.823797Z",
     "start_time": "2019-12-05T23:40:20.607385Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics</th>\n",
       "      <th>idx</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>4086</th>\n",
       "      <th>4087</th>\n",
       "      <th>4088</th>\n",
       "      <th>4089</th>\n",
       "      <th>4090</th>\n",
       "      <th>4091</th>\n",
       "      <th>4092</th>\n",
       "      <th>4093</th>\n",
       "      <th>4094</th>\n",
       "      <th>4095</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>33407.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.509541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.018812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.679488</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29555.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.795153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.155841</td>\n",
       "      <td>0.043871</td>\n",
       "      <td>2.865308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.179471</td>\n",
       "      <td>0.549198</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79341.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.017876</td>\n",
       "      <td>2.738089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.499179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.004055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.274214</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9406.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.492087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239919</td>\n",
       "      <td>3.340010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.14826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.192654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.489918</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92485.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.017035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.544566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.973426</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86010.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2221.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.034029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.085165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93245.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2219.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.411434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.053507</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.605391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.162922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.092675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.310158</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15656.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2222.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.596903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.511248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98478.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2223.0</td>\n",
       "      <td>1.356347</td>\n",
       "      <td>1.915114</td>\n",
       "      <td>0.181121</td>\n",
       "      <td>1.021998</td>\n",
       "      <td>0.682499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.045778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84103.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2224.0</td>\n",
       "      <td>0.399728</td>\n",
       "      <td>0.824599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.132298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.680424</td>\n",
       "      <td>4.284847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2270 rows × 4098 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         topics     idx         0         1         2         3         4  \\\n",
       "ID                                                                          \n",
       "33407.0     0.0     0.0  0.000000  3.509541  0.000000  0.000000  0.000000   \n",
       "29555.0     3.0     2.0  0.000000  0.000000  0.000000  1.795153  0.000000   \n",
       "79341.0     7.0     1.0  0.017876  2.738089  0.000000  0.000000  2.499179   \n",
       "9406.0      6.0     5.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "92485.0     7.0     6.0  0.000000  1.017035  0.000000  0.000000  0.510078   \n",
       "...         ...     ...       ...       ...       ...       ...       ...   \n",
       "86010.0     7.0  2221.0  0.000000  0.000000  0.080336  0.000000  0.000000   \n",
       "93245.0     1.0  2219.0  0.000000  4.411434  0.000000  0.000000  1.053507   \n",
       "15656.0     6.0  2222.0  0.000000  2.596903  0.000000  0.000000  0.000000   \n",
       "98478.0     5.0  2223.0  1.356347  1.915114  0.181121  1.021998  0.682499   \n",
       "84103.0     1.0  2224.0  0.399728  0.824599  0.000000  0.000000  0.000000   \n",
       "\n",
       "                5         6         7  ...      4086      4087  4088     4089  \\\n",
       "ID                                     ...                                      \n",
       "33407.0  0.000000  0.000000  0.000000  ...  0.000000  0.000000   0.0  0.00000   \n",
       "29555.0  1.155841  0.043871  2.865308  ...  0.000000  0.000000   0.0  0.00000   \n",
       "79341.0  0.000000  0.000000  0.000000  ...  0.525963  0.000000   0.0  0.00000   \n",
       "9406.0   0.000000  2.492087  0.000000  ...  0.239919  3.340010   0.0  2.14826   \n",
       "92485.0  0.000000  0.000000  0.000000  ...  0.000000  0.000000   0.0  0.00000   \n",
       "...           ...       ...       ...  ...       ...       ...   ...      ...   \n",
       "86010.0  0.000000  2.034029  0.000000  ...  0.000000  2.085165   0.0  0.00000   \n",
       "93245.0  0.000000  0.605391  0.000000  ...  2.162922  0.000000   0.0  0.00000   \n",
       "15656.0  0.000000  0.000000  0.000000  ...  1.511248  0.000000   0.0  0.00000   \n",
       "98478.0  0.000000  0.000000  1.045778  ...  0.000000  0.000000   0.0  0.00000   \n",
       "84103.0  0.000000  0.000000  0.000000  ...  0.000000  0.000000   0.0  0.00000   \n",
       "\n",
       "         4090      4091      4092      4093      4094      4095  \n",
       "ID                                                               \n",
       "33407.0   0.0  0.000000  1.018812  0.000000  2.679488  0.000000  \n",
       "29555.0   0.0  0.232908  0.000000  1.179471  0.549198  0.000000  \n",
       "79341.0   0.0  3.004055  0.000000  0.000000  4.274214  0.000000  \n",
       "9406.0    0.0  0.000000  2.192654  0.000000  0.489918  0.000000  \n",
       "92485.0   0.0  2.544566  0.000000  0.000000  4.973426  0.000000  \n",
       "...       ...       ...       ...       ...       ...       ...  \n",
       "86010.0   0.0  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "93245.0   0.0  2.092675  0.000000  0.000000  1.310158  0.000000  \n",
       "15656.0   0.0  0.000000  0.000000  0.010338  0.000000  0.000000  \n",
       "98478.0   0.0  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "84103.0   0.0  0.000000  5.132298  0.000000  3.680424  4.284847  \n",
       "\n",
       "[2270 rows x 4098 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_ididx = combined.set_index(['ID'])\n",
    "combined_ididx #2270 rows! expecting 2225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T23:40:41.622347Z",
     "start_time": "2019-12-05T23:40:41.575375Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topics    0.000000\n",
       "idx       0.000000\n",
       "0         0.000000\n",
       "1         3.509541\n",
       "2         0.000000\n",
       "            ...   \n",
       "4091      0.000000\n",
       "4092      1.018812\n",
       "4093      0.000000\n",
       "4094      2.679488\n",
       "4095      0.000000\n",
       "Name: 33407.0, Length: 4098, dtype: float64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testrow0 = combined_ididx.loc[33407] #use loc for index here (by value) instead of iloc\n",
    "testrow0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T23:40:45.935026Z",
     "start_time": "2019-12-05T23:40:45.915343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(testrow0)  #pasta classified as roast (topic 0), might be a case where clustering would help?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T23:40:48.125527Z",
     "start_time": "2019-12-05T23:40:48.095593Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topics    6.000000\n",
       "idx       5.000000\n",
       "0         0.000000\n",
       "1         0.000000\n",
       "2         0.000000\n",
       "            ...   \n",
       "4091      0.000000\n",
       "4092      2.192654\n",
       "4093      0.000000\n",
       "4094      0.489918\n",
       "4095      0.000000\n",
       "Name: 9406.0, Length: 4098, dtype: float64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testrow3 = combined_ididx.loc[9406] #use loc for index here instead of iloc\n",
    "testrow3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T23:41:24.115322Z",
     "start_time": "2019-12-05T23:41:24.100330Z"
    }
   },
   "outputs": [],
   "source": [
    "#cake classified as cake still (topic 6), good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T23:21:24.847968Z",
     "start_time": "2019-12-05T23:21:00.184Z"
    }
   },
   "outputs": [],
   "source": [
    "#need to put all the ftns together in main pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T23:42:00.812588Z",
     "start_time": "2019-12-05T23:41:59.616342Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4088</th>\n",
       "      <th>4089</th>\n",
       "      <th>4090</th>\n",
       "      <th>4091</th>\n",
       "      <th>4092</th>\n",
       "      <th>4093</th>\n",
       "      <th>4094</th>\n",
       "      <th>4095</th>\n",
       "      <th>4096</th>\n",
       "      <th>4097</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.492087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239919</td>\n",
       "      <td>3.340010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.148260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.192654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.489918</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.274039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.724523</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.938444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.979124</td>\n",
       "      <td>1.231072</td>\n",
       "      <td>0.034272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.488151</td>\n",
       "      <td>0.192400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.431639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.317004</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.772132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.373650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.489011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179028</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1.63672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.101847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.396604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.992393</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.321203</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>6</td>\n",
       "      <td>2169</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.245846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.356380</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.266896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.831635</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>6</td>\n",
       "      <td>2172</td>\n",
       "      <td>0</td>\n",
       "      <td>0.82622</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.438433</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202663</td>\n",
       "      <td>0.903240</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.131788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.666926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.423386</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>6</td>\n",
       "      <td>2194</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.370191</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.514775</td>\n",
       "      <td>1.910061</td>\n",
       "      <td>4.864564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.072880</td>\n",
       "      <td>1.667925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.029452</td>\n",
       "      <td>0.041010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>6</td>\n",
       "      <td>2191</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0116296</td>\n",
       "      <td>0.446989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.294821</td>\n",
       "      <td>2.410504</td>\n",
       "      <td>0.849241</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.202494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.293362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>6</td>\n",
       "      <td>2222</td>\n",
       "      <td>0</td>\n",
       "      <td>2.5969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.511248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows × 4098 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1    2          3         4     5         6         7         8     \\\n",
       "0      6     5    0          0  0.000000   0.0  0.000000  0.000000  2.492087   \n",
       "1      6     9    0   0.274039  0.000000   0.0  0.000000  0.000000  1.724523   \n",
       "2      6    10    0          0  0.000000   0.0  0.000000  0.000000  0.488151   \n",
       "3      6    17    0          0  0.000000   0.0  0.000000  0.000000  0.000000   \n",
       "4      6    19    0    1.63672  0.000000   0.0  0.101847  0.000000  0.396604   \n",
       "..   ...   ...  ...        ...       ...   ...       ...       ...       ...   \n",
       "148    6  2169    0          0  0.000000   0.0  0.000000  0.000000  0.000000   \n",
       "149    6  2172    0    0.82622  0.000000   0.0  0.000000  0.000000  0.438433   \n",
       "150    6  2194    0          0  0.370191   0.0  0.514775  1.910061  4.864564   \n",
       "151    6  2191    0  0.0116296  0.446989   0.0  0.294821  2.410504  0.849241   \n",
       "152    6  2222    0     2.5969  0.000000   0.0  0.000000  0.000000  0.000000   \n",
       "\n",
       "         9     ...      4088      4089  4090      4091      4092      4093  \\\n",
       "0    0.000000  ...  0.239919  3.340010   0.0  2.148260  0.000000  0.000000   \n",
       "1    0.000000  ...  0.000000  0.000000   0.0  0.938444  0.000000  0.979124   \n",
       "2    0.192400  ...  0.000000  1.431639   0.0  0.573350  0.000000  0.000000   \n",
       "3    0.000000  ...  0.000000  3.772132   0.0  1.373650  0.000000  0.489011   \n",
       "4    0.000000  ...  0.000000  0.000000   0.0  0.000000  0.992393  0.000000   \n",
       "..        ...  ...       ...       ...   ...       ...       ...       ...   \n",
       "148  3.245846  ...  0.000000  2.356380   0.0  0.266896  0.000000  0.000000   \n",
       "149  0.000000  ...  0.202663  0.903240   0.0  4.131788  0.000000  0.000000   \n",
       "150  0.000000  ...  2.072880  1.667925   0.0  0.000000  0.000000  0.000000   \n",
       "151  0.000000  ...  0.000000  0.000000   0.0  0.000000  0.000000  1.202494   \n",
       "152  0.000000  ...  1.511248  0.000000   0.0  0.000000  0.000000  0.000000   \n",
       "\n",
       "         4094      4095      4096  4097  \n",
       "0    2.192654  0.000000  0.489918   0.0  \n",
       "1    1.231072  0.034272  0.000000   0.0  \n",
       "2    0.000000  0.000000  1.317004   0.0  \n",
       "3    0.000000  0.000000  0.179028   0.0  \n",
       "4    0.000000  0.000000  2.321203   0.0  \n",
       "..        ...       ...       ...   ...  \n",
       "148  0.000000  0.000000  1.831635   0.0  \n",
       "149  1.666926  0.000000  2.423386   0.0  \n",
       "150  5.029452  0.041010  0.000000   0.0  \n",
       "151  0.000000  2.293362  0.000000   0.0  \n",
       "152  0.000000  0.010338  0.000000   0.0  \n",
       "\n",
       "[153 rows x 4098 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#need to write another ftn to get entire row with features in df, not just ID list?\n",
    "\n",
    "#find all the cakes\n",
    "def features_by_topic(idlist, combineddf):\n",
    "    '''Takes in an id list from get_recipes_by_topic,\n",
    "    finds the features of images in that list, save in a dataframe'''\n",
    "    ftgroup = []\n",
    "    for recipeid in idlist:\n",
    "        ftrow = np.array(combineddf.loc[recipeid])\n",
    "        ftgroup.append(ftrow)\n",
    "    return pd.DataFrame(ftgroup)\n",
    "\n",
    "cake_features_df = features_by_topic(cakes_list, combined_ididx) \n",
    "cake_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find cosine similarity within topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T23:21:24.851874Z",
     "start_time": "2019-12-05T23:21:00.212Z"
    }
   },
   "outputs": [],
   "source": [
    "#def cos_sim_by_topic(imgft, topicft):\n",
    "    '''Takes in features for one image (row in df), \n",
    "    and a df of all images in the same topic group\n",
    "    return the cosine similarity (one vs all) as an array'''\n",
    "#can use prev ftn?    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T23:42:20.765116Z",
     "start_time": "2019-12-05T23:42:20.726820Z"
    }
   },
   "outputs": [],
   "source": [
    "#from cosine_similarity notebook:\n",
    "def cos_sim_vs_all(imageft, datasetft):\n",
    "    '''Find the pairwise cosine similarity between features of the chosen image and all the images in the dataset'''\n",
    "    sim_list = []\n",
    "    for i in range(len(datasetft)):\n",
    "        cos_sim = cosine_similarity(imageft, datasetft.iloc[i].values.reshape(1,-1)) #output of cosine_sim is already an array here\n",
    "        sim_list.append(cos_sim)\n",
    "        #print(np.stack(sim_list, axis=0).shape)\n",
    "    #return sim_list\n",
    "    return np.stack(sim_list, axis=0) #use stack to convert a list of arrays to one array\n",
    "#convert ft df to matrix?*\n",
    "\n",
    "#get error when using np.array to convert output to array: https://stackoverflow.com/questions/4674473/valueerror-setting-an-array-element-with-a-sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T23:42:23.931104Z",
     "start_time": "2019-12-05T23:42:23.811897Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.        ]],\n",
       "\n",
       "       [[0.5753688 ]],\n",
       "\n",
       "       [[0.50661623]],\n",
       "\n",
       "       [[0.46837556]],\n",
       "\n",
       "       [[0.41748737]]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compare first cake with the rest of the cakes\n",
    "\n",
    "cos_sim_vs_all(cake_features_df.iloc[0].values.reshape(1,-1), cake_features_df.head())  #use iloc for df indices!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T00:12:31.568558Z",
     "start_time": "2019-12-06T00:12:31.332397Z"
    }
   },
   "outputs": [],
   "source": [
    "sim_cake0 = cos_sim_vs_all(cake_features_df.iloc[0].values.reshape(1,-1), cake_features_df.head(40)) \n",
    "#get old error on larger df? fixed for small dataset\n",
    "\n",
    "#problem with cake_features_df, before cosine similarity\n",
    "#*debug--by going through different number of rows, see where it crashes, good up to row 40 in cake_features, row 41 corrupted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T00:13:29.661367Z",
     "start_time": "2019-12-06T00:13:29.645573Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             6\n",
       "1           465\n",
       "2             0\n",
       "3       2.72996\n",
       "4             0\n",
       "         ...   \n",
       "4093          0\n",
       "4094    3.06167\n",
       "4095          0\n",
       "4096    1.37959\n",
       "4097          0\n",
       "Name: 39, Length: 4098, dtype: object"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cake_features_df.iloc[39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T00:14:13.383078Z",
     "start_time": "2019-12-06T00:14:13.368962Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              6\n",
       "1              5\n",
       "2              0\n",
       "3              0\n",
       "4              0\n",
       "          ...   \n",
       "4093           0\n",
       "4094     2.19265\n",
       "4095           0\n",
       "4096    0.489918\n",
       "4097           0\n",
       "Name: 0, Length: 4098, dtype: object"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cake_features_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T00:14:28.222320Z",
     "start_time": "2019-12-06T00:14:28.207688Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               6\n",
       "1            2194\n",
       "2               0\n",
       "3               0\n",
       "4        0.370191\n",
       "          ...    \n",
       "4093            0\n",
       "4094      5.02945\n",
       "4095    0.0410097\n",
       "4096            0\n",
       "4097            0\n",
       "Name: 150, Length: 4098, dtype: object"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cake_features_df.iloc[150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T00:13:07.195167Z",
     "start_time": "2019-12-06T00:13:07.050354Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [6.0, 494.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.8...\n",
       "1       [6.0, 1495.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14146...\n",
       "2       [5.0, 494.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.8...\n",
       "3       [5.0, 1495.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14146...\n",
       "4                                                     NaN\n",
       "                              ...                        \n",
       "4093                                                  NaN\n",
       "4094                                                  NaN\n",
       "4095                                                  NaN\n",
       "4096                                                  NaN\n",
       "4097                                                  NaN\n",
       "Name: 40, Length: 4098, dtype: object"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cake_features_df.iloc[40] #index mismatch after this? could be hard to fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T00:13:16.820804Z",
     "start_time": "2019-12-06T00:13:16.799819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [6.0, 494.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.8...\n",
       "1       [6.0, 1495.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14146...\n",
       "2       [5.0, 494.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.8...\n",
       "3       [5.0, 1495.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14146...\n",
       "4                                                     NaN\n",
       "                              ...                        \n",
       "4093                                                  NaN\n",
       "4094                                                  NaN\n",
       "4095                                                  NaN\n",
       "4096                                                  NaN\n",
       "4097                                                  NaN\n",
       "Name: 41, Length: 4098, dtype: object"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cake_features_df.iloc[41] #just these 2 rows? transposed? recipe ID 494 and 1495, check in feature matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T00:15:24.667632Z",
     "start_time": "2019-12-06T00:15:24.653248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               6\n",
       "1             495\n",
       "2               0\n",
       "3               0\n",
       "4        0.436465\n",
       "          ...    \n",
       "4093     0.697132\n",
       "4094     0.101824\n",
       "4095            0\n",
       "4096    0.0972521\n",
       "4097     0.126291\n",
       "Name: 42, Length: 4098, dtype: object"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cake_features_df.iloc[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T00:14:57.569165Z",
     "start_time": "2019-12-06T00:14:57.548723Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             6\n",
       "1          2172\n",
       "2             0\n",
       "3       0.82622\n",
       "4             0\n",
       "         ...   \n",
       "4093          0\n",
       "4094    1.66693\n",
       "4095          0\n",
       "4096    2.42339\n",
       "4097          0\n",
       "Name: 149, Length: 4098, dtype: object"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cake_features_df.iloc[149]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T00:15:15.601404Z",
     "start_time": "2019-12-06T00:15:15.581359Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             6\n",
       "1          2086\n",
       "2             0\n",
       "3             0\n",
       "4             0\n",
       "         ...   \n",
       "4093          0\n",
       "4094    2.26688\n",
       "4095          0\n",
       "4096          0\n",
       "4097          0\n",
       "Name: 140, Length: 4098, dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cake_features_df.iloc[140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T00:20:45.967505Z",
     "start_time": "2019-12-06T00:20:45.344246Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 153 entries, 0 to 152\n",
      "Columns: 4098 entries, 0 to 4097\n",
      "dtypes: float64(4094), object(4)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "#check large df of features for all images, how many NaNs?\n",
    "cake_features_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T00:21:20.490928Z",
     "start_time": "2019-12-06T00:21:04.953041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>...</th>\n",
       "      <th>4088</th>\n",
       "      <th>4089</th>\n",
       "      <th>4090</th>\n",
       "      <th>4091</th>\n",
       "      <th>4092</th>\n",
       "      <th>4093</th>\n",
       "      <th>4094</th>\n",
       "      <th>4095</th>\n",
       "      <th>4096</th>\n",
       "      <th>4097</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.138306</td>\n",
       "      <td>0.196955</td>\n",
       "      <td>0.306323</td>\n",
       "      <td>0.278115</td>\n",
       "      <td>1.271333</td>\n",
       "      <td>0.453859</td>\n",
       "      <td>0.310347</td>\n",
       "      <td>0.855819</td>\n",
       "      <td>0.014650</td>\n",
       "      <td>0.126931</td>\n",
       "      <td>...</td>\n",
       "      <td>0.785164</td>\n",
       "      <td>1.054000</td>\n",
       "      <td>0.041524</td>\n",
       "      <td>0.817231</td>\n",
       "      <td>0.199240</td>\n",
       "      <td>0.223808</td>\n",
       "      <td>0.792708</td>\n",
       "      <td>0.214912</td>\n",
       "      <td>0.942807</td>\n",
       "      <td>0.400255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.493999</td>\n",
       "      <td>0.627865</td>\n",
       "      <td>0.690406</td>\n",
       "      <td>0.768469</td>\n",
       "      <td>1.486258</td>\n",
       "      <td>0.937399</td>\n",
       "      <td>0.757086</td>\n",
       "      <td>1.539012</td>\n",
       "      <td>0.148727</td>\n",
       "      <td>0.413419</td>\n",
       "      <td>...</td>\n",
       "      <td>1.192526</td>\n",
       "      <td>1.447391</td>\n",
       "      <td>0.225336</td>\n",
       "      <td>1.244994</td>\n",
       "      <td>0.512637</td>\n",
       "      <td>0.527468</td>\n",
       "      <td>1.273527</td>\n",
       "      <td>0.530906</td>\n",
       "      <td>1.339174</td>\n",
       "      <td>0.787239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128557</td>\n",
       "      <td>0.394728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.271816</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.127231</td>\n",
       "      <td>0.331795</td>\n",
       "      <td>0.065030</td>\n",
       "      <td>1.144676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.273832</td>\n",
       "      <td>1.687366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.101956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.186786</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.343614</td>\n",
       "      <td>0.310251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>3.222694</td>\n",
       "      <td>5.178796</td>\n",
       "      <td>4.264771</td>\n",
       "      <td>4.579515</td>\n",
       "      <td>6.595136</td>\n",
       "      <td>5.218029</td>\n",
       "      <td>5.473833</td>\n",
       "      <td>7.591406</td>\n",
       "      <td>1.763126</td>\n",
       "      <td>2.885091</td>\n",
       "      <td>...</td>\n",
       "      <td>6.635208</td>\n",
       "      <td>6.637444</td>\n",
       "      <td>1.455257</td>\n",
       "      <td>5.639992</td>\n",
       "      <td>2.634250</td>\n",
       "      <td>2.496529</td>\n",
       "      <td>6.256621</td>\n",
       "      <td>3.081371</td>\n",
       "      <td>6.430125</td>\n",
       "      <td>4.297850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 4094 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             4           5           6           7           8           9     \\\n",
       "count  147.000000  147.000000  147.000000  147.000000  147.000000  147.000000   \n",
       "mean     0.138306    0.196955    0.306323    0.278115    1.271333    0.453859   \n",
       "std      0.493999    0.627865    0.690406    0.768469    1.486258    0.937399   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.000000    0.000000    0.000000    0.000000    0.666706    0.000000   \n",
       "75%      0.000000    0.000000    0.166531    0.000000    2.127231    0.331795   \n",
       "max      3.222694    5.178796    4.264771    4.579515    6.595136    5.218029   \n",
       "\n",
       "             10          11          12          13    ...        4088  \\\n",
       "count  147.000000  147.000000  147.000000  147.000000  ...  147.000000   \n",
       "mean     0.310347    0.855819    0.014650    0.126931  ...    0.785164   \n",
       "std      0.757086    1.539012    0.148727    0.413419  ...    1.192526   \n",
       "min      0.000000    0.000000    0.000000    0.000000  ...    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000  ...    0.000000   \n",
       "50%      0.000000    0.000000    0.000000    0.000000  ...    0.128557   \n",
       "75%      0.065030    1.144676    0.000000    0.000000  ...    1.273832   \n",
       "max      5.473833    7.591406    1.763126    2.885091  ...    6.635208   \n",
       "\n",
       "             4089        4090        4091        4092        4093        4094  \\\n",
       "count  147.000000  147.000000  147.000000  147.000000  147.000000  147.000000   \n",
       "mean     1.054000    0.041524    0.817231    0.199240    0.223808    0.792708   \n",
       "std      1.447391    0.225336    1.244994    0.512637    0.527468    1.273527   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.394728    0.000000    0.000000    0.000000    0.000000    0.022898   \n",
       "75%      1.687366    0.000000    1.101956    0.000000    0.000000    1.186786   \n",
       "max      6.637444    1.455257    5.639992    2.634250    2.496529    6.256621   \n",
       "\n",
       "             4095        4096        4097  \n",
       "count  147.000000  147.000000  147.000000  \n",
       "mean     0.214912    0.942807    0.400255  \n",
       "std      0.530906    1.339174    0.787239  \n",
       "min      0.000000    0.000000    0.000000  \n",
       "25%      0.000000    0.000000    0.000000  \n",
       "50%      0.000000    0.271816    0.000000  \n",
       "75%      0.000000    1.343614    0.310251  \n",
       "max      3.081371    6.430125    4.297850  \n",
       "\n",
       "[8 rows x 4094 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cake_features_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T00:20:18.628979Z",
     "start_time": "2019-12-06T00:20:18.614293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only 147 rows with values, 3 rows corrupted? 41 42 and which one?\n",
    "len(cake_features_df) #also after rerunning kernel, len 153 instead of 150, so now 6 bad rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T00:26:50.286851Z",
     "start_time": "2019-12-06T00:26:49.431312Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "41\n",
      "96\n",
      "97\n",
      "102\n",
      "103\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-fe54516b68fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcake_features_df\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mcake_features_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/anaconda/envs/metis/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1424\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/anaconda/envs/metis/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m             \u001b[0;31m# validate the location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/anaconda/envs/metis/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2086\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2087\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2088\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2090\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "for row in cake_features_df:\n",
    "    if cake_features_df.iloc[row].isnull().any()==True:\n",
    "        print(row) #maybe instead of returning old idx, want ID? if not a single int or float, corrupted (like in idx 40 above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 pairs of consecutive rows are corrupted - how? how to fix? might have more in other topics\n",
    "#problem seems to be with large df after combining and resetting index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T00:31:53.572832Z",
     "start_time": "2019-12-06T00:31:52.656059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Float64Index: 2270 entries, 33407.0 to 84103.0\n",
      "Columns: 4098 entries, topics to 4095\n",
      "dtypes: float64(4098)\n",
      "memory usage: 71.1 MB\n"
     ]
    }
   ],
   "source": [
    "combined_ididx.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T00:35:36.426365Z",
     "start_time": "2019-12-06T00:35:36.249766Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot index by location index with a non-integer key",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-b7d32b375dc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbad_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcombined_ididx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcombined_ididx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mbad_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbad_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/anaconda/envs/metis/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1424\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/anaconda/envs/metis/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2152\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2153\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2154\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot index by location index with a non-integer key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m             \u001b[0;31m# validate the location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot index by location index with a non-integer key"
     ]
    }
   ],
   "source": [
    "bad_list = []\n",
    "for i in combined_ididx.iloc['idx']:\n",
    "    if combined_ididx.iloc[i].isnull().any()==True:\n",
    "        bad_list.append(i)\n",
    "bad_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bug before cos sim, Join_on_IDs_debug.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T00:24:46.525507Z",
     "start_time": "2019-12-06T00:24:46.487978Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([6.00000000e+00, 4.94000000e+02, 0.00000000e+00, ...,\n",
       "       0.00000000e+00, 3.29863286e+00, 6.16847277e-02]),\n",
       "       array([   6., 1495.,    0., ...,    0.,    0.,    0.]),\n",
       "       array([5.00000000e+00, 4.94000000e+02, 0.00000000e+00, ...,\n",
       "       0.00000000e+00, 3.29863286e+00, 6.16847277e-02]),\n",
       "       ..., nan, nan, nan], dtype=object)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(cake_features_df.iloc[40]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If after filtering by topic, cos similarity results not good, consider going back to doc-topic matrix and perform clustering instead of argmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modelling with LDA -- skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T23:54:33.788577Z",
     "start_time": "2019-12-03T23:54:33.784983Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/\n",
    "#also, LDA exercise lecture notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T23:54:33.801618Z",
     "start_time": "2019-12-03T23:54:33.791511Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T22:25:55.333745Z",
     "start_time": "2019-12-03T22:25:55.330930Z"
    }
   },
   "source": [
    "#### Text cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T23:54:33.808996Z",
     "start_time": "2019-12-03T23:54:33.804889Z"
    }
   },
   "outputs": [],
   "source": [
    "# stop = set(stopwords.words('english'))\n",
    "# exclude = set(string.punctuation)\n",
    "# lemma = WordNetLemmatizer()\n",
    "# def clean(doc):\n",
    "#     stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "#     punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "#     normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "#     return normalized\n",
    "\n",
    "# doc_clean = [clean(doc).split() for doc in doc_complete]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T23:54:33.816087Z",
     "start_time": "2019-12-03T23:54:33.811910Z"
    }
   },
   "outputs": [],
   "source": [
    "# #create a Gensim dictionary from the texts\n",
    "# dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# #remove extremes (similar to the min/max df step used when creating the tf-idf matrix)\n",
    "# dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
    "\n",
    "# #convert the dictionary to a bag of words corpus for reference\n",
    "# corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T23:54:33.833025Z",
     "start_time": "2019-12-03T23:54:33.819126Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use nltk stopwords\n",
    "nltkstopwords = stopwords.words('english')\n",
    "len(nltkstopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T23:54:33.842014Z",
     "start_time": "2019-12-03T23:54:33.835609Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#custom stop_words\n",
    "nltkstopwords.extend(['minutes','add','remove'])\n",
    "len(nltkstopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T23:54:33.852544Z",
     "start_time": "2019-12-03T23:54:33.845541Z"
    }
   },
   "outputs": [],
   "source": [
    "#follow lecture notebook, use countvectorizer, then convert to Gensim\n",
    "#analyzer=word: Tokenize by word\n",
    "#ngram_range=(1,2): Keep all 1 and 2-word grams\n",
    "#stop_words=english: Remove all English stop words\n",
    "#token_pattern=\\\\b[a-z][a-z]+\\\\b: Match all tokens with 2 or more (strictly) alphabet characters\n",
    "\n",
    "\n",
    "#Create a CountVectorizer for parsing/counting words\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2),  \n",
    "                                   stop_words=nltkstopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T23:54:35.597373Z",
     "start_time": "2019-12-03T23:54:33.856169Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 2), preprocessor=None,\n",
       "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                            'itself', ...],\n",
       "                strip_accents=None, token_pattern='\\\\b[a-z][a-z]+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.fit(cld_withphotos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T23:54:36.597508Z",
     "start_time": "2019-12-03T23:54:35.600413Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101747, 2225)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the term-document matrix\n",
    "# Transpose it so the terms are the rows(for gensim?)\n",
    "doc_word = count_vectorizer.transform(cld_withphotos).transpose()\n",
    "doc_word.shape #without transpose, document-term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T23:54:37.639108Z",
     "start_time": "2019-12-03T23:54:36.600399Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2215</th>\n",
       "      <th>2216</th>\n",
       "      <th>2217</th>\n",
       "      <th>2218</th>\n",
       "      <th>2219</th>\n",
       "      <th>2220</th>\n",
       "      <th>2221</th>\n",
       "      <th>2222</th>\n",
       "      <th>2223</th>\n",
       "      <th>2224</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>abd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>abd boiled</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>able</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>able cover</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>able hold</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2225 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0     1     2     3     4     5     6     7     8     9     ...  \\\n",
       "abd            0     0     0     0     0     0     0     0     0     0  ...   \n",
       "abd boiled     0     0     0     0     0     0     0     0     0     0  ...   \n",
       "able           0     0     0     0     0     0     0     0     0     0  ...   \n",
       "able cover     0     0     0     0     0     0     0     0     0     0  ...   \n",
       "able hold      0     0     0     0     0     0     0     0     0     0  ...   \n",
       "\n",
       "            2215  2216  2217  2218  2219  2220  2221  2222  2223  2224  \n",
       "abd            0     0     0     0     0     0     0     0     0     0  \n",
       "abd boiled     0     0     0     0     0     0     0     0     0     0  \n",
       "able           0     0     0     0     0     0     0     0     0     0  \n",
       "able cover     0     0     0     0     0     0     0     0     0     0  \n",
       "able hold      0     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 2225 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(doc_word.toarray(), count_vectorizer.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T23:54:37.647183Z",
     "start_time": "2019-12-03T23:54:37.642727Z"
    }
   },
   "outputs": [],
   "source": [
    "#convert SciPy matrix to Gensim friendly corpus:\n",
    "corpus = matutils.Sparse2Corpus(doc_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map matrix rows to words (tokens)\n",
    "\n",
    "save a mapping (dict) of row id to word (token) for later use by gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T23:54:37.702272Z",
     "start_time": "2019-12-03T23:54:37.650616Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101747"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word = dict((v, k) for k, v in count_vectorizer.vocabulary_.items())\n",
    "len(id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating an LDA model\n",
    "It requires our corpus of word counts, mapping of row ids to words, and the number of topics (3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T23:54:37.708581Z",
     "start_time": "2019-12-03T23:54:37.704705Z"
    }
   },
   "outputs": [],
   "source": [
    "#without additional custom stopwords, LDA topics not as distinct as NMF ones, also have some strange phrases\n",
    "#went back to edit stopwords, to remove minutes, add, remove\n",
    "\n",
    "#also try TF-IDF vectorization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T23:55:00.282047Z",
     "start_time": "2019-12-03T23:54:37.711132Z"
    }
   },
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus=corpus, num_topics=20, id2word=id2word, passes=5) #slow? ~1min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T23:55:00.336755Z",
     "start_time": "2019-12-03T23:55:00.284457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*\"jars\" + 0.003*\"brandy rum\" + 0.002*\"sauce little\" + 0.002*\"sterilised\" + 0.002*\"sterilised jars\" + 0.001*\"zester\" + 0.001*\"based saucepan\" + 0.001*\"pan leaves\" + 0.001*\"base pan\" + 0.001*\"sterilise\"'),\n",
       " (1,\n",
       "  '0.017*\"dough\" + 0.009*\"bowl\" + 0.009*\"icing\" + 0.008*\"baking\" + 0.007*\"sugar\" + 0.007*\"place\" + 0.006*\"chocolate\" + 0.006*\"surface\" + 0.006*\"roll\" + 0.006*\"mixture\"'),\n",
       " (2,\n",
       "  '0.002*\"leave chicken\" + 0.001*\"vanilla pods\" + 0.001*\"chop butter\" + 0.001*\"large dish\" + 0.001*\"dust cocoa\" + 0.001*\"processor whiz\" + 0.001*\"whiz smooth\" + 0.001*\"carefully hot\" + 0.001*\"half biscuits\" + 0.001*\"job taste\"'),\n",
       " (3,\n",
       "  '0.011*\"pastry\" + 0.011*\"pudding\" + 0.008*\"basin\" + 0.008*\"job\" + 0.007*\"water\" + 0.005*\"flour\" + 0.005*\"foil\" + 0.005*\"dough\" + 0.005*\"kid\" + 0.005*\"kid job\"'),\n",
       " (4,\n",
       "  '0.003*\"stock absorbed\" + 0.002*\"ladleful\" + 0.002*\"ladleful stock\" + 0.001*\"little freshly\" + 0.001*\"flour cover\" + 0.001*\"soft rice\" + 0.001*\"everything coated\" + 0.001*\"walnut oil\" + 0.001*\"another ladleful\" + 0.001*\"bowl another\"'),\n",
       " (5,\n",
       "  '0.003*\"syrup\" + 0.002*\"heat\" + 0.002*\"blended\" + 0.002*\"whisking\" + 0.002*\"chocolate butter\" + 0.002*\"cream\" + 0.001*\"continue\" + 0.001*\"oven stir\" + 0.001*\"cheese stir\" + 0.001*\"minute place\"'),\n",
       " (6,\n",
       "  '0.001*\"place bulgur\" + 0.001*\"spelt\" + 0.000*\"rum babas\" + 0.000*\"ale\" + 0.000*\"spelt mixture\" + 0.000*\"oz ale\" + 0.000*\"lettuce halves\" + 0.000*\"cabbage mixture\" + 0.000*\"lettuces\" + 0.000*\"browned crispy\"'),\n",
       " (7,\n",
       "  '0.008*\"trout\" + 0.007*\"croquettes\" + 0.005*\"bread\" + 0.003*\"well\" + 0.003*\"place\" + 0.003*\"together\" + 0.003*\"lemon\" + 0.003*\"leaves\" + 0.003*\"cheesecakes\" + 0.003*\"croquette\"'),\n",
       " (8,\n",
       "  '0.002*\"warm milk\" + 0.002*\"paste small\" + 0.002*\"cook gentle\" + 0.001*\"gentle heat\" + 0.001*\"smooth whisk\" + 0.001*\"continuously whisk\" + 0.001*\"biscuits plastic\" + 0.001*\"butter bring\" + 0.001*\"heat\" + 0.001*\"crush rolling\"'),\n",
       " (9,\n",
       "  '0.029*\"cake\" + 0.011*\"cm\" + 0.009*\"top\" + 0.009*\"icing\" + 0.008*\"sugar\" + 0.006*\"tin\" + 0.005*\"mixture\" + 0.005*\"bowl\" + 0.005*\"cream\" + 0.005*\"cm cake\"'),\n",
       " (10,\n",
       "  '0.009*\"water\" + 0.009*\"chicken\" + 0.009*\"place\" + 0.007*\"oil\" + 0.007*\"coriander\" + 0.007*\"heat\" + 0.007*\"lime\" + 0.007*\"fish\" + 0.006*\"juice\" + 0.006*\"serve\"'),\n",
       " (11,\n",
       "  '0.012*\"oven\" + 0.009*\"mixture\" + 0.009*\"pastry\" + 0.007*\"place\" + 0.007*\"bowl\" + 0.007*\"tin\" + 0.006*\"butter\" + 0.006*\"top\" + 0.006*\"sugar\" + 0.005*\"heat\"'),\n",
       " (12,\n",
       "  '0.017*\"stir\" + 0.017*\"heat\" + 0.012*\"fry\" + 0.009*\"pan\" + 0.008*\"rice\" + 0.008*\"cook\" + 0.007*\"oil\" + 0.007*\"lentils\" + 0.006*\"stir fry\" + 0.006*\"simmer\"'),\n",
       " (13,\n",
       "  '0.002*\"jam hot\" + 0.002*\"pods\" + 0.002*\"freezer\" + 0.002*\"whisk\" + 0.002*\"pour mixture\" + 0.002*\"slowly whisk\" + 0.001*\"meanwhile whisk\" + 0.001*\"container freeze\" + 0.001*\"freezer proof\" + 0.001*\"hot cream\"'),\n",
       " (14,\n",
       "  '0.010*\"dough\" + 0.009*\"place\" + 0.007*\"bowl\" + 0.007*\"mix\" + 0.006*\"oven\" + 0.006*\"oil\" + 0.005*\"side\" + 0.004*\"leave\" + 0.004*\"onto\" + 0.003*\"large\"'),\n",
       " (15,\n",
       "  '0.017*\"heat\" + 0.015*\"pan\" + 0.011*\"oil\" + 0.011*\"cook\" + 0.009*\"salt\" + 0.009*\"pepper\" + 0.007*\"season\" + 0.007*\"stir\" + 0.007*\"sauce\" + 0.007*\"fry\"'),\n",
       " (16,\n",
       "  '0.001*\"form even\" + 0.001*\"lidded jar\" + 0.001*\"store tightly\" + 0.001*\"away heat\" + 0.001*\"arrange sliced\" + 0.001*\"heat sunlight\" + 0.001*\"possible store\" + 0.001*\"jar away\" + 0.001*\"tightly lidded\" + 0.001*\"sunlight\"'),\n",
       " (17,\n",
       "  '0.001*\"crushed dried\" + 0.001*\"dressing sprinkle\" + 0.001*\"teriyaki sauce\" + 0.001*\"teriyaki\" + 0.001*\"beef middle\" + 0.001*\"tofu stir\" + 0.001*\"paste thick\" + 0.001*\"move much\" + 0.001*\"teaspoon vegetable\" + 0.001*\"together light\"'),\n",
       " (18,\n",
       "  '0.011*\"mixture\" + 0.007*\"cream\" + 0.007*\"whisk\" + 0.007*\"spoon\" + 0.005*\"mascarpone\" + 0.005*\"egg\" + 0.005*\"fold\" + 0.004*\"bowl\" + 0.004*\"sugar\" + 0.004*\"sugar together\"'),\n",
       " (19,\n",
       "  '0.017*\"turkey\" + 0.014*\"juices\" + 0.010*\"roasting\" + 0.007*\"gammon\" + 0.007*\"gravy\" + 0.007*\"tray\" + 0.006*\"roasting tray\" + 0.006*\"skin\" + 0.005*\"juices run\" + 0.005*\"chicken\"')]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()\n",
    "#10 most important words for 20 topics, 5 passes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 topics, 5 passes\n",
    "\n",
    "0. dessert - pastry\n",
    "1. dessert - cake\n",
    "2. dessert - cream, mascarpone\n",
    "3. chicken salad\n",
    "4. ? pan, heat, sugar\n",
    "5. beef, yorkshire pudding\n",
    "6. ? sieving flour\n",
    "7. dessert - pudding\n",
    "8. fishcakes\n",
    "9. fish chowder\n",
    "10. fish fillets\n",
    "11. rice and chicken stir fry\n",
    "12. ? veg and fish\n",
    "13. ? lentils and fish\n",
    "14. ? eggs and stock\n",
    "15. savoury tart? bacon and onions\n",
    "16. ? pan, heat, oil\n",
    "17. baked chicken, fish\n",
    "18. soy, icing? dessert?\n",
    "19. ? japanese mayo and mulled wine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA is stochastic! will get different topics each time we run it!** -- problematic for assigning documents to topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T23:55:41.468882Z",
     "start_time": "2019-12-03T23:55:00.341164Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"sauce spaghetti\" + 0.001*\"duck legs\" + 0.001*\"duck fat\" + 0.000*\"hot enough\" + 0.000*\"reserved duck\" + 0.000*\"duck meat\" + 0.000*\"beans spinach\" + 0.000*\"parfait\" + 0.000*\"check oil\" + 0.000*\"give batter\"'),\n",
       " (1,\n",
       "  '0.017*\"oil\" + 0.014*\"pan\" + 0.012*\"heat\" + 0.008*\"fry\" + 0.006*\"cook\" + 0.006*\"bowl\" + 0.006*\"salt\" + 0.005*\"frying\" + 0.005*\"frying pan\" + 0.005*\"drain\"'),\n",
       " (2,\n",
       "  '0.013*\"place\" + 0.012*\"oven\" + 0.008*\"salt\" + 0.007*\"oil\" + 0.007*\"bowl\" + 0.007*\"mix\" + 0.007*\"pepper\" + 0.007*\"chicken\" + 0.005*\"serve\" + 0.005*\"season\"'),\n",
       " (3,\n",
       "  '0.003*\"spoon yoghurt\" + 0.001*\"bruise\" + 0.001*\"almond milk\" + 0.001*\"browned put\" + 0.001*\"make vinaigrette\" + 0.001*\"pour top\" + 0.001*\"squeeze fresh\" + 0.000*\"clam\" + 0.000*\"help create\" + 0.000*\"soup mixture\"'),\n",
       " (4,\n",
       "  '0.032*\"cake\" + 0.023*\"cm\" + 0.011*\"cm cake\" + 0.010*\"icing\" + 0.009*\"board\" + 0.008*\"cake board\" + 0.008*\"top\" + 0.006*\"cakes\" + 0.005*\"repeat\" + 0.005*\"around\"'),\n",
       " (5,\n",
       "  '0.015*\"lime\" + 0.013*\"stir fry\" + 0.011*\"wok\" + 0.011*\"soy\" + 0.010*\"soy sauce\" + 0.010*\"sauce\" + 0.008*\"chicken\" + 0.008*\"paste\" + 0.007*\"curry\" + 0.007*\"lime juice\"'),\n",
       " (6,\n",
       "  '0.006*\"dressing\" + 0.005*\"noodles\" + 0.004*\"churros\" + 0.004*\"fillets\" + 0.004*\"steamer\" + 0.003*\"coconut\" + 0.003*\"put\" + 0.003*\"fruit\" + 0.003*\"fillets top\" + 0.003*\"sea bass\"'),\n",
       " (7,\n",
       "  '0.001*\"lengthways scrape\" + 0.001*\"mix feta\" + 0.001*\"granita\" + 0.000*\"vinegar reduce\" + 0.000*\"anchovies\" + 0.000*\"bring liquid\" + 0.000*\"celery softened\" + 0.000*\"chilli continue\" + 0.000*\"lamb chunks\" + 0.000*\"serve granita\"'),\n",
       " (8,\n",
       "  '0.008*\"job\" + 0.007*\"base\" + 0.007*\"chocolate\" + 0.006*\"cheesecake\" + 0.006*\"cream\" + 0.006*\"place\" + 0.005*\"mixture\" + 0.005*\"bowl\" + 0.005*\"kid\" + 0.005*\"kid job\"'),\n",
       " (9,\n",
       "  '0.007*\"fish\" + 0.006*\"fillets\" + 0.003*\"salmon\" + 0.003*\"haddock\" + 0.003*\"blanc\" + 0.003*\"beurre blanc\" + 0.003*\"eggs\" + 0.003*\"boiling\" + 0.002*\"steamer\" + 0.002*\"chowder\"'),\n",
       " (10,\n",
       "  '0.004*\"tin base\" + 0.002*\"syrup\" + 0.002*\"whisk\" + 0.002*\"tin lay\" + 0.002*\"way place\" + 0.002*\"rest filling\" + 0.002*\"surface gather\" + 0.002*\"blended\" + 0.002*\"whisking\" + 0.002*\"leave mixture\"'),\n",
       " (11,\n",
       "  '0.001*\"watermelon\" + 0.001*\"lolly\" + 0.001*\"freezer overnight\" + 0.001*\"lolly moulds\" + 0.001*\"ice lolly\" + 0.001*\"dissolves heat\" + 0.001*\"dissolves\" + 0.001*\"sugar dissolves\" + 0.000*\"moulds place\" + 0.000*\"knots\"'),\n",
       " (12,\n",
       "  '0.023*\"heat\" + 0.018*\"pan\" + 0.013*\"cook\" + 0.012*\"stir\" + 0.008*\"water\" + 0.008*\"simmer\" + 0.008*\"fry\" + 0.008*\"sauce\" + 0.007*\"bring\" + 0.007*\"oil\"'),\n",
       " (13,\n",
       "  '0.001*\"chop vegetables\" + 0.001*\"pur pan\" + 0.001*\"remaining garlic\" + 0.001*\"processor whiz\" + 0.001*\"whiz smooth\" + 0.001*\"along herbs\" + 0.001*\"mushrooms carrots\" + 0.001*\"shape patties\" + 0.001*\"whiz\" + 0.001*\"herbs spices\"'),\n",
       " (14,\n",
       "  '0.001*\"mussels\" + 0.001*\"cheeks\" + 0.001*\"tartlet\" + 0.000*\"chickens\" + 0.000*\"ox cheeks\" + 0.000*\"ox\" + 0.000*\"gooseberries\" + 0.000*\"discard open\" + 0.000*\"gooseberry\" + 0.000*\"beards\"'),\n",
       " (15,\n",
       "  '0.024*\"dough\" + 0.009*\"roll\" + 0.009*\"place\" + 0.008*\"flour\" + 0.006*\"pastry\" + 0.006*\"water\" + 0.006*\"surface\" + 0.006*\"bowl\" + 0.006*\"oven\" + 0.005*\"baking\"'),\n",
       " (16,\n",
       "  '0.004*\"lime leaves\" + 0.004*\"thai\" + 0.003*\"milk fish\" + 0.003*\"curry paste\" + 0.002*\"chicken browned\" + 0.002*\"sugar snap\" + 0.002*\"snap peas\" + 0.002*\"snap\" + 0.002*\"sweet sour\" + 0.002*\"lime\"'),\n",
       " (17,\n",
       "  '0.015*\"oven\" + 0.008*\"batter\" + 0.007*\"tin\" + 0.005*\"muffin\" + 0.005*\"flour\" + 0.005*\"pudding\" + 0.004*\"beef\" + 0.004*\"yorkshire\" + 0.004*\"fat\" + 0.004*\"milk\"'),\n",
       " (18,\n",
       "  '0.012*\"mixture\" + 0.011*\"sugar\" + 0.010*\"bowl\" + 0.010*\"cake\" + 0.007*\"tin\" + 0.007*\"icing\" + 0.007*\"oven\" + 0.007*\"cool\" + 0.006*\"cream\" + 0.006*\"whisk\"'),\n",
       " (19,\n",
       "  '0.027*\"pastry\" + 0.010*\"oven\" + 0.008*\"together\" + 0.008*\"mixture\" + 0.008*\"tin\" + 0.007*\"baking\" + 0.007*\"egg\" + 0.006*\"sugar\" + 0.006*\"butter\" + 0.006*\"bake\"')]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda20_10passes = models.LdaModel(corpus=corpus, num_topics=20, id2word=id2word, passes=10)\n",
    "#compare 10 passes with 5 passes above, do more passes give better topics?\n",
    "lda20_10passes.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 topics, 10 passes - might be too many? But topic #17 is roast chicken and turkey!\n",
    "\n",
    "0. hamburger (assemble with kids)\n",
    "1. ?\n",
    "2. beef, egg batter\n",
    "3. dessert - butter, yogurt, fruit and jam\n",
    "4. indian\n",
    "5. dessert - cake\n",
    "6. ?\n",
    "7. dessert - pastry\n",
    "8. salad/dessert\n",
    "9. grill/bbq\n",
    "10. chips and dip\n",
    "11. tacos?\n",
    "12. thai noodles\n",
    "13. ? corn and stevia\n",
    "14. baking\n",
    "15. ? boiled\n",
    "16. ? pickles\n",
    "17. roast turkey and chicken!\n",
    "18. ?\n",
    "19. ? similar to 18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T23:56:58.615929Z",
     "start_time": "2019-12-03T23:55:41.471570Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.038*\"cake\" + 0.019*\"icing\" + 0.013*\"cm\" + 0.009*\"sugar\" + 0.009*\"tin\" + 0.009*\"top\" + 0.007*\"chocolate\" + 0.007*\"beat\" + 0.006*\"using\" + 0.006*\"cakes\"'),\n",
       " (1,\n",
       "  '0.004*\"pancakes\" + 0.003*\"pancake\" + 0.002*\"cook golden\" + 0.001*\"cm non\" + 0.001*\"pancakes warm\" + 0.001*\"milk food\" + 0.001*\"remaining batter\" + 0.001*\"repeat remaining\" + 0.001*\"pancake cook\" + 0.001*\"keep cooked\"'),\n",
       " (2,\n",
       "  '0.002*\"spoon yoghurt\" + 0.001*\"smooth alternatively\" + 0.001*\"onion chopped\" + 0.001*\"lard large\" + 0.001*\"sprinkle fresh\" + 0.001*\"curry sprinkle\" + 0.001*\"blitz smooth\" + 0.001*\"dark rye\" + 0.001*\"stir coated\" + 0.001*\"lentils saucepan\"'),\n",
       " (3,\n",
       "  '0.000*\"ml floz\" + 0.000*\"floz\" + 0.000*\"version\" + 0.000*\"torn\" + 0.000*\"salady\" + 0.000*\"seedless delectably\" + 0.000*\"chopped tending\" + 0.000*\"quarters chunky\" + 0.000*\"tending towards\" + 0.000*\"black olives\"'),\n",
       " (4,\n",
       "  '0.004*\"griddle\" + 0.003*\"iron\" + 0.003*\"cakes\" + 0.003*\"iron griddle\" + 0.003*\"splash milk\" + 0.002*\"welsh cakes\" + 0.002*\"welsh\" + 0.002*\"side lightly\" + 0.002*\"parts spring\" + 0.002*\"three side\"'),\n",
       " (5,\n",
       "  '0.002*\"together vinegar\" + 0.001*\"pasta machine\" + 0.001*\"ingredients serve\" + 0.001*\"onions cucumber\" + 0.001*\"lobster\" + 0.001*\"coats salad\" + 0.001*\"vinegar honey\" + 0.001*\"herbs like\" + 0.001*\"meanwhile pan\" + 0.001*\"small oven\"'),\n",
       " (6,\n",
       "  '0.027*\"chicken\" + 0.008*\"marinade\" + 0.008*\"place\" + 0.006*\"mix\" + 0.005*\"cooked\" + 0.005*\"pieces\" + 0.005*\"bowl\" + 0.005*\"ingredients\" + 0.005*\"oven\" + 0.005*\"juices\"'),\n",
       " (7,\n",
       "  '0.003*\"bamboo skewers\" + 0.002*\"chilli cumin\" + 0.001*\"large griddle\" + 0.001*\"brown griddle\" + 0.001*\"thread chicken\" + 0.001*\"occasionally browned\" + 0.001*\"marinade stir\" + 0.001*\"combine leave\" + 0.001*\"pre soaked\" + 0.001*\"shapes around\"'),\n",
       " (8,\n",
       "  '0.003*\"burgers\" + 0.003*\"dip\" + 0.003*\"lemon\" + 0.003*\"cucumber\" + 0.002*\"juice olive\" + 0.002*\"place tomatoes\" + 0.002*\"tahini\" + 0.002*\"oil thyme\" + 0.002*\"pastry needs\" + 0.002*\"grater\"'),\n",
       " (9,\n",
       "  '0.012*\"heat\" + 0.012*\"mixture\" + 0.010*\"pan\" + 0.009*\"sugar\" + 0.008*\"set\" + 0.008*\"water\" + 0.007*\"bowl\" + 0.007*\"place\" + 0.007*\"stir\" + 0.006*\"cream\"'),\n",
       " (10,\n",
       "  '0.022*\"job\" + 0.014*\"kid job\" + 0.014*\"kid\" + 0.009*\"adult\" + 0.008*\"adult job\" + 0.004*\"vegetables\" + 0.003*\"want\" + 0.003*\"jars\" + 0.003*\"like\" + 0.003*\"kids\"'),\n",
       " (11,\n",
       "  '0.001*\"glass garnish\" + 0.000*\"cocktail shaker\" + 0.000*\"shaker\" + 0.000*\"martini glass\" + 0.000*\"shaker shake\" + 0.000*\"lemon orange\" + 0.000*\"chop herbs\" + 0.000*\"hot knife\" + 0.000*\"herbs finely\" + 0.000*\"mixture martini\"'),\n",
       " (12,\n",
       "  '0.002*\"tomatoes garlic\" + 0.002*\"blended together\" + 0.002*\"juices released\" + 0.001*\"greek style\" + 0.001*\"style yoghurt\" + 0.001*\"marrow\" + 0.001*\"mince mixture\" + 0.001*\"place tomatoes\" + 0.001*\"pepper garlic\" + 0.001*\"scrambled\"'),\n",
       " (13,\n",
       "  '0.007*\"chops\" + 0.004*\"fishcakes\" + 0.003*\"pork chops\" + 0.003*\"brandy rum\" + 0.002*\"sausage rolls\" + 0.002*\"tabasco sauce\" + 0.002*\"tuna\" + 0.002*\"top potatoes\" + 0.002*\"remaining dressing\" + 0.002*\"dried fruits\"'),\n",
       " (14,\n",
       "  '0.019*\"oven\" + 0.011*\"place\" + 0.008*\"tin\" + 0.007*\"gas\" + 0.007*\"top\" + 0.007*\"butter\" + 0.006*\"bowl\" + 0.006*\"pan\" + 0.006*\"mixture\" + 0.006*\"preheat oven\"'),\n",
       " (15,\n",
       "  '0.005*\"brownies\" + 0.003*\"basket\" + 0.002*\"soaks\" + 0.002*\"tofu\" + 0.002*\"brownie\" + 0.002*\"vanilla essence\" + 0.002*\"essence\" + 0.002*\"xx\" + 0.002*\"smooth well\" + 0.001*\"fold mixture\"'),\n",
       " (16,\n",
       "  '0.016*\"whisk\" + 0.014*\"egg\" + 0.010*\"mixture\" + 0.009*\"cream\" + 0.007*\"egg whites\" + 0.007*\"whites\" + 0.006*\"bowl\" + 0.006*\"mascarpone\" + 0.005*\"whisk egg\" + 0.005*\"peaks\"'),\n",
       " (17,\n",
       "  '0.017*\"heat\" + 0.014*\"pan\" + 0.013*\"oil\" + 0.011*\"cook\" + 0.010*\"salt\" + 0.010*\"pepper\" + 0.009*\"stir\" + 0.008*\"season\" + 0.008*\"fry\" + 0.007*\"sauce\"'),\n",
       " (18,\n",
       "  '0.011*\"baking\" + 0.009*\"oven\" + 0.009*\"flour\" + 0.008*\"dough\" + 0.007*\"together\" + 0.007*\"mix\" + 0.007*\"bowl\" + 0.007*\"tray\" + 0.006*\"butter\" + 0.006*\"milk\"'),\n",
       " (19,\n",
       "  '0.017*\"pastry\" + 0.012*\"dough\" + 0.009*\"oven\" + 0.007*\"baking\" + 0.007*\"place\" + 0.007*\"roll\" + 0.007*\"bowl\" + 0.006*\"egg\" + 0.006*\"together\" + 0.006*\"flour\"')]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda20_15passes = models.LdaModel(corpus=corpus, num_topics=20, id2word=id2word, passes=15) #very slow, stuck?\n",
    "#do more passes give better topics?\n",
    "lda20_15passes.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T23:58:11.841317Z",
     "start_time": "2019-12-03T23:56:58.621153Z"
    }
   },
   "outputs": [],
   "source": [
    "lda_10 = models.LdaModel(corpus=corpus, num_topics=10, id2word=id2word, passes=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T23:58:11.897802Z",
     "start_time": "2019-12-03T23:58:11.844774Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.003*\"duck\" + 0.002*\"dripping\" + 0.001*\"parts\" + 0.001*\"beef dripping\" + 0.001*\"pancakes\" + 0.001*\"red onions\" + 0.001*\"parts spring\" + 0.001*\"onions potatoes\" + 0.001*\"pour tall\" + 0.001*\"tomatoes olives\"'),\n",
       " (1,\n",
       "  '0.009*\"job\" + 0.006*\"kid job\" + 0.006*\"kid\" + 0.004*\"adult\" + 0.003*\"adult job\" + 0.002*\"vegetables\" + 0.002*\"lemon\" + 0.002*\"want\" + 0.002*\"like\" + 0.002*\"crumble\"'),\n",
       " (2,\n",
       "  '0.003*\"jars\" + 0.002*\"jam\" + 0.001*\"place ribs\" + 0.001*\"boil\" + 0.001*\"sugar\" + 0.001*\"boil cook\" + 0.001*\"evaporated\" + 0.001*\"almost evaporated\" + 0.001*\"almost\" + 0.001*\"hot\"'),\n",
       " (3,\n",
       "  '0.004*\"croquettes\" + 0.001*\"croquette\" + 0.001*\"top frying\" + 0.001*\"flour milk\" + 0.001*\"pasta broccoli\" + 0.001*\"top grated\" + 0.001*\"tomatoes using\" + 0.001*\"whiz\" + 0.001*\"cheese place\" + 0.001*\"melted season\"'),\n",
       " (4,\n",
       "  '0.013*\"heat\" + 0.012*\"pan\" + 0.010*\"oil\" + 0.009*\"cook\" + 0.007*\"salt\" + 0.007*\"stir\" + 0.007*\"pepper\" + 0.006*\"water\" + 0.006*\"fry\" + 0.006*\"serve\"'),\n",
       " (5,\n",
       "  '0.007*\"pasta\" + 0.002*\"machine\" + 0.002*\"flour\" + 0.002*\"ready\" + 0.001*\"oranges\" + 0.001*\"sieve\" + 0.001*\"use\" + 0.001*\"strip\" + 0.001*\"frozen\" + 0.001*\"blended\"'),\n",
       " (6,\n",
       "  '0.007*\"lemon\" + 0.004*\"juice\" + 0.003*\"peel\" + 0.003*\"finely\" + 0.003*\"chop\" + 0.003*\"half\" + 0.002*\"cut\" + 0.002*\"place\" + 0.002*\"finely chop\" + 0.002*\"ingredients\"'),\n",
       " (7,\n",
       "  '0.008*\"oven\" + 0.006*\"place\" + 0.005*\"water\" + 0.005*\"foil\" + 0.004*\"pudding\" + 0.004*\"top\" + 0.004*\"chicken\" + 0.004*\"cover\" + 0.004*\"turkey\" + 0.003*\"roasting\"'),\n",
       " (8,\n",
       "  '0.011*\"mixture\" + 0.008*\"sugar\" + 0.008*\"bowl\" + 0.007*\"oven\" + 0.007*\"place\" + 0.006*\"heat\" + 0.006*\"cream\" + 0.006*\"butter\" + 0.005*\"tin\" + 0.005*\"cool\"'),\n",
       " (9,\n",
       "  '0.011*\"pastry\" + 0.010*\"cake\" + 0.009*\"dough\" + 0.007*\"icing\" + 0.007*\"cm\" + 0.007*\"oven\" + 0.005*\"baking\" + 0.005*\"bowl\" + 0.005*\"roll\" + 0.005*\"together\"')]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_10.print_topics()#20 most important words for 10 topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not many repeated words across 10 topics which is good. Can probably have more topics? \n",
    "\n",
    "Topics: (*get different topics for diff number of passes - when does it converge?)\n",
    "\n",
    "0. ? cocktail (crepes? special char not english)\n",
    "1. kid-friendly\n",
    "2. ice lolly\n",
    "3. dessert - pastry\n",
    "4. dessert - pastry\n",
    "5. dessert - chocolate, meringue\n",
    "6. teriyaki\n",
    "7. ? pan\n",
    "8. dessert - cake\n",
    "9. ? corn and game meat?\n",
    "\n",
    "20 topics seem better than 10, use lda above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T23:58:11.909016Z",
     "start_time": "2019-12-03T23:58:11.900323Z"
    }
   },
   "outputs": [],
   "source": [
    "#* can set a large number (50 or 100) to make sure it converges, then compare 20 with 10 topics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20 topics, 100 passes\n",
    "\n",
    "*Gensim can run in parallel? 100 and 50 passes both >5min30s, converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T00:05:14.341882Z",
     "start_time": "2019-12-03T23:58:11.915826Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.012*\"pan\" + 0.011*\"heat\" + 0.011*\"pasta\" + 0.011*\"cook\" + 0.007*\"sauce\" + 0.006*\"stir\" + 0.005*\"large\" + 0.005*\"gently\" + 0.005*\"put\" + 0.004*\"frying\"'),\n",
       " (1,\n",
       "  '0.007*\"fillets\" + 0.004*\"beurre\" + 0.004*\"haddock\" + 0.003*\"blanc\" + 0.003*\"beurre blanc\" + 0.003*\"smoked haddock\" + 0.003*\"walnuts\" + 0.003*\"chowder\" + 0.002*\"haddock fillets\" + 0.002*\"lamb\"'),\n",
       " (2,\n",
       "  '0.014*\"mixture\" + 0.011*\"sugar\" + 0.010*\"pastry\" + 0.008*\"oven\" + 0.008*\"bowl\" + 0.007*\"tin\" + 0.007*\"cream\" + 0.007*\"egg\" + 0.007*\"whisk\" + 0.006*\"together\"'),\n",
       " (3,\n",
       "  '0.013*\"place\" + 0.013*\"oven\" + 0.010*\"salt\" + 0.009*\"pepper\" + 0.008*\"oil\" + 0.008*\"bowl\" + 0.007*\"mix\" + 0.007*\"season\" + 0.006*\"well\" + 0.006*\"preheat\"'),\n",
       " (4,\n",
       "  '0.002*\"topping ingredients\" + 0.002*\"remaining fruit\" + 0.002*\"slice bread\" + 0.001*\"back cling\" + 0.001*\"another slice\" + 0.001*\"evaporated sugar\" + 0.001*\"peel back\" + 0.001*\"beans top\" + 0.001*\"eggs using\" + 0.001*\"boil cook\"'),\n",
       " (5,\n",
       "  '0.000*\"matchstick\" + 0.000*\"matchstick potatoes\" + 0.000*\"pork pan\" + 0.000*\"another teaspoon\" + 0.000*\"lovage\" + 0.000*\"high cut\" + 0.000*\"juices pour\" + 0.000*\"instead olives\" + 0.000*\"sides fit\" + 0.000*\"squashy releasing\"'),\n",
       " (6,\n",
       "  '0.006*\"pancakes\" + 0.006*\"pancake\" + 0.002*\"remaining batter\" + 0.001*\"pancakes warm\" + 0.001*\"cm non\" + 0.001*\"low temperature\" + 0.001*\"pancake cook\" + 0.001*\"wrapping\" + 0.001*\"keep cooked\" + 0.001*\"temperature oven\"'),\n",
       " (7,\n",
       "  '0.016*\"cake\" + 0.009*\"icing\" + 0.009*\"sugar\" + 0.008*\"top\" + 0.008*\"mixture\" + 0.008*\"tin\" + 0.008*\"bowl\" + 0.008*\"cm\" + 0.007*\"oven\" + 0.007*\"cool\"'),\n",
       " (8,\n",
       "  '0.014*\"chicken\" + 0.010*\"job\" + 0.010*\"turkey\" + 0.010*\"oven\" + 0.008*\"roasting\" + 0.008*\"pork\" + 0.007*\"juices\" + 0.007*\"marinade\" + 0.006*\"kid\" + 0.006*\"kid job\"'),\n",
       " (9,\n",
       "  '0.002*\"sides deep\" + 0.002*\"dish generously\" + 0.001*\"cook prawns\" + 0.001*\"pie place\" + 0.001*\"cream stiff\" + 0.001*\"sugar medium\" + 0.001*\"top hot\" + 0.001*\"sugar necessary\" + 0.001*\"zest blend\" + 0.001*\"wilted spinach\"'),\n",
       " (10,\n",
       "  '0.001*\"skewers cook\" + 0.001*\"marinade stir\" + 0.001*\"lobster\" + 0.000*\"heat ridged\" + 0.000*\"grill pan\" + 0.000*\"pork wok\" + 0.000*\"cleaver\" + 0.000*\"ridged grill\" + 0.000*\"marinate chicken\" + 0.000*\"lobster heat\"'),\n",
       " (11,\n",
       "  '0.002*\"water tender\" + 0.002*\"cod\" + 0.002*\"omelette\" + 0.002*\"well refresh\" + 0.002*\"fish roasting\" + 0.002*\"chilli cumin\" + 0.002*\"dish fill\" + 0.002*\"lumps flour\" + 0.002*\"lime leaves\" + 0.002*\"beans pan\"'),\n",
       " (12,\n",
       "  '0.005*\"slice\" + 0.004*\"bread\" + 0.003*\"berries\" + 0.003*\"inside\" + 0.003*\"grated chocolate\" + 0.002*\"cling film\" + 0.002*\"cling\" + 0.002*\"film\" + 0.002*\"slices\" + 0.002*\"end\"'),\n",
       " (13,\n",
       "  '0.000*\"roughened\" + 0.000*\"anchovy fillets\" + 0.000*\"fillets help\" + 0.000*\"flowers basil\" + 0.000*\"courgette flowers\" + 0.000*\"dissolve anchovy\" + 0.000*\"followed courgette\" + 0.000*\"immediately individual\" + 0.000*\"stage ladles\" + 0.000*\"pan sweat\"'),\n",
       " (14,\n",
       "  '0.002*\"cut eggs\" + 0.002*\"bring room\" + 0.001*\"tabbouleh\" + 0.001*\"tablespoons fat\" + 0.001*\"bread serve\" + 0.001*\"piri\" + 0.001*\"tomato half\" + 0.001*\"lemon balm\" + 0.001*\"balm\" + 0.001*\"juices cook\"'),\n",
       " (15,\n",
       "  '0.003*\"dripping\" + 0.002*\"beef\" + 0.002*\"cm beef\" + 0.002*\"beef dripping\" + 0.002*\"place cm\" + 0.002*\"batter leave\" + 0.002*\"roasting tray\" + 0.002*\"batter\" + 0.002*\"bottom\" + 0.002*\"flour pinch\"'),\n",
       " (16,\n",
       "  '0.020*\"dough\" + 0.013*\"pastry\" + 0.009*\"roll\" + 0.008*\"flour\" + 0.007*\"place\" + 0.007*\"oven\" + 0.007*\"bowl\" + 0.006*\"surface\" + 0.006*\"baking\" + 0.006*\"egg\"'),\n",
       " (17,\n",
       "  '0.001*\"muffin holes\" + 0.001*\"eight holes\" + 0.001*\"peelings\" + 0.000*\"simmer cooked\" + 0.000*\"nog\" + 0.000*\"egg nog\" + 0.000*\"sundae\" + 0.000*\"sundae glasses\" + 0.000*\"lollies\" + 0.000*\"cut equal\"'),\n",
       " (18,\n",
       "  '0.022*\"heat\" + 0.019*\"pan\" + 0.012*\"cook\" + 0.011*\"stir\" + 0.011*\"oil\" + 0.010*\"fry\" + 0.009*\"water\" + 0.008*\"sauce\" + 0.007*\"simmer\" + 0.006*\"serve\"'),\n",
       " (19,\n",
       "  '0.002*\"syrup\" + 0.002*\"blended\" + 0.001*\"whisking\" + 0.001*\"cream machine\" + 0.001*\"machine\" + 0.001*\"minute place\" + 0.001*\"begin whisking\" + 0.001*\"thread\" + 0.001*\"machine churn\" + 0.001*\"heat dissolve\"')]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda20_100passes = models.LdaModel(corpus=corpus, num_topics=20, id2word=id2word, passes=100) #very slow, stuck?\n",
    "lda20_100passes.print_topics() #compare with 5,10,15 passes above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10 topics, 100 passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T00:11:53.053791Z",
     "start_time": "2019-12-03T23:54:33.953Z"
    }
   },
   "outputs": [],
   "source": [
    "lda10_100passes = models.LdaModel(corpus=corpus, num_topics=10, id2word=id2word, passes=100) #very slow, stuck?\n",
    "lda10_100passes.print_topics() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA is problematic because topics change each time the code is run; go back to use NMF topics to assign labels to recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map documents to Topic space, using LDA Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T00:11:53.055355Z",
     "start_time": "2019-12-03T23:54:33.963Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transform the docs from the word space to the topic space (like \"transform\" in sklearn)\n",
    "#lda_corpus = lda[corpus]\n",
    "#lda_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T00:11:53.057107Z",
     "start_time": "2019-12-03T23:54:33.968Z"
    }
   },
   "outputs": [],
   "source": [
    "# Store the documents' topic vectors in a list\n",
    "#lda_docs = [doc for doc in lda_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T00:11:53.059936Z",
     "start_time": "2019-12-03T23:54:33.974Z"
    }
   },
   "outputs": [],
   "source": [
    "#document vectors in the topic space -- measures of the component of each document along each topic\n",
    "# Check out the document vectors in the topic space for the first 5 documents \n",
    "#(out of 20 topics, expect most will have fewer than 3)  #10 passes\n",
    "#lda_docs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, first document is 19 or 18 (they have similar words), 2nd is most similar to topic 5, 3rd is topic 18, 4th is topic 5, 5th is topic 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T01:37:47.927719Z",
     "start_time": "2019-12-04T01:37:47.620590Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       cook the pasta in a pan of boiling salted wa...\n",
       "2       to make the basic dough  line a baking tray ...\n",
       "3       preheat the oven to c c fan gas      for the...\n",
       "7       preheat the oven to c f gas      take a very...\n",
       "18      preheat the oven to c c fan gas      heat g ...\n",
       "Name: instructions, dtype: object"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check actual documents to see if results make sense\n",
    "cld_withphotos.head()#just instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T01:37:50.064771Z",
     "start_time": "2019-12-04T01:37:50.017037Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1                                       15 minute pasta\n",
       "2                                          3D biscuits \n",
       "3                               2-hour Christmas dinner\n",
       "7     A classic sponge cake (with passion fruit fill...\n",
       "18            Albanian baked lamb with rice (Tavë kosi)\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "withphotos.title.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the 2nd and 4th recipe in the subset with photos are indeed desserts (topic 5) (20 topics, 10 passes?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T01:39:05.689520Z",
     "start_time": "2019-12-04T01:39:05.676092Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1                                          15 minute pasta\n",
       "2                                             3D biscuits \n",
       "3                                  2-hour Christmas dinner\n",
       "7        A classic sponge cake (with passion fruit fill...\n",
       "18               Albanian baked lamb with rice (Tavë kosi)\n",
       "                               ...                        \n",
       "10574                                   Yorkshire puddings\n",
       "10576                           Yorkshire ‘tapas’ puddings\n",
       "10582                                             Yule log\n",
       "10585                             Za’atar cod with relish \n",
       "10589                                Zesty tofu cheesecake\n",
       "Name: title, Length: 2225, dtype: object"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "withphotos.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering on topics - Assign image to topics/cluster?\n",
    "\n",
    "Dec. 3\n",
    "*Roberto: after getting document-topic matrix, can either assign document to topic with highest weight, or cluster in topic space (documents are points in that space)! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T00:11:53.068559Z",
     "start_time": "2019-12-03T23:54:34.085Z"
    }
   },
   "outputs": [],
   "source": [
    "#kmeans clustering:\n",
    "# from sklearn.externals import joblib\n",
    "\n",
    "# #uncomment the below to save your model \n",
    "# #since I've already run my model I am loading from the pickle\n",
    "\n",
    "# #joblib.dump(km,  'doc_cluster.pkl')\n",
    "\n",
    "# km = joblib.load('doc_cluster.pkl')\n",
    "# clusters = km.labels_.tolist()\n",
    "#http://brandonrose.org/clustering - also has LDA example\n",
    "\n",
    "    #for clustering on topics, may need cos sim/euc dist of topics first?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T00:11:53.070354Z",
     "start_time": "2019-12-03T23:54:34.090Z"
    }
   },
   "outputs": [],
   "source": [
    "#try kmeans (or DBSCAN?) on local or AWS - separate from topic results? ask\n",
    "    #assign each image to cluster, then find cluster of test image, find cosine similarity (0.5-1day?)\n",
    "    #pickle and move to Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also:\n",
    "https://datascience.stackexchange.com/questions/19991/cluster-documents-based-on-topic-similarity/19999?noredirect=1#comment23532_19999\n",
    "> Topics are clusters\n",
    "> There is next to no difference between subspace clustering and topic modeling, except maybe that text is sparse and integer while subspace clusterers usually assume dense and continuous data.\n",
    "So rather than trying to cluster again, just use your topics.\n",
    "Yes, documents can belong to multiple topics. That is because text usually is this way, and forcing everything to have a unique label reduces the quality, because it cannot reflect reality anymore.\n",
    " If you insist every document should have a unique cluster, just use argmax(topic weights).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (why topic modelling is not used for classification) Topic modeling produces a number of topics for tokens in your text. Overall, you end up with a number of different topics (even per sentence) and in lots of texts there will be multiple prominent topic with high scores. That doesn't help you determine which class the text belongs to overall.\n",
    "https://www.quora.com/Why-isnt-topic-modelling-used-more-for-document-classification\n",
    "\n",
    "topic modelling with LDA example https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/ *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T00:11:53.072026Z",
     "start_time": "2019-12-03T23:54:34.096Z"
    }
   },
   "outputs": [],
   "source": [
    "#argmax(topic weights) - from topic modelling output? get topic weights for new image, then cos sim\n",
    "    #would need to assign each of the 2k images to its top topic\n",
    "#also under topic modelling lecture: LDA+gensim example notebook\n",
    "\n",
    "#if use topic modelling and LDA, don't actually make use of the 10 topics I already have (repeating work?)\n",
    "    #didn't use them for prediction in project4 either, want to improve on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T00:11:53.074934Z",
     "start_time": "2019-12-03T23:54:34.102Z"
    }
   },
   "outputs": [],
   "source": [
    "#(Also, how to incorporate this with user uploaded info (ingredient or cuisine type?)--may not need that if sep into topics/clusters?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
